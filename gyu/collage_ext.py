# run_gui_actor.py

import os
import argparse
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= "0"  # ëª‡ë²ˆ GPU ì‚¬ìš©í• ì§€ ("0,1", "2" ë“±)

max_memory = {
    0: "67GiB",
    # 1: "75GiB",
    # 2: "75GiB",
    "cpu": "120GiB",  # ë‚¨ëŠ” ê±´ CPU ì˜¤í”„ë¡œë”©xs
}

parser = argparse.ArgumentParser()
parser.add_argument('--no_early_exit', action='store_true', help='Disable early exit')
parser.add_argument('--max_pixels', type=int, help='Maximum pixels for image resize')
args = parser.parse_args()

#! Hyperparameter =====================================================================================

ATTN_IMPL = "eager"  # attention implement "eager" "sdpa" "flash" "efficient"

# Image Resize Ratios
# MAX_PIXELS = None
# MAX_PIXELS = 1280 * 28 * 28
MAX_PIXELS = 3211264
# MAX_PIXELS = args.max_pixels if args.max_pixels else None
S1_RESIZE_RATIO = 0.35  # Stage 1 crop resize ratio
S2_RESIZE_RATIO = 1.00  # Stage 2 crop resize ratio
THUMBNAIL_RESIZE_RATIO = 0.10  # Thumbnail resize ratio

SELECT_THRESHOLD = 0.7  # score >= tau * max_score ì¸ ëª¨ë“  crop select
# EARLY_EXIT ì„¤ì •: --no_early_exitì´ë©´ False, ê¸°ë³¸ê°’ True

EARLY_EXIT = False
EARLY_EXIT_THRE = 0.6  # 1ë“± attention * thre > 2ë“± attentionì´ë¼ë©´ early exit

SET_OF_MARK = False
COLLAGE = True

# Crop Extension í•˜ì´í¼íŒŒë¼ë¯¸í„°
CROP_EDGE_THRESHOLD = 50  # ëë¶€ë¶„ìœ¼ë¡œ ë³¼ pixel ê±°ë¦¬ (attention ê³ ì ì´ ì´ ê±°ë¦¬ ë‚´ì— ìˆìœ¼ë©´ í™•ì¥ ê³ ë ¤)
CROP_EXTENSION_PIXELS = 100  # í™•ì¥í•  pixel ìˆ˜

is_ee = "ee" if EARLY_EXIT else "not_ee"
SAVE_DIR = f"./attn_output/" + is_ee + "_" + str(MAX_PIXELS) + "_" + \
    str(S1_RESIZE_RATIO) + "_" + str(S2_RESIZE_RATIO) + "_" + "0905_gyu_gk20_vis"  #! Save Path (íŠ¹ì§•ì´ ìˆë‹¤ë©´ ì ì–´ì£¼ì„¸ìš”)

SAVE_DIR = f"gyu/attn_output/0907_collage_ext_nosom"
#! Argument ==========================================================================================

SEED = 0

# Dataset & Model
MLLM_PATH = "microsoft/GUI-Actor-3B-Qwen2.5-VL"
SCREENSPOT_IMGS = "./data/screenspotv2_image"  # input image ê²½ë¡œ
SCREENSPOT_JSON = "./data"  # jsoníŒŒì¼ ê²½ë¡œ
TASKS = ["mobile","web", "desktop"]
# TASKS = ["web"]
# SAMPLE_RANGE = slice(160,162)  #! ìƒ˜í”Œ ë²”ìœ„ ì§€ì • (3ë²ˆ ìƒ˜í”Œì´ë©´ 3,4 / 5~9ë²ˆ ìƒ˜í”Œì´ë©´ 5,10 / ì „ì²´ ì‚¬ìš©ì´ë©´ None)
# SAMPLE_RANGE = slice(485, 486)  #! ìƒ˜í”Œ ë²”ìœ„ ì§€ì • (3ë²ˆ ìƒ˜í”Œì´ë©´ 3,4 / 5~9ë²ˆ ìƒ˜í”Œì´ë©´ 5,10 / ì „ì²´ ì‚¬ìš©ì´ë©´ None)
SAMPLE_RANGE = slice(None)

# Visualize & Logging
STAGE0_VIS = True
STAGE1_VIS = True
STAGE2_VIS = True
ITER_LOG = True  # csv, md
TFOPS_PROFILING = True
MEMORY_EVAL = True
MEMORY_VIS = True

# Question
# QUESTION_TEMPLATE="""Where should you tap to {task_prompt}?"""
QUESTION_TEMPLATE="""
You are an assistant trained to navigate the android phone. Given a
task instruction, a screen observation, guess where should you tap.
# Intruction
{task_prompt}"""

#! ==================================================================================================

# Standard Library
import json
import re
import sys
sys.path.append(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.setrecursionlimit(10000)  # DeepSpeed logging
import time
from copy import deepcopy
from typing import List, Tuple
from math import sqrt
import matplotlib.pyplot as plt
import threading

# Third-Party Libraries
import numpy as np
from PIL import Image, ImageDraw
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoTokenizer, set_seed
if TFOPS_PROFILING:
    from deepspeed.profiling.flops_profiler import FlopsProfiler

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from iter_logger import init_iter_logger, append_iter_log  # log csv ê¸°ë¡ íŒŒì¼
from gui_actor.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer
from gui_actor.multi_image_inference import multi_image_inference
from visualize_util import get_highest_attention_patch_bbox, _visualize_early_exit_results, _visualize_stage1_results, _visualize_stage2_results, visualize_crop
from crop import crop_img as run_crop #! ì–´ë–¤ crop íŒŒì¼ ì‚¬ìš©?

#! ==============================================================================================

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        return super(NpEncoder, self).default(obj)
    
def resize_image(image, resize_to_pixels=MAX_PIXELS):
    image_width, image_height = image.size
    if (resize_to_pixels is not None) and ((image_width * image_height) != resize_to_pixels):
        resize_ratio = (resize_to_pixels / (image_width * image_height)) ** 0.5
        image_width_resized, image_height_resized = int(image_width * resize_ratio), int(image_height * resize_ratio)
        image = image.resize((image_width_resized, image_height_resized))
    return image, image_width_resized, image_height_resized 


def warm_up_model(model, tokenizer, processor):
    print("ğŸ”„ Warming up the model...")
    dummy_instruction = "This is a dummy instruction for warm-up."
    dummy_image = Image.new("RGB", (1000, 1000), color=(255, 255, 255))  # 1000x1000 í°ìƒ‰ ì´ë¯¸ì§€
    dummy_crop = {
        "img": dummy_image,
        "resized_img": dummy_image,
        "id": 1,
        "bbox": [0, 0, 1000, 1000]
    }
    dummy_crop_list = [dummy_crop]
    dummy_msgs = create_guiactor_msgs(crop_list=dummy_crop_list, instruction=dummy_instruction)
    
    # ì˜ˆì—´ìš© inference ì‹¤í–‰
    for _ in range(3):  # 3ë²ˆ ë°˜ë³µ
        with torch.no_grad():
            if TFOPS_PROFILING:
                prof.start_profile()
            _ = multi_image_inference(dummy_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
            if TFOPS_PROFILING:
                prof.stop_profile()
                prof.get_total_flops()
    print("âœ… Warm-up complete!")


def create_guiactor_msgs(crop_list, instruction):
    user_content = []
    for crop in crop_list:
        img = crop["resized_img"]
        user_content.append({"type": "image", "image": img})

    user_content.append({
        "type": "text",
        "text": instruction,
    })
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": user_content,
        },
    ]
    return conversation

def resize_crop_list(crop_list: List, ratio: float):
    stage_crop_list = []

    for crop in crop_list:
        crop_id = crop.get("id")

        # ì¸ë„¤ì¼(id=0)ì€ thumbnail ë¹„ìœ¨, ë‚˜ë¨¸ì§€ëŠ” crop ë¹„ìœ¨ ì‚¬ìš©
        if crop_id == 0:
            crop_ratio = THUMBNAIL_RESIZE_RATIO
        else:
            crop_ratio = ratio

        new_crop = deepcopy(crop)
        crop_img = deepcopy(crop["img"])

        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
        crop_width, crop_height = crop_img.size
        crop_img = crop_img.resize((int(crop_width * crop_ratio), int(crop_height * crop_ratio)))
        new_crop["resized_img"] = crop_img
        stage_crop_list.append(new_crop)

    return stage_crop_list

def select_crop(crop_list, tau):
    """
    score >= tau * max_score ì¸ ëª¨ë“  cropì˜ idë§Œ ë°˜í™˜ (id==0ì€ ë¬´ì‹œ)
    """
    filtered = [crop for crop in crop_list if crop.get("id") != 0]
    scores = [float(c["s1_att_sum"]) for c in filtered]
    max_score = max(scores)
    threshold = max_score * float(tau)
    out_ids = []
    for crop in filtered:
        cid = crop.get("id")
        if float(crop["s1_att_sum"]) >= threshold:
            out_ids.append(cid)
    return out_ids

def check_gt_in_selected_y_ranges(y_ranges: List, gt_bbox: List, image_width: int):
    """
    ì„ íƒëœ Y ë²”ìœ„ë“¤ì´ GT bboxì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
    """
    if not y_ranges:
        return False
        
    for y_top, y_bottom in y_ranges:
        # Y ë²”ìœ„ë¥¼ ì „ì²´ ë„ˆë¹„ì˜ bboxë¡œ ë³€í™˜
        range_bbox = [0, y_top, image_width, y_bottom]
        
        # GT bboxì™€ êµì§‘í•© í™•ì¸
        al, at, ar, ab = range_bbox
        bl, bt, br, bb = gt_bbox
        inter_left = max(al, bl)
        inter_top = max(at, bt)
        inter_right = min(ar, br)
        inter_bottom = min(ab, bb)
        
        if (inter_right > inter_left) and (inter_bottom > inter_top):
            return True
    
    return False

def check_gt_in_selected_crops(top_q_bboxes: List, gt_bbox: List):
    def rect_intersects(a, b):
        # a, b: [left, top, right, bottom]
        al, at, ar, ab = a
        bl, bt, br, bb = b
        # êµì§‘í•© ì˜ì—­ ê³„ì‚°
        inter_left = max(al, bl)
        inter_top = max(at, bt)
        inter_right = min(ar, br)
        inter_bottom = min(ab, bb)
        return (inter_right > inter_left) and (inter_bottom > inter_top)
    return any(rect_intersects(gt_bbox, bbox) for bbox in top_q_bboxes)


def check_gt_center_point_in_selected_crops(top_q_bboxes: List, gt_bbox: List):
    gt_left, gt_top, gt_right, gt_bottom = gt_bbox
    gt_center = [(gt_left + gt_right) / 2, (gt_top + gt_bottom) / 2]

    for bbox in top_q_bboxes:
        is_gt_in_crop = point_in_bbox(gt_center, bbox)
        
        if is_gt_in_crop: return True

    return False

def compute_attention_scores(crop_list, per_image_outputs):
    """ê° cropì˜ attention score ê³„ì‚°"""
    for i, crop in enumerate(crop_list):
        crop_att_scores_np = np.array(per_image_outputs[i]['attn_scores'][0])
        total_att_score = np.sum(crop_att_scores_np)
        
        # ë©´ì ì˜ ì œê³±ê·¼ìœ¼ë¡œ normalize
        bbox = crop.get('bbox')
        if bbox is not None:
            left, top, right, bottom = bbox
            area = max(1, (right - left) * (bottom - top))
        else:
            area = 1
        crop['s1_att_sum'] = total_att_score / sqrt(area)

def find_top_crop_for_early_exit(crop_list, per_image_outputs):
    """Early Exitìš© ìµœê³  ì ìˆ˜ cropê³¼ point ì°¾ê¸°"""
    top_score = -1
    top_point = None
    top_crop_id = -1
    
    for i, crop in enumerate(crop_list):
        if crop.get("id") == 0:  # ì¸ë„¤ì¼ì€ ìŠ¤í‚µ
            continue
            
        crop_att_scores_np = np.array(per_image_outputs[i]['attn_scores'][0])
        crop_top_point = per_image_outputs[i]['topk_points'][0]
        crop_top_score = np.max(crop_att_scores_np)
        
        crop['top_point'] = {'point': crop_top_point, 'score': crop_top_score}
        
        if top_score < crop_top_score:
            top_score = crop_top_score
            top_point = crop_top_point
            top_crop_id = crop['id']
    
    return top_point, top_crop_id

def get_highest_attention_patch_bbox(image_result: dict) -> list:
    """
    per_image ê²°ê³¼ì—ì„œ ì–´í…ì…˜ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ íŒ¨ì¹˜ë¥¼ ì°¾ì•„ 
    í•´ë‹¹ íŒ¨ì¹˜ì˜ ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ ë°˜í™˜
    """
    # 1. ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
    attn_scores = np.array(image_result['attn_scores'][0])
    n_width = image_result['n_width']
    n_height = image_result['n_height']

    # 2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ íŒ¨ì¹˜ì˜ 1ì°¨ì› ì¸ë±ìŠ¤ ì°¾ê¸°
    highest_score_idx = np.argmax(attn_scores)

    # 3. 1ì°¨ì› ì¸ë±ìŠ¤ë¥¼ 2ì°¨ì› íŒ¨ì¹˜ ê·¸ë¦¬ë“œ ì¢Œí‘œ (patch_x, patch_y)ë¡œ ë³€í™˜
    # (patch_xëŠ” ê°€ë¡œ ì¸ë±ìŠ¤, patch_yëŠ” ì„¸ë¡œ ì¸ë±ìŠ¤)
    patch_y = highest_score_idx // n_width
    patch_x = highest_score_idx % n_width

    # 4. íŒ¨ì¹˜ ê·¸ë¦¬ë“œ ì¢Œí‘œë¥¼ ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¡œ ê³„ì‚°
    # ê° íŒ¨ì¹˜ì˜ ì •ê·œí™”ëœ ë„ˆë¹„ì™€ ë†’ì´
    patch_norm_width = 1.0 / n_width
    patch_norm_height = 1.0 / n_height
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
    left = patch_x * patch_norm_width
    top = patch_y * patch_norm_height
    right = (patch_x + 1) * patch_norm_width
    bottom = (patch_y + 1) * patch_norm_height
    
    return [left, top, right, bottom]


def get_highest_attention_patch_info(image_result: dict) -> dict:
    """
    per_image ê²°ê³¼ì—ì„œ ì–´í…ì…˜ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ íŒ¨ì¹˜ì™€ ë‘ ë²ˆì§¸ë¡œ ë†’ì€ íŒ¨ì¹˜ë¥¼ ì°¾ì•„
    í•´ë‹¹ íŒ¨ì¹˜ë“¤ì˜ ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œì™€ ìŠ¤ì½”ì–´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        image_result (dict): prediction_results['per_image'] ë¦¬ìŠ¤íŠ¸ì˜ ë‹¨ì¼ ì•„ì´í…œ.

    Returns:
        dict: ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ì˜ ë”•ì…”ë„ˆë¦¬
              {
                  'highest': {'bbox': [l,t,r,b], 'score': float}, 
                  'second_highest': {'bbox': [l,t,r,b], 'score': float}
              }
    """
    # 1. ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
    attn_scores = np.array(image_result['attn_scores'][0])
    n_width = image_result['n_width']
    n_height = image_result['n_height']

    # 2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ì •ë ¬í•˜ì—¬ ê°€ì¥ ë†’ì€ ë‘ ê°œì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    sorted_indices = np.argsort(attn_scores)
    highest_score_idx = sorted_indices[-1]
    second_highest_score_idx = sorted_indices[-2]

    # 3. ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ìŠ¤ì½”ì–´ ê°’ ê°€ì ¸ì˜¤ê¸°
    highest_score = attn_scores[highest_score_idx]
    second_highest_score = attn_scores[second_highest_score_idx]

    # 4. ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ ì •ì˜
    def _calculate_bbox(index: int) -> list:
        """1ì°¨ì› ì¸ë±ìŠ¤ë¡œë¶€í„° ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        patch_y = index // n_width
        patch_x = index % n_width

        patch_norm_width = 1.0 / n_width
        patch_norm_height = 1.0 / n_height
        
        left = patch_x * patch_norm_width
        top = patch_y * patch_norm_height
        right = (patch_x + 1) * patch_norm_width
        bottom = (patch_y + 1) * patch_norm_height
        
        return [left, top, right, bottom]

    # 5. ê°€ì¥ ë†’ì€ ìŠ¤ì½”ì–´ì™€ ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ìŠ¤ì½”ì–´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
    highest_bbox = _calculate_bbox(highest_score_idx)
    second_highest_bbox = _calculate_bbox(second_highest_score_idx)
    
    # 6. ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ê²°ê³¼ ë°˜í™˜ (ìŠ¤ì½”ì–´ í¬í•¨)
    return {
        'highest': {
            'bbox': highest_bbox,
            'score': float(highest_score) # numpy floatì„ ì¼ë°˜ floatìœ¼ë¡œ ë³€í™˜
        },
        'second_highest': {
            'bbox': second_highest_bbox,
            'score': float(second_highest_score) # numpy floatì„ ì¼ë°˜ floatìœ¼ë¡œ ë³€í™˜
        }
    }
def get_attention_points_from_crops(crop_list, per_image_outputs):
    """
    ê° cropì—ì„œ attention ê³ ì ë“¤ì˜ ì›ë³¸ ì´ë¯¸ì§€ ë‚´ Y ì¢Œí‘œë¥¼ ìˆ˜ì§‘
    """
    attention_points = []
    
    for i, crop in enumerate(crop_list):
        if crop.get("id") == 0:  # ì¸ë„¤ì¼ì€ ìŠ¤í‚µ
            continue
            
        # í•´ë‹¹ cropì˜ inference ê²°ê³¼ ì°¾ê¸°
        crop_result = None
        for result in per_image_outputs:
            if result.get('index') == crop.get("id"):
                crop_result = result
                break
                
        if crop_result is None:
            continue
            
        # ìµœê³  attention ìœ„ì¹˜ ì°¾ê¸°
        attn_scores = np.array(crop_result['attn_scores'][0])
        n_width = crop_result['n_width']
        n_height = crop_result['n_height']
        
        # ìƒìœ„ ëª‡ ê°œ attention ê³ ì  ìˆ˜ì§‘ (í™•ì¥ í›„ë³´ë“¤)
        top_indices = np.argsort(attn_scores)[-3:]  # ìƒìœ„ 3ê°œ
        
        for idx in top_indices:
            patch_y = idx // n_width
            patch_x = idx % n_width
            
            # íŒ¨ì¹˜ë¥¼ ì‹¤ì œ crop ë‚´ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            crop_img = crop["img"]
            crop_w, crop_h = crop_img.size
            
            patch_norm_width = 1.0 / n_width
            patch_norm_height = 1.0 / n_height
            
            # íŒ¨ì¹˜ ì¤‘ì‹¬ì ì˜ crop ë‚´ ì¢Œí‘œ
            patch_center_y = (patch_y + 0.5) * patch_norm_height * crop_h
            
            # cropì˜ ì›ë³¸ ì´ë¯¸ì§€ ë‚´ bbox
            crop_bbox = crop["bbox"]
            L, T, R, B = crop_bbox
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë‚´ ì ˆëŒ€ Y ì¢Œí‘œ
            abs_y = T + patch_center_y
            
            attention_points.append({
                'y': abs_y,
                'score': float(attn_scores[idx]),
                'crop_id': crop.get("id"),
                'crop_bbox': crop_bbox
            })
    
    return attention_points

def extend_y_ranges_with_attention(y_ranges, attention_points, original_height):
    """
    Y ë²”ìœ„ë“¤ì„ attention ê³ ì  ê¸°ë°˜ìœ¼ë¡œ í™•ì¥
    """
    if not y_ranges or not attention_points:
        return y_ranges
    
    extended_ranges = []
    
    for y_top, y_bottom in y_ranges:
        new_top = y_top
        new_bottom = y_bottom
        
        # ì´ ë²”ìœ„ ë‚´ì˜ attention ê³ ì ë“¤ ì°¾ê¸°
        for point in attention_points:
            point_y = point['y']
            
            # ì´ í¬ì¸íŠ¸ê°€ í˜„ì¬ ë²”ìœ„ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            if y_top <= point_y <= y_bottom:
                # ìœ„ìª½ ê²½ê³„ ê·¼ì²˜ì¸ì§€ í™•ì¸
                if point_y - y_top <= CROP_EDGE_THRESHOLD:
                    if y_top > 0:  # í™•ì¥ ê°€ëŠ¥í•œ ê²½ìš°
                        new_top = max(0, y_top - CROP_EXTENSION_PIXELS)
                        print(f"ğŸ”„ Y-range extended upward: {y_top} -> {new_top} (attention at {point_y:.0f})")
                
                # ì•„ë˜ìª½ ê²½ê³„ ê·¼ì²˜ì¸ì§€ í™•ì¸  
                if y_bottom - point_y <= CROP_EDGE_THRESHOLD:
                    if y_bottom < original_height:  # í™•ì¥ ê°€ëŠ¥í•œ ê²½ìš°
                        new_bottom = min(original_height, y_bottom + CROP_EXTENSION_PIXELS)
                        print(f"ğŸ”„ Y-range extended downward: {y_bottom} -> {new_bottom} (attention at {point_y:.0f})")
        
        extended_ranges.append((int(new_top), int(new_bottom)))
    
    return extended_ranges


def get_y_ranges_from_selected_crops(selected_crop_ids, crop_list):
    """
    ì„ íƒëœ cropë“¤ì˜ Y ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ê³  ì¸ì ‘í•œ ê²ƒë“¤ì„ ë³‘í•©
    """
    if not selected_crop_ids:
        return []
    
    # ì„ íƒëœ cropë“¤ì˜ Y ë²”ìœ„ ìˆ˜ì§‘ (id=0ì€ ì¸ë„¤ì¼ì´ë¯€ë¡œ ì œì™¸)
    y_ranges = []
    for crop in crop_list:
        if crop.get("id") in selected_crop_ids and crop.get("id") != 0:
            bbox = crop["bbox"]
            y_ranges.append((bbox[1], bbox[3]))  # (top, bottom)
    
    if not y_ranges:
        return []
    
    # Y ë²”ìœ„ë¥¼ top ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    y_ranges.sort()
    
    # ì¸ì ‘í•œ ë²”ìœ„ë“¤ì„ ë³‘í•©
    merged_ranges = [y_ranges[0]]
    for current_top, current_bottom in y_ranges[1:]:
        last_top, last_bottom = merged_ranges[-1]
        
        # í˜„ì¬ ë²”ìœ„ê°€ ì´ì „ ë²”ìœ„ì™€ ì¸ì ‘í•˜ê±°ë‚˜ ê²¹ì¹˜ë©´ ë³‘í•©
        if current_top <= last_bottom:
            merged_ranges[-1] = (last_top, max(last_bottom, current_bottom))
        else:
            merged_ranges.append((current_top, current_bottom))
    
    return merged_ranges

def create_stage2_crops_from_y_ranges(y_ranges, original_image):
    """
    Y ë²”ìœ„ë“¤ë¡œë¶€í„° Stage2ìš© cropë“¤ì„ ìƒì„±
    """
    if not y_ranges:
        return []
    
    orig_w, orig_h = original_image.size
    s2_crops = []
    
    # ì¸ë„¤ì¼ ì¶”ê°€ (id=0)
    s2_crops.append({
        "img": original_image,
        "id": 0,
        "bbox": [0, 0, orig_w, orig_h]
    })
    
    # Y ë²”ìœ„ë³„ë¡œ crop ìƒì„±
    for i, (y_top, y_bottom) in enumerate(y_ranges):
        bbox = [0, y_top, orig_w, y_bottom]
        crop_img = original_image.crop((0, y_top, orig_w, y_bottom))
        
        s2_crops.append({
            "img": crop_img,
            "id": i + 1,
            "bbox": bbox
        })
    
    return s2_crops



def check_early_exit_condition(top_point, top_crop_id, crop_list, per_image_outputs, gt_bbox, original_image):
    """Early Exit ì¡°ê±´ í™•ì¸"""
    if top_point is None or top_crop_id == -1:
        return False, False, None
    
    should_exit_early = False
    early_exit_success = False
    corrected_point = None

    ori_w, ori_h = original_image.size

    # ì¸ë„¤ì¼ì˜ ìµœê³  attention patch ì°¾ê¸°
    thumb_res = next((res for res in per_image_outputs if res['index'] == 0), None)
    # thumb_top_patch_bbox = get_highest_attention_patch_bbox(thumb_res)
 
    top2_patches = get_highest_attention_patch_info(thumb_res)
    top1 = top2_patches['highest']
    top2 = top2_patches['second_highest']

    top1_score = top1['score']
    top2_score = top2['score']

    if  EARLY_EXIT_THRE * top1_score > top2_score:  # early exit

        l, t, r, b = top1['bbox']
        denorm_thumb_top_patch_bbox = [l*ori_w, t*ori_h, r*ori_w, b*ori_h]
        
        # ì¢Œí‘œ ë³´ì •
        top_crop = next((c for c in crop_list if c['id'] == top_crop_id), None)
        top_crop_bbox = top_crop["bbox"]
        corrected_point = denormalize_crop_point(
            point_in_crop=top_point, 
            crop_size=top_crop['img'].size,
            crop_bbox=top_crop_bbox
        )
        
        should_exit_early = point_in_bbox(corrected_point, denorm_thumb_top_patch_bbox)

        # Early Exit ë§ì•˜ëŠ”ê°€?
        early_exit_success = point_in_bbox(corrected_point, gt_bbox)
    elif STAGE1_VIS:

        top_crop = next((c for c in crop_list if c['id'] == top_crop_id), None)
        top_crop_bbox = top_crop["bbox"]
        corrected_point = denormalize_crop_point(
            point_in_crop=top_point, 
            crop_size=top_crop['img'].size,
            crop_bbox=top_crop_bbox
        )
    
    return should_exit_early, early_exit_success, corrected_point

# def run_selection_pass_with_guiactor(msgs, crop_list, gt_bbox: List, attn_vis_dir: str, original_image, img_path, instruction):
#     """Stage 1 inference ë° Early Exit íŒë‹¨"""
    

#     os.makedirs(f"{s1_dir}/test", exist_ok=True)
#     for c in crop_list:
#         c['resized_img'].save(f"{s1_dir}/test/test_{c['id']}.png")
#     # Inference ìˆ˜í–‰
#     pred = multi_image_inference(msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
#     per_image_outputs = pred["per_image"]
    
#     # Attention scores ê³„ì‚°
#     compute_attention_scores(crop_list, per_image_outputs)
    
#     # Early Exit ì²´í¬
#     should_exit_early, early_exit_success = False, False
    
#     if EARLY_EXIT:
#         top_point, top_crop_id = find_top_crop_for_early_exit(crop_list, per_image_outputs)
#         should_exit_early, early_exit_success, corrected_top_point = check_early_exit_condition(
#             top_point, top_crop_id, crop_list, per_image_outputs, gt_bbox, original_image
#         )    # Early Exití•˜ë©´ select_crop ìŠ¤í‚µ
#     if should_exit_early:
#         top_q_crop_ids = []
#         top_q_bboxes = []
#     else:
#         # Select crop: score >= tau * max_scoreì¸ crops ì„ íƒ
#         top_q_crop_ids = select_crop(crop_list, tau=SELECT_THRESHOLD)
#         top_q_bboxes = [crop["bbox"] for crop in crop_list if crop.get("id") in top_q_crop_ids]
    
#     # ì‹œê°í™” (í•„ìš”ì‹œ)
#     if STAGE1_VIS and EARLY_EXIT and should_exit_early:
#         _visualize_early_exit_results(crop_list, pred, corrected_top_point, gt_bbox, attn_vis_dir, instruction, img_path)
#     elif STAGE1_VIS and not should_exit_early:
#         _visualize_stage1_results(crop_list, pred, attn_vis_dir, instruction)
    
#     return top_q_crop_ids, top_q_bboxes, crop_list, should_exit_early, early_exit_success

def run_selection_pass_with_guiactor(msgs, crop_list, gt_bbox: List, attn_vis_dir: str, original_image, img_path, instruction):
    """Stage 1 inference ë° Early Exit íŒë‹¨"""
    
    # Inference ìˆ˜í–‰
    pred = multi_image_inference(msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    per_image_outputs = pred["per_image"]
    
    # Attention scores ê³„ì‚°
    compute_attention_scores(crop_list, per_image_outputs)
    
    # Early Exit ì²´í¬
    should_exit_early, early_exit_success = False, False
    
    if EARLY_EXIT:
        top_point, top_crop_id = find_top_crop_for_early_exit(crop_list, per_image_outputs)
        should_exit_early, early_exit_success, corrected_top_point = check_early_exit_condition(
            top_point, top_crop_id, crop_list, per_image_outputs, gt_bbox, original_image
        )
    
    # Early Exití•˜ë©´ select_crop ìŠ¤í‚µ, ì•„ë‹ˆë©´ ì¢Œí‘œ ê¸°ë°˜ í™•ì¥ ì ìš©
    if should_exit_early:
        selected_y_ranges = []
        use_vanilla = False
        num_selected_crops = 0
    else:
        # Select crop: score >= tau * max_scoreì¸ crops ì„ íƒ
        top_q_crop_ids = select_crop(crop_list, tau=SELECT_THRESHOLD)
        num_selected_crops = len(top_q_crop_ids)  # ë³‘í•© ì „ ì›ë˜ ì„ íƒëœ í¬ë¡­ ê°œìˆ˜
        
        # ëª¨ë“  í¬ë¡­ì´ ì„ íƒë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì¸ë„¤ì¼ ì œì™¸)
        total_crops = len([c for c in crop_list if c.get("id") != 0])
        
        if num_selected_crops == total_crops:
            # ëª¨ë“  í¬ë¡­ì´ ì„ íƒë˜ì—ˆìœ¼ë©´ vanilla inference ì‚¬ìš©
            print(f"ğŸ”„ All {total_crops} crops selected, using vanilla inference")
            selected_y_ranges = []
            use_vanilla = True
        else:
            # ì„ íƒëœ cropë“¤ì˜ Y ë²”ìœ„ ë³‘í•©
            initial_y_ranges = get_y_ranges_from_selected_crops(top_q_crop_ids, crop_list)
            
            # Attention ê³ ì ë“¤ ìˆ˜ì§‘
            attention_points = get_attention_points_from_crops(crop_list, per_image_outputs)
            
            # Y ë²”ìœ„ë¥¼ attention ê¸°ë°˜ìœ¼ë¡œ í™•ì¥
            orig_w, orig_h = original_image.size
            selected_y_ranges = extend_y_ranges_with_attention(initial_y_ranges, attention_points, orig_h)
            use_vanilla = False
    
    # ì‹œê°í™” (í•„ìš”ì‹œ)
    if STAGE1_VIS and EARLY_EXIT and should_exit_early:
        _visualize_early_exit_results(crop_list, pred, corrected_top_point, gt_bbox, attn_vis_dir, instruction, img_path)
    elif STAGE1_VIS and not should_exit_early:
        _visualize_stage1_results(crop_list, pred, attn_vis_dir, instruction)
    
    return selected_y_ranges, should_exit_early, early_exit_success, use_vanilla, num_selected_crops


def denormalize_crop_point(point_in_crop, crop_size, crop_bbox):
    crop_w, crop_h = crop_size

    scaled_point = [point_in_crop[0] * crop_w, point_in_crop[1] * crop_h]
    corrected_point = [scaled_point[0] + crop_bbox[0], scaled_point[1] + crop_bbox[1]] 

    return corrected_point

def denorm_point(norm_point, orig_w, orig_h):
    new_x = norm_point[0] * orig_w
    new_y = norm_point[1] * orig_h
    return [new_x, new_y]

def abs_point_to_crop_point(abs_point, crop_origin):
    x, y = abs_point
    origin_x, origin_y = crop_origin
    new_x = x - origin_x
    new_y = y - origin_y
    return [new_x, new_y]

def norm_point(point, orig_w, orig_h):
    x, y = point
    if (0 <= x <= 1) and (0 <= y <= 1):
        return point
    new_x = x / orig_w
    new_y = y / orig_h

    return [new_x, new_y]

def find_best_crop_point(crop_list, per_image_outputs):
    """ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ cropê³¼ point ì°¾ê¸°"""
    top_score = -1
    top_point = None
    top_crop_id = -1

    for i, crop in enumerate(crop_list):
        if crop.get("id") == 0:  # ì¸ë„¤ì¼ ìŠ¤í‚µ
            continue
            
        crop_att_scores_np = np.array(per_image_outputs[i]['attn_scores'][0])
        crop_top_point = per_image_outputs[i]['topk_points'][0]
        crop_top_score = np.max(crop_att_scores_np)

        crop['top_point'] = {'point': crop_top_point, 'score': crop_top_score}

        if top_score < crop_top_score:
            top_score = crop_top_score
            top_point = crop_top_point
            top_crop_id = crop['id']
    
    return top_point, top_crop_id

def concat_images_vertically(image_list, vis_dir=None):
    # ì´ë¯¸ì§€ ì—´ê¸°
    images = [Image.open(img) for img in image_list]

    # ê°€ë¡œëŠ” ìµœëŒ€ ë„“ì´, ì„¸ë¡œëŠ” í•©ì‚°
    widths = [img.width for img in images]
    heights = [img.height for img in images]

    total_height = sum(heights)
    max_width = max(widths)

    # ìƒˆë¡œìš´ ìº”ë²„ìŠ¤ ìƒì„±
    new_img = Image.new("RGB", (max_width, total_height), (255, 255, 255))

    # ê° ì´ë¯¸ì§€ë¥¼ ì°¨ë¡€ë¡œ ë¶™ì´ê¸°
    y_offset = 0
    for img in images:
        new_img.paste(img, (0, y_offset))
        y_offset += img.height
    if vis_dir:
        # ê²°ê³¼ ì €ì¥
        new_img.save(f"{vis_dir}/concat.png")
    return new_img

def create_collage_crop_list(crop_list: List, vis_dir=None):
    new_crop_list = []

    thumbnail_crop = next((crop for crop in crop_list if crop['id'] == 0), None)
    crops_to_concat = deepcopy(crop_list)[1:]


      # 2) í¬ê¸° ì •ë³´
    widths = [crop['resized_img'].width for crop in crops_to_concat]
    heights = [crop['resized_img'].height for crop in crops_to_concat]
    canvas_w = max(widths)
    canvas_h = sum(heights)

    background=(255, 255, 255)
    collage_canvas = Image.new("RGB", (canvas_w, canvas_h), background)


    # 5) ë¶™ì´ê¸° + bbox ê³„ì‚°
    x = 0
    y = 0
    # for idx, im in enumerate(used):
    for idx, crop in enumerate(crops_to_concat):
        crop_img = crop['resized_img']
        collage_canvas.paste(crop_img, (x, y))

        # bbox (xyxy: right/bottom exclusive)
        x_min, y_min = x, y
        x_max, y_max = x + crop_img.width, y + crop_img.height

        crop['collage_bbox'] = [x_min, y_min, x_max, y_max]

        y += crop_img.height

    max_id = max(crop['id'] for crop in crop_list)
    collage_crop = {
      # 'img': collage_canvas
      'id': max_id + 1,
      'collage_crop': True,
      'resized_img': collage_canvas,
      'used_crops': crops_to_concat
    }

    new_crop_list = [thumbnail_crop, collage_crop]
    


    if vis_dir is not None:
        os.makedirs(vis_dir, exist_ok=True)
        for c in new_crop_list:
          c['resized_img'].save(f"{vis_dir}/collage_crop_{c['id']}.png")


    # print(new_crop_list)
    return new_crop_list


def create_vanilla_conversation(image, instruction):
    """vanilla inferenceë¥¼ ìœ„í•œ ë‹¨ì¼ ì´ë¯¸ì§€ conversation ìƒì„±"""
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def run_vanilla_inference(image, instruction, gt_bbox):
    """ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ vanilla inference ìˆ˜í–‰"""
    conversation = create_vanilla_conversation(image, instruction)
    
    try:
        pred = inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=3)
        
        # ìµœê³  ì ìˆ˜ í¬ì¸íŠ¸ ì¶”ì¶œ
        if pred["topk_points"] and len(pred["topk_points"]) > 0:
            px, py = pred["topk_points"][0]
            w, h = image.size
            corrected_point = [px * w, py * h]
            is_success = point_in_bbox(corrected_point, gt_bbox)
            return is_success
        else:
            return False
            
    except Exception as e:
        print(f"Vanilla inference error: {e}")
        return False

def run_refinement_pass_with_guiactor(crop_list: List, instruction: str, original_image: Image, save_dir: str, gt_bbox: List, img_path: str):
    """Stage 2: ì„ íƒëœ cropë“¤ë¡œ ìµœì¢… grounding ìˆ˜í–‰"""
    
    # Stage 2 ìš© ë¦¬ì‚¬ì´ì¦ˆ
    s2_resized_crop_list = resize_crop_list(crop_list=crop_list, ratio=S2_RESIZE_RATIO)

    if SET_OF_MARK:
        s2_resized_crop_list = apply_som(
            crop_list=s2_resized_crop_list, 
            thumbnail_resize_ratio=THUMBNAIL_RESIZE_RATIO, 
            vis_dir=s2_dir + "/som"
        )
    # for c in s2_resized_crop_list:
    #     if c['id'] != 0:
    collage_crop_list = create_collage_crop_list(crop_list=s2_resized_crop_list, vis_dir=f"{save_dir}/collage")

    s2_msgs = create_guiactor_msgs(crop_list=collage_crop_list, instruction=instruction)
    

    # crop í•©ì¹˜ê¸°, bbox ë©ì–´ë¦¬ ì¶”ê°€



    # Inference
    pred = multi_image_inference(s2_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    per_image_outputs = pred["per_image"]
    
    # ìµœê³  ì ìˆ˜ crop ì°¾ê¸°
    # top_point, top_crop_id = find_best_crop_point(collage_crop_list, per_image_outputs)
    top_point, _ = find_best_crop_point(collage_crop_list, per_image_outputs)
    
    if top_point is None:
        return False
    
    # crop bboxë“¤ì„ ë‹¤ concat cropë‚´ ì¢Œí‘œê³„ë¡œ ë³€í™˜ - A
    # ì €ì¥í•  ë•Œ ì›ë³¸ crop id, bbox ê°™ì´ ì €ì¥
    """
    {id, resized_img, used_crops}
    """
    ######
    # concat crop í¬ì¸íŠ¸ë¥¼ ì¼ë‹¨ denorm
    collage_crop = max(collage_crop_list, key=lambda x: x['id'])
    collage_w, collage_h = collage_crop['resized_img'].size
    denorm_collage_top_point = denorm_point(norm_point=top_point, orig_w=collage_w, orig_h=collage_h)
    
    top_crop = None
    for used_crop in collage_crop['used_crops']:
        collage_bbox = used_crop['collage_bbox']
        if point_in_bbox(denorm_collage_top_point, collage_bbox):
            top_crop = used_crop
            break

    
    crop_denorm_top_point = abs_point_to_crop_point(
      abs_point=denorm_collage_top_point, 
      crop_origin=(top_crop['collage_bbox'][0], top_crop['collage_bbox'][1])
    )


    collage_top_crop_w = top_crop['collage_bbox'][2] - top_crop['collage_bbox'][0]
    collage_top_crop_h = top_crop['collage_bbox'][3] - top_crop['collage_bbox'][1]
    crop_norm_top_point = norm_point(
      point=crop_denorm_top_point, 
      orig_w=collage_top_crop_w, 
      orig_h=collage_top_crop_h
    )
    # ë³€í™˜ëœ bbox(A) ë‚´ ì¢Œí‘œê³„ë¡œ ë³€í™˜ - B
    # Bë¥¼ normalize -> return
    # ì¢Œí‘œë¥¼ í¬ë¡­ ì¢Œí‘œê³„ë¡œ
    # í¬ë¡­ ì¢Œí‘œê³„ë¥¼ ì˜¤ë¦¬ì§€ë‚  ì¢Œí‘œê³„ë¡œ

    ######

    # ì›ë³¸ cropì—ì„œ bbox ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    # top_crop = next((c for c in crop_list if c['id'] == top_crop_id), None)
    # if top_crop is None:
    #     return False
        
    # top_crop_bbox = top_crop["bbox"]
    
    # ì¢Œí‘œ ë³´ì • ë° ì„±ê³µ ì—¬ë¶€ íŒë‹¨
    corrected_point = denormalize_crop_point(
        point_in_crop=crop_norm_top_point, 
        crop_size=top_crop['img'].size, 
        crop_bbox=top_crop['bbox']
    )
    is_success = point_in_bbox(corrected_point, gt_bbox)

    # ì‹œê°í™” (í•„ìš”ì‹œ)
    if STAGE2_VIS:
        _visualize_stage2_results(save_dir, collage_crop_list, pred, gt_bbox, corrected_point, instruction, img_path)
        
    return is_success

def point_in_bbox(point, bbox):
    """
    point: (x, y)
    bbox: (left, top, right, bottom)
    ê²½ê³„ í¬í•¨
    """
    x, y = point
    l, t, r, b = bbox
    return (l <= x <= r) and (t <= y <= b)

def monitor_memory(interval=0.1):
    start = time.time()
    while not stop_flag:
        mem = torch.cuda.memory_allocated() / 1024**3
        now = time.time() - start
        mem_log.append(mem)
        time_log.append(now)
        time.sleep(interval)

def apply_som(crop_list: List, thumbnail_resize_ratio: float, vis_dir=None):


    def _color_for_id(idx: int) -> Tuple[int, int, int]:
        """
        crop id(>0)ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ìƒ‰ìƒì— ë§¤í•‘.
        í•„ìš”í•œ ë§Œí¼ ìˆœí™˜ ì‚¬ìš©.
        """
        palette = [
            (230, 25, 75),   # red
            (60, 180, 75),   # green
            (0, 130, 200),   # blue
            (245, 130, 48),  # orange
            (145, 30, 180),  # purple
            (70, 240, 240),  # cyan
        ]
        # idëŠ” 1ë¶€í„° ì‹œì‘í•œë‹¤ê³  ê°€ì • (0ì€ ì¸ë„¤ì¼)
        return palette[(idx - 1) % len(palette)]

    def _clamp_bbox(b: List, W: int, H: int) -> List:
        """ì´ë¯¸ì§€ ê²½ê³„ ì•ˆìœ¼ë¡œ bboxë¥¼ ì •ìˆ˜ë¡œ ë³´ì •"""
        l = max(0, min(int(round(b[0])), W - 1))
        t = max(0, min(int(round(b[1])), H - 1))
        r = max(0, min(int(round(b[2])), W - 1))
        btm = max(0, min(int(round(b[3])), H - 1))
        if r < l:
            l, r = r, l
        if btm < t:
            t, btm = btm, t
        return l, t, r, btm

    # ì¸ë„¤ì¼ì— bbox ê·¸ë¦¬ê¸°
    # ì´ë•Œ sub cropìˆëŠ” ê±°ë§Œ ê·¸ë¦¬ê¸°

    # subcrop ëŒë©´ì„œ ì¸ë„¤ì¼ì— ê·¸ë¦¬ê¸°
    # 

    # sub cropì— bbox ê·¸ë¦¬ê¸°

    thumb_item = next((it for it in crop_list if it.get("id") == 0), None)

     # ì¸ë„¤ì¼ì— bbox ê·¸ë¦¬ê¸° (sub crop ìˆëŠ” ê²ƒë§Œ)
    thumb_img = thumb_item["resized_img"]
    # ë³µì‚¬í•´ì„œ ë®ì–´ì“°ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ í•œ ì¤„ í™œì„±í™”:
    # thumb_img = thumb_img.copy()

    W, H = thumb_img.size
    draw_thumb = ImageDraw.Draw(thumb_img)
    thumb_line_w = max(2, int(round(min(W, H) * 0.006)))  # ì¸ë„¤ì¼ì—ì„œëŠ” ì¡°ê¸ˆ ë” ë‘ê»ê²Œ(ì•½ 0.6%)

    for it in crop_list:
        cid = it.get("id")
        if cid is None or cid == 0:
            continue  # ì¸ë„¤ì¼ ìì‹ ì€ ìŠ¤í‚µ
        bbox = it.get("bbox")
        if bbox is None:
            continue

        color = _color_for_id(cid)
        s = thumbnail_resize_ratio
        scaled = (bbox[0] * s, bbox[1] * s, bbox[2] * s, bbox[3] * s)
        l, t, r, btm = _clamp_bbox(scaled, W, H)
        # l, t, r, btm = _clamp_bbox(bbox, W, H)
        draw_thumb.rectangle([l, t, r, btm], outline=color, width=thumb_line_w)

    # ìˆ˜ì •ëœ ì¸ë„¤ì¼ ë°˜ì˜
    thumb_item["resized_img"] = thumb_img

    # ê° sub crop ì´ë¯¸ì§€ì— ë™ì¼ ìƒ‰ìƒ í…Œë‘ë¦¬ ê·¸ë¦¬ê¸°
    for it in crop_list:
        cid = it.get("id")
        if cid is None or cid == 0 or it.get("resized_img") is None:
            continue
        subimg = it["resized_img"]
        # ë³µì‚¬í•´ì„œ ë®ì–´ì“°ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ í•œ ì¤„ í™œì„±í™”:
        # subimg = subimg.copy()
        w, h = subimg.size
        crop_line_w = max(2, int(round(min(w, h) * 0.015)))  # ì•½ 1.5%
        draw = ImageDraw.Draw(subimg)
        draw.rectangle([0, 0, w - 1, h - 1], outline=_color_for_id(cid), width=crop_line_w)
        it["resized_img"] = subimg

    if vis_dir is not None:
        os.makedirs(vis_dir, exist_ok=True)
        for c in crop_list:
            c['resized_img'].save(f"{vis_dir}/crop_{c['id']}.png")
        


    return crop_list


def apply_collage(crop_list: List ):
    # ì´ë¯¸ì§€ë¥¼ í•©ì¹˜ê¸°
    # 
    return



#! ================================================================================================

if __name__ == '__main__':

    set_seed(SEED)

    # Model Import (NVIDIA CUDA)
    model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
        device_map="balanced",  # NVIDIA GPU
        max_memory=max_memory, 
        low_cpu_mem_usage=True
    )
    # Model Import (Mac)
    # model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
    #     MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
    #     device_map="mps", # Mac
    #     # max_memory=max_memory, 
    #     low_cpu_mem_usage=False
    # )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH, max_pixels=MAX_PIXELS)

    if TFOPS_PROFILING:
        prof = FlopsProfiler(model)

    # save_dir í´ë”ëª…ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„± (save_dir -> save_dir_1 -> save_dir_2)
    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    warm_up_model(model, tokenizer, processor)

    # Process
    for task in TASKS:
        # ê° taskë³„ë¡œ ë³„ë„ì˜ ë¡œê·¸ íŒŒì¼ ìƒì„±
        init_iter_logger(  
            save_dir=save_dir,
            csv_name=f"iter_log_{task}.csv",
            md_name=f"iter_log_{task}.md",
            headers=[  # ìˆœì„œ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°
                "idx", "crop_time", "num_crop", "early_exit", "num_selected_crop", 
                "s1_time", "s1_tflops", "s1_hit", "s2_time", "s2_tflops", "s2_hit", 
                "total_time", "total_flops_tflops", "peak_memory_gb", "acc_uptonow", 
                "filename", "instruction"
            ],
            write_md=True, use_fsync=True, use_lock=True
        )
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"
        screenspot_data = json.load(open(os.path.join(SCREENSPOT_JSON, dataset), 'r'))
        screenspot_data = screenspot_data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        # í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        task_res = []
        num_action = 0
        s0_time_sum = s1_time_sum = s2_time_sum = total_flops = 0.0
        early_exit_count = early_exit_success_count = final_success_count = stage1_success_count = 0
        peak_memory_sum = 0.0  # í”¼í¬ ë©”ëª¨ë¦¬ í•©ê³„ ì¶”ê°€
        
        # data_sourceë³„ í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        data_source_stats = {}

        if MEMORY_VIS:
            memory_dir = os.path.join(save_dir, "gpu_usage", task)
            os.makedirs(memory_dir, exist_ok=True)

        for j, item in tqdm(enumerate(screenspot_data)):

            if MEMORY_EVAL:
                mem_log = []
                time_log = []
                stop_flag = False
                monitor_thread = threading.Thread(target=monitor_memory, daemon=True)
                monitor_thread.start()
                torch.cuda.reset_peak_memory_stats()

            s1_tflops = s2_tflops = total_flops_this = 0.0

            print("\n\n----------------------\n")

            num_action += 1
            
            # íŒŒì¼ ë° ë°ì´í„° ë¡œë“œ
            filename = item["img_filename"]
            filename_wo_ext, ext = os.path.splitext(filename)
            img_path = os.path.join(SCREENSPOT_IMGS, filename)
            if not os.path.exists(img_path):
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["instruction"]
            original_bbox = item["bbox"]
            original_bbox = [original_bbox[0], original_bbox[1], 
                           original_bbox[0] + original_bbox[2], original_bbox[1] + original_bbox[3]]

            orig_w, orig_h = original_image.size
            
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì²˜ë¦¬
            # if MAX_PIXELS is not None and orig_w * orig_h > MAX_PIXELS:
            #     resized_image, w_resized, h_resized = resize_image(original_image)
            #     # bboxë„ ë¦¬ì‚¬ì´ì¦ˆ ë¹„ìœ¨ì— ë§ì¶° ìŠ¤ì¼€ì¼ë§
            #     resize_ratio = (w_resized * h_resized) ** 0.5 / (orig_w * orig_h) ** 0.5
            #     scaled_bbox = [int(coord * resize_ratio) for coord in original_bbox]
            # else:
                # ë¦¬ì‚¬ì´ì¦ˆê°€ í•„ìš”ì—†ëŠ” ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            resized_image = original_image
            w_resized, h_resized = orig_w, orig_h
            resize_ratio = 1.0
            scaled_bbox = original_bbox
                
            # data_source ì •ë³´ ì¶”ì¶œ (ì—†ìœ¼ë©´ "unknown"ìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •)
            data_source = item.get("data_source", "unknown")
            
            # data_sourceë³„ ì²˜ë¦¬ ì˜µì…˜ -> í° (ì„¸ë¡œí™”ë©´) = cropí•  ë•Œ ì„¸ë¡œë¶„í•  ì•ˆí•¨
            skip_vertical_split = data_source in ["ios", "android"]
            
            # ë””ë ‰í† ë¦¬ ì„¤ì • (ì‹œê°í™”ìš© - í•„ìš”ì‹œì—ë§Œ)
            if any([STAGE0_VIS, STAGE1_VIS, STAGE2_VIS]):
                inst_dir_name = re.sub(r'\W+', '_', instruction).strip('_')
                inst_dir = os.path.join(save_dir, "seg", filename_wo_ext, inst_dir_name)
                s1_dir = os.path.join(inst_dir, "stage1") 
                s2_dir = os.path.join(inst_dir, "stage2")
                os.makedirs(s1_dir, exist_ok=True)
                os.makedirs(s2_dir, exist_ok=True)
            else:
                inst_dir = s1_dir = s2_dir = None

            #! ==================================================================
            #! Stage 0 | Segmentation
            #! ==================================================================

            seg_start = time.time()

            # resize í•˜ì§€ ì•Šì€ ì›ë³¸ì˜ í™”ì§ˆì„ ê¸°ì¤€ìœ¼ë¡œ crop íšŸìˆ˜ ì •í•˜ê¸°
            if orig_h < 1000:  # ì €í™”ì§ˆì´ë‚˜ ê°€ë¡œí™”ë©´ -> 2ë“±ë¶„
                h_cuts = 1
                h_tolerance = 0.20
            elif orig_h < 1440:  # ì¤‘ê°„í™”ì§ˆ -> 3ë“±ë¶„
                h_cuts = 2
                h_tolerance = 0.12
            else:  # ê³ í™”ì§ˆì´ë‚˜ ì„¸ë¡œí™”ë©´ -> 4ë“±ë¶„
                h_cuts = 3
                h_tolerance = 0.08
            crop_list = run_crop(orig_img=resized_image, h_cuts=h_cuts, h_tolerance=h_tolerance)  # resizeëœê±¸ë¡œ crop
            s0_crop_list = resize_crop_list(crop_list=crop_list, ratio=S1_RESIZE_RATIO)
            seg_end = time.time()
            seg_time = seg_end - seg_start

            if STAGE0_VIS and inst_dir:
                all_crops_bboxes = [crop["bbox"] for crop in s0_crop_list]
                visualize_crop(save_dir=inst_dir, gt_bbox=scaled_bbox, top_q_bboxes=all_crops_bboxes,
                                instruction=instruction, filename="s1_all_crop.png", img_path=img_path, click_point=None)

            #! ==================================================================
            #! Stage 1 | Find Top Q + Inference
            #! ==================================================================

            # Calculate Stage 1 FLOPs

            if TFOPS_PROFILING:
                prof.start_profile()

            s1_start = time.time()

            if SET_OF_MARK:
                s0_crop_list = apply_som(
                    crop_list=s0_crop_list, 
                    thumbnail_resize_ratio=THUMBNAIL_RESIZE_RATIO, 
                    vis_dir=s1_dir + "/som"
                    )

            s1_msgs = create_guiactor_msgs(crop_list=s0_crop_list, instruction=instruction)
            

            
            
            # s1_top_q_crop_ids, s1_top_q_bboxes, s0_crop_list_out, should_exit_early, early_exit_success = run_selection_pass_with_guiactor(
            #     msgs=s1_msgs,
            #     crop_list=s0_crop_list,
            #     gt_bbox=scaled_bbox,  # ìŠ¤ì¼€ì¼ëœ bbox ì‚¬ìš©
            #     attn_vis_dir=s1_dir or "",
            #     original_image=resized_image,
            #     img_path=img_path,
            #     instruction=instruction
            # )

            selected_y_ranges, should_exit_early, early_exit_success, use_vanilla, num_selected_crops = run_selection_pass_with_guiactor(
                msgs=s1_msgs,
                crop_list=s0_crop_list,
                gt_bbox=scaled_bbox,  # ìŠ¤ì¼€ì¼ëœ bbox ì‚¬ìš©
                attn_vis_dir=s1_dir or "",
                original_image=resized_image,
                img_path=img_path,
                instruction=instruction
            )

            s1_infence_end = time.time()
            s1_time = s1_infence_end - s1_start

            if TFOPS_PROFILING:
                prof.stop_profile()
                s1_tflops = prof.get_total_flops()
                s1_tflops /= 1e12

            if should_exit_early:
                print(f"âœ‚ï¸  Crops : 1 | ğŸš€ Early Exit")
                early_exit_count +=1
                if early_exit_success:
                    s1_hit = "âœ…ğŸš€"
                    early_exit_success_count += 1
                else:
                    s1_hit = "âŒğŸš€"
                stage1_success = False  # Early exitì´ë¯€ë¡œ stage1 ì„±ê³µ ì—¬ë¶€ ë¯¸ì •ì˜

            elif use_vanilla:
                # Vanilla inference ì‚¬ìš© (ëª¨ë“  í¬ë¡­ ì„ íƒë¨)
                stage1_success = True  # ëª¨ë“  í¬ë¡­ì´ ì„ íƒë˜ì—ˆìœ¼ë¯€ë¡œ í•­ìƒ ì„±ê³µ
                s1_hit = "âœ…ğŸŒ"  # vanilla í‘œì‹œ
                stage1_success_count += 1

            else:  # GTê°€ ì•ˆì— ë“¤ì–´ê°€ëŠ”ì§€ ì²´í¬
                stage1_success = check_gt_in_selected_y_ranges(selected_y_ranges, scaled_bbox, w_resized)
                s1_hit = "âœ…" if stage1_success else "âŒ"
                if stage1_success:
                    stage1_success_count += 1

            # ë¶ˆí•„ìš”í•œ ë”•ì…”ë„ˆë¦¬ ì—°ì‚° ì œê±° - ê²°ê³¼ ì €ì¥ìš©ë„ë§Œ
            # res_board_dictëŠ” ì‚¬ì‹¤ìƒ ë¯¸ì‚¬ìš©
            
            #! ==================================================================
            #! [Stage 2] Attention Refinement Pass
            #! ==================================================================
            
            # Early Exit
            s2_tflops = 0.0
            if should_exit_early:
                final_success = early_exit_success
                s2_time = 0.0
                s2_tflops = 0.0
            elif use_vanilla:
                # Vanilla inference ì‚¬ìš© (ëª¨ë“  í¬ë¡­ ì„ íƒë¨)
                if TFOPS_PROFILING:
                    prof.start_profile()
                s2_inference_start = time.time()
                
                final_success = run_vanilla_inference(resized_image, instruction, scaled_bbox)
                
                s2_inference_end = time.time()
                s2_time = s2_inference_end - s2_inference_start
                if TFOPS_PROFILING:
                    prof.stop_profile()
                    s2_tflops = prof.get_total_flops()
                    s2_tflops /= 1e12
            else:
                # Y ë²”ìœ„ë“¤ë¡œë¶€í„° Stage2ìš© crop ìƒì„±
                s2_input_crops = create_stage2_crops_from_y_ranges(selected_y_ranges, resized_image)

                # Calculate Stage 2 FLOPs
                s2_resized_crops = resize_crop_list(crop_list=s2_input_crops, ratio=S2_RESIZE_RATIO)
                s2_msgs = create_guiactor_msgs(crop_list=s2_resized_crops, instruction=instruction)

                if TFOPS_PROFILING:
                    prof.start_profile()
                s2_inference_start = time.time()

                final_success = run_refinement_pass_with_guiactor(
                    crop_list=s2_input_crops,
                    instruction=instruction,
                    original_image=resized_image,
                    save_dir=s2_dir or "",
                    gt_bbox=scaled_bbox,  # ìŠ¤ì¼€ì¼ëœ bbox ì‚¬ìš©
                    img_path=img_path
                )
                s2_inference_end = time.time()
                s2_time = s2_inference_end - s2_inference_start
                if TFOPS_PROFILING:
                    prof.stop_profile()
                    s2_tflops = prof.get_total_flops()
                    s2_tflops /= 1e12
        

            #! ==================================================================
            #! [Common Processing]
            #! ==================================================================
            
            # ê³µí†µ í†µê³„ ì—…ë°ì´íŠ¸
            s0_time_sum += seg_time
            s1_time_sum += s1_time
                
            # ì„±ëŠ¥ ë¡œê¹…
            total_time = seg_time + s1_time + s2_time
            if TFOPS_PROFILING:
                total_flops_this = s1_tflops + (s2_tflops if not should_exit_early else 0)
                total_flops += total_flops_this

            if len(s0_crop_list) != 2 and not should_exit_early:
                if use_vanilla:
                    print(f"âœ‚ï¸  Crops : {len(s0_crop_list)-1} | ğŸŒ Vanilla Inference (All {num_selected_crops} crops selected)")
                else:
                    print(f"âœ‚ï¸  Crops : {len(s0_crop_list)-1} | Select Crops : {num_selected_crops} â†’ Y-ranges : {len(selected_y_ranges)}")
            print(f"ğŸ•– Times - Seg: {seg_time:.2f}s | S1: {s1_time:.2f}s | S2: {s2_time:.2f}s | Total: {total_time:.2f}s")
            if TFOPS_PROFILING:
                print(f"ğŸ”¥ FLOPs - S1: {s1_tflops:.2f} | S2: {s2_tflops:.2f} | Total: {total_flops_this:.2f} TFLOPs")
            print(f"{'âœ…ğŸš¨ Early Exit Success' if should_exit_early and early_exit_success else 'âŒğŸš¨ Early Exit Fail' if should_exit_early else 'âœ…ğŸŒ Vanilla Success' if use_vanilla and final_success else 'âŒğŸŒ Vanilla Fail' if use_vanilla else 'âœ… Grounding Success' if final_success else 'âŒ Grounding Fail'}")

         
            #! ==================================================================
            #! [End]
            #! ==================================================================

            if MEMORY_EVAL:
                time.sleep(0.1)
                stop_flag = True
                monitor_thread.join()

                # í”¼í¬ ë©”ëª¨ë¦¬ ê³„ì‚° (GB ë‹¨ìœ„, ì†Œìˆ˜ì  3ìë¦¬)
                peak_memory = max(mem_log) if mem_log else 0.0
                peak_memory_gb = round(peak_memory, 3)

                if MEMORY_VIS:
                    plt.figure(figsize=(10, 4))
                    plt.plot(time_log, mem_log)
                    plt.xlabel("Time (s)")
                    plt.ylabel("GPU Memory Allocated (GB)")
                    plt.title("GPU Memory Usage Over Time")
                    plt.grid(True)
                    plt.savefig(f"{memory_dir}/{num_action}_{filename}")
                    plt.close()  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ close ì¶”ê°€

            s2_time_sum += s2_time
            final_success_count += final_success
            if MEMORY_EVAL:
                peak_memory_sum += peak_memory_gb

            # data_sourceë³„ í†µê³„ ì—…ë°ì´íŠ¸
            if data_source not in data_source_stats:
                data_source_stats[data_source] = {
                    'num_action': 0,
                    's0_time_sum': 0.0,
                    's1_time_sum': 0.0,
                    's2_time_sum': 0.0,
                    'total_flops': 0.0,
                    'early_exit_count': 0,
                    'early_exit_success_count': 0,
                    'final_success_count': 0,
                    'stage1_success_count': 0,
                    'peak_memory_sum': 0.0  # í”¼í¬ ë©”ëª¨ë¦¬ í•©ê³„ ì¶”ê°€
                }
            
            stats = data_source_stats[data_source]
            stats['num_action'] += 1
            stats['s0_time_sum'] += seg_time
            stats['s1_time_sum'] += s1_time
            stats['s2_time_sum'] += s2_time
            if TFOPS_PROFILING:
                stats['total_flops'] += total_flops_this
            if MEMORY_EVAL:
                stats['peak_memory_sum'] += peak_memory_gb
            if should_exit_early:
                stats['early_exit_count'] += 1
                if early_exit_success:
                    stats['early_exit_success_count'] += 1
            else:
                # Early exitì´ ì•„ë‹ ë•Œë§Œ stage1 ì„±ê³µ ì¹´ìš´íŠ¸
                if stage1_success:
                    stats['stage1_success_count'] += 1
            if final_success:
                stats['final_success_count'] += 1

            up2now_gt_score = final_success_count / num_action * 100
            print(f"Up2Now Grounding Accuracy: {up2now_gt_score}%")

            # Iter log - ê°œì„ ëœ ë¡œê¹…
            append_iter_log(
                idx=j+1,
                filename=filename_wo_ext,
                instruction=instruction[:50] + "..." if len(instruction) > 50 else instruction,
                crop_time=f"{seg_time:.3f}",
                num_crop=len(s0_crop_list)-1,
                early_exit="â˜‘ï¸" if should_exit_early else "ğŸ«¥",
                num_selected_crop=num_selected_crops if not should_exit_early else 0,
                s1_time=f"{s1_time:.3f}",
                s1_tflops=f"{s1_tflops:.2f}",
                s1_hit=s1_hit,
                s2_time=f"{s2_time:.3f}",
                s2_tflops=f"{s2_tflops:.2f}" if not should_exit_early else "0.00",
                s2_hit="âœ…" if final_success else "âŒ",
                total_time=f"{total_time:.3f}",
                total_flops_tflops=f"{total_flops_this:.2f}",
                peak_memory_gb=f"{peak_memory_gb:.3f}" if MEMORY_EVAL else "N/A",
                acc_uptonow=f"{up2now_gt_score:.2f}"
            )

            # JSON ê¸°ë¡ - í•µì‹¬ ì •ë³´ë§Œ
            item_res = {
                'filename': filename,
                'instruction': instruction,
                'gt_bbox': original_bbox,
                'data_source': data_source,
                'num_crop': len(s0_crop_list) - 1,
                'early_exit': should_exit_early,
                'early_exit_success': early_exit_success,
                'stage1_success': stage1_success if not should_exit_early else None,
                's1_hit': s1_hit,
                's2_hit': final_success,
                'seg_time': seg_time,
                's1_time': s1_time,
                's2_time': s2_time,
                'total_time': total_time,
                's1_tflops': s1_tflops,
                's2_tflops': s2_tflops if not should_exit_early else 0,
                'total_flops': total_flops_this,
                'peak_memory_gb': peak_memory_gb if MEMORY_EVAL else None
            }
            task_res.append(item_res)

        #! ==================================================
        # Json ì •ë¦¬
        with open(os.path.join(save_dir, dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        # ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        non_early_exit_samples = num_action - early_exit_count
        metrics = {
            "task": task,
            "total_samples": num_action,
            "stage1_accuracy": stage1_success_count / non_early_exit_samples * 100 if non_early_exit_samples > 0 else 0,
            "accuracy": final_success_count / num_action * 100,
            "early_exit_rate": early_exit_count / num_action * 100,
            "early_exit_success_rate": early_exit_success_count / early_exit_count * 100 if early_exit_count > 0 else 0,
            "avg_times": {
                "segmentation": s0_time_sum / num_action,
                "stage1": s1_time_sum / num_action,
                "stage2": s2_time_sum / num_action,
                "total": (s0_time_sum + s1_time_sum + s2_time_sum) / num_action
            },
            "avg_flops_tflops": total_flops / num_action,
            "avg_peak_memory_gb": round(peak_memory_sum / num_action, 3) if MEMORY_EVAL else None,
            "hyperparameters": {
                "max_pixels": MAX_PIXELS,
                "select_threshold": SELECT_THRESHOLD,
                "early_exit": EARLY_EXIT,
                "early_exit_thre": EARLY_EXIT_THRE,
                "s1_resize_ratio": S1_RESIZE_RATIO,
                "s2_resize_ratio": S2_RESIZE_RATIO,
                "thumbnail_resize_ratio" : THUMBNAIL_RESIZE_RATIO,
                "attn_impl" : ATTN_IMPL
            }
        }

        #! ==================================================
        # Json ì •ë¦¬
        with open(os.path.join(save_dir, dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        # ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        non_early_exit_samples = num_action - early_exit_count
        metrics = {
            "task": task,
            "total_samples": num_action,
            "stage1_accuracy": stage1_success_count / non_early_exit_samples * 100 if non_early_exit_samples > 0 else 0,
            "accuracy": final_success_count / num_action * 100,
            "early_exit_rate": early_exit_count / num_action * 100,
            "early_exit_success_rate": early_exit_success_count / early_exit_count * 100 if early_exit_count > 0 else 0,
            "avg_times": {
                "segmentation": s0_time_sum / num_action,
                "stage1": s1_time_sum / num_action,
                "stage2": s2_time_sum / num_action,
                "total": (s0_time_sum + s1_time_sum + s2_time_sum) / num_action
            },
            "avg_flops_tflops": total_flops / num_action,
            "avg_peak_memory_gb": round(peak_memory_sum / num_action, 3) if MEMORY_EVAL else None,
            "hyperparameters": {
                "max_pixels": MAX_PIXELS,
                "select_threshold": SELECT_THRESHOLD,
                "early_exit": EARLY_EXIT,
                "early_exit_thre": EARLY_EXIT_THRE,
                "s1_resize_ratio": S1_RESIZE_RATIO,
                "s2_resize_ratio": S2_RESIZE_RATIO,
                "thumbnail_resize_ratio" : THUMBNAIL_RESIZE_RATIO,
                "attn_impl" : ATTN_IMPL
            }
        }

        with open(os.path.join(save_dir, f"results_{task}.json"), "w") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=4)

        # data_sourceë³„ ë©”íŠ¸ë¦­ ì €ì¥
        data_source_metrics = {}
        for ds, stats in data_source_stats.items():
            if stats['num_action'] > 0:
                non_early_exit_samples = stats['num_action'] - stats['early_exit_count']
                data_source_metrics[ds] = {
                    "task": task,
                    "data_source": ds,
                    "total_samples": stats['num_action'],
                    "stage1_accuracy": stats['stage1_success_count'] / non_early_exit_samples * 100 if non_early_exit_samples > 0 else 0,
                    "accuracy": stats['final_success_count'] / stats['num_action'] * 100,
                    "early_exit_rate": stats['early_exit_count'] / stats['num_action'] * 100,
                    "early_exit_success_rate": stats['early_exit_success_count'] / stats['early_exit_count'] * 100 if stats['early_exit_count'] > 0 else 0,
                    "avg_times": {
                        "segmentation": stats['s0_time_sum'] / stats['num_action'],
                        "stage1": stats['s1_time_sum'] / stats['num_action'],
                        "stage2": stats['s2_time_sum'] / stats['num_action'],
                        "total": (stats['s0_time_sum'] + stats['s1_time_sum'] + stats['s2_time_sum']) / stats['num_action']
                    },
                    "avg_flops_tflops": stats['total_flops'] / stats['num_action'],
                    "avg_peak_memory_gb": round(stats['peak_memory_sum'] / stats['num_action'], 3) if MEMORY_EVAL else None
                }
        
        with open(os.path.join(save_dir, f"results_{task}_source.json"), "w") as dsf:
            json.dump(data_source_metrics, dsf, ensure_ascii=False, indent=4)

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("=" * 60)
        print(f"ğŸ“Š Final Results for {task}:")
        print(f"Total Samples: {num_action}")
        print(f"Stage1 Accuracy: {metrics['stage1_accuracy']:.2f}%")
        print(f"Accuracy: {metrics['accuracy']:.2f}%")
        print(f"Early Exit Rate: {metrics['early_exit_rate']:.2f}%")
        print(f"Early Exit Success Rate: {metrics['early_exit_success_rate']:.2f}%") 
        print(f"Avg Times: Seg {metrics['avg_times']['segmentation']:.3f}s, S1 {metrics['avg_times']['stage1']:.3f}s, S2 {metrics['avg_times']['stage2']:.3f}s, Total {metrics['avg_times']['total']:.3f}s")
        print(f"Avg FLOPs: {metrics['avg_flops_tflops']:.2f} TFLOPs")
        if MEMORY_EVAL and metrics['avg_peak_memory_gb'] is not None:
            print(f"Avg Peak Memory: {metrics['avg_peak_memory_gb']:.3f} GB")
        
        # data_sourceë³„ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š Results by Data Source:")
        for ds, ds_metrics in data_source_metrics.items():
            memory_str = f", Mem: {ds_metrics['avg_peak_memory_gb']:.3f}GB" if MEMORY_EVAL and ds_metrics['avg_peak_memory_gb'] is not None else ""
            print(f"  {ds}: {ds_metrics['total_samples']} samples, S1 Acc: {ds_metrics['stage1_accuracy']:.2f}%, "
                  f"Acc: {ds_metrics['accuracy']:.2f}%, Early Exit: {ds_metrics['early_exit_rate']:.2f}%, "
                  f"TFLOPs: {ds_metrics['avg_flops_tflops']:.2f}{memory_str}")
        print("=" * 60)