'''
Final version
'''

import os
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('gpu', type=int, default=0, help='GPU number')
parser.add_argument('--r', type=float, default=0.50, help='Stage 1 Resize ratio')
parser.add_argument('--th', type=float, default=0.12, help='Stage 1 Crop threshold')
parser.add_argument('--p', type=int, default=0, help='Stage 1 Crop Padding')
parser.add_argument('--v', action='store_true', help='Whether to save visualization images')
parser.add_argument('--mac', action='store_true', help='Whether to run on Mac (MPS)')
args = parser.parse_args()

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)

#! Hyperparameter =====================================================================================

ATTN_IMPL = "eager"                      # attention implement "eager" "sdpa" "flash" "efficient"

# Image Resize Ratios
RESIZE_RATIO = args.r

# Crop Limitations
MAX_CROPS = 3  # ìµœëŒ€ crop ê°œìˆ˜

# Connected Region Based Cropping
REGION_THRESHOLD = args.th              # ì—°ê²°ëœ ì˜ì—­ ê²€ì¶œì„ ìœ„í•œ ì„ê³„ê°’ (0~1)  # TODO: 0.1 ~ 0.5 ì¤‘ ìµœì  ì°¾ê¸°
MIN_PATCHES = 1                         # ìµœì†Œ íŒ¨ì¹˜ ìˆ˜ (ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œê±°)
BBOX_PADDING = args.p                   # bbox ìƒí•˜ì¢Œìš°ë¡œ í™•ì¥í•  í”½ì…€  # TODO: 0 ~ 50 ì¤‘ ìµœì  ì°¾ê¸°

# Ensemble Hyperparameters
STAGE1_ENSEMBLE_RATIO = 0.70              # Stage1 attention weight
STAGE2_ENSEMBLE_RATIO = 1 - STAGE1_ENSEMBLE_RATIO  # Stage2 crop weight
ENSEMBLE_TOP_PATCHES = 100                # Stage2ì—ì„œ ì•™ìƒë¸”ì— ì‚¬ìš©í•  ìƒìœ„ íŒ¨ì¹˜ ê°œìˆ˜ (Qwen2.5VLìš©)

# ìµœëŒ€ PIXELS ì œí•œ
MAX_PIXELS = 3211264  # Processë‹¨ì—ì„œ ì ìš©
# MAX_PIXELS = 1280*28*28  # Processë‹¨ì—ì„œ ì ìš©

# csvì— ê¸°ë¡í•  method ì´ë¦„
method = "figure_visualize_mg"

memo = f"resize{RESIZE_RATIO:.2f}_region_thresh{REGION_THRESHOLD:.2f}_pad{BBOX_PADDING}"

# INDEX_START = 
#! Argument ==========================================================================================

SEED = 0

# Dataset & Model
MLLM_PATH = "microsoft/GUI-Actor-3B-Qwen2.5-VL"
SCREENSPOT_IMGS = "../data/screenspotv2_image"      # input image ê²½ë¡œ
SCREENSPOT_JSON = "../data"                         # input image jsoníŒŒì¼ ê²½ë¡œ
# TASKS = ["mobile", "web", "desktop"]

TASKS = ["mobile"]

# SAMPLE_RANGE = slice(None)

SAMPLE_RANGE = slice(437, 442)  # Main Figure
# SAMPLE_RANGE = slice(235, 237)  # Main Figure

# Visualize & Logging
VISUALIZE = True
VIS_ONLY_WRONG = False                          # Trueë©´ í‹€ë¦° ê²ƒë§Œ ì‹œê°í™”, Falseë©´ ëª¨ë“  ê²ƒ ì‹œê°í™”
TFOPS_PROFILING = True
MEMORY_VIS = False
# Save Path
SAVE_DIR = f"../attn_output/" + method + "/" + memo

#! ==================================================================================================

# Standard Library
import os
import sys
import time
import re
import json
import logging
logging.disable(logging.CRITICAL)  # ëª¨ë“  ë¡œê¹… í˜¸ì¶œ ë¬´ë ¥í™”
from typing import Dict, List
from collections import deque

# Third-Party Libraries
import numpy as np
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoTokenizer, set_seed

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from util.iter_logger import init_iter_logger, append_iter_log  # log csv ê¸°ë¡ íŒŒì¼
from gui_actor.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer
from gui_actor.inference import inference
from gui_actor.multi_image_inference import multi_image_inference
from util.visualize_figure_util import visualize_stage1_attention_crops, visualize_stage2_multi_attention, visualize_stage3_point_ensemble
if TFOPS_PROFILING:
    from deepspeed.profiling.flops_profiler import FlopsProfiler

#! ==============================================================================================

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        return super(NpEncoder, self).default(obj)

def warm_up_model(model, tokenizer, processor):
    print("ğŸ‹ï¸â€â™‚ï¸ Warming up the model...")
    dummy_instruction = "This is a dummy instruction for warm-up."
    dummy_image = Image.new("RGB", (1000, 1000), color=(255, 255, 255))  # 1000x1000 í°ìƒ‰ ì´ë¯¸ì§€
    dummy_msgs = create_conversation_stage1(image=dummy_image, instruction=dummy_instruction, resize_ratio=1.0)
    
    # ì˜ˆì—´ìš© inference ì‹¤í–‰
    for _ in range(3):  # 3ë²ˆ ë°˜ë³µ
        with torch.no_grad():
            _ = inference(dummy_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    print("ğŸ‹ï¸â€â™‚ï¸ Warm-up complete!")

def create_conversation_stage1(image, instruction, resize_ratio):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        # Additional prompt
                        f"This is a resized screenshot of the whole GUI, scaled by {resize_ratio}. "
                        # previous prompt
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def create_conversation_stage2(crop_list, instruction):
    user_content = []
    for crop in crop_list:
        user_content.append({"type": "image", "image": crop["img"]})
    user_content.append({
        "type": "text",
        "text": instruction,
    })
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        # Additional prompt
                        f"This is a list of {len(crop_list)} cropped screenshots of the GUI, each showing a part of the GUI. "
                        # previous prompt
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": user_content,
        },
    ]
    return conversation

def get_connected_region_bboxes_from_scores(
    image_result: Dict,
    threshold: float,
    min_patches: int = 1
) -> List[Dict]:
    '''
    ê°„ë‹¨í•œ ë²„ì „: attention * threshold ë„˜ëŠ” ë¶€ë¶„ë“¤ì„ 8ë°©í–¥ ì—°ê²°ë¡œ í•©ì³ì„œ box ìƒì„±
    '''
    # 1) ì…ë ¥ íŒŒì‹± ë° ì„ê³„ê°’ ê³„ì‚°
    attn_scores_1d = np.array(image_result["attn_scores"][0], dtype=np.float32)
    n_w = int(image_result["n_width"])
    n_h = int(image_result["n_height"])
    attn = attn_scores_1d.reshape(n_h, n_w)
    
    vmax = float(attn.max()) if attn.size > 0 else 0.0
    thr_val = float(vmax * threshold) if threshold <= 1.0 else float(threshold)
    
    # 2) ê¸°ì¤€ ë„˜ëŠ” íŒ¨ì¹˜ë“¤ ë§ˆìŠ¤í¬ ìƒì„±
    mask = (attn >= thr_val)
    
    # 3) BFSë¡œ ì—°ê²°ëœ ì˜ì—­ë“¤ ì°¾ê¸°
    visited = np.zeros_like(mask, dtype=bool)
    regions = []
    neighbors = [(di, dj) for di in (-1,0,1) for dj in (-1,0,1) if not (di==0 and dj==0)]  # 8ë°©í–¥
    
    for y in range(n_h):
        for x in range(n_w):
            if not mask[y, x] or visited[y, x]:
                continue
                
            # BFSë¡œ ì—°ê²°ëœ ì˜ì—­ ì°¾ê¸°
            region = [(y, x)]
            queue = deque([(y, x)])
            visited[y, x] = True
            
            while queue:
                cy, cx = queue.popleft()
                for dy, dx in neighbors:
                    ny, nx = cy + dy, cx + dx
                    if (0 <= ny < n_h and 0 <= nx < n_w and 
                        mask[ny, nx] and not visited[ny, nx]):
                        visited[ny, nx] = True
                        queue.append((ny, nx))
                        region.append((ny, nx))
            
            if len(region) >= min_patches:
                regions.append(region)
    
    # 4) ê° ì˜ì—­ì˜ bboxì™€ ì ìˆ˜ ê³„ì‚°
    out = []
    for region in regions:
        ys = [p[0] for p in region]
        xs = [p[1] for p in region]
        y_min, y_max = min(ys), max(ys)
        x_min, x_max = min(xs), max(xs)
        
        # ì •ê·œí™”ëœ bbox
        l = x_min / n_w
        t = y_min / n_h  
        r = (x_max + 1) / n_w
        b = (y_max + 1) / n_h
        
        # ì ìˆ˜ ê³„ì‚°
        region_scores = attn[ys, xs]
        score_sum = float(region_scores.sum())
        score_mean = float(region_scores.mean())
        
        out.append({
            "bbox": [l, t, r, b],
            "patch_bbox": [int(x_min), int(y_min), int(x_max), int(y_max)],
            "size": int(len(region)),
            "score_sum": score_sum,
            "score_mean": score_mean,
            "score_norm": score_sum / (vmax * len(region) + 1e-9),
        })
    
    # 5) ì ìˆ˜ìˆœ ì •ë ¬
    out.sort(key=lambda x: x["score_sum"], reverse=True)
    return out

def run_stage1_attention_inference(original_image, instruction):
    """Stage 1: ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  inference"""
    
    orig_w, orig_h = original_image.size
    # ì´ë¯¸ì§€ ê³ ì • ë¦¬ì‚¬ì´ì¦ˆ
    resize_ratio = RESIZE_RATIO
    resized_w, resized_h = int(orig_w * resize_ratio), int(orig_h * resize_ratio)
    
    # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ë¡œ inference
    resized_image = original_image.resize((resized_w, resized_h))
    conversation = create_conversation_stage1(resized_image, instruction, resize_ratio)
    pred = inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=1)
    
    # ê²°ê³¼ì— ë¦¬ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ê°€
    pred['resize_ratio'] = resize_ratio
    pred['original_size'] = (orig_w, orig_h)
    pred['resized_size'] = resized_image.size
    
    return pred, resized_image

def find_connected_regions(pred_result, resized_image, resize_ratio):
    """ì–´í…ì…˜ì—ì„œ ì—°ê²°ëœ ì˜ì—­ë“¤ ì°¾ê¸°"""

    regions = get_connected_region_bboxes_from_scores(
        image_result=pred_result,
        threshold=REGION_THRESHOLD,
        min_patches=MIN_PATCHES
    )
    
    resized_w, resized_h = resized_image.size
    orig_w = resized_w / resize_ratio
    orig_h = resized_h / resize_ratio
    
    # ê° ì˜ì—­ì„ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜í•˜ê³  ì •ë³´ êµ¬ì„±
    connected_regions = []
    for i, region in enumerate(regions):
        # ì •ê·œí™”ëœ bboxë¥¼ ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        l, t, r, b = region["bbox"]  # ì •ê·œí™”ëœ ì¢Œí‘œ (0~1)
        
        # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ì—ì„œì˜ í”½ì…€ ì¢Œí‘œ
        resized_left = l * resized_w
        resized_top = t * resized_h
        resized_right = r * resized_w
        resized_bottom = b * resized_h
        
        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜
        orig_left = resized_left / resize_ratio
        orig_top = resized_top / resize_ratio
        orig_right = resized_right / resize_ratio
        orig_bottom = resized_bottom / resize_ratio
        
        # bboxì— íŒ¨ë”© ì ìš©
        padded_left = max(0, int(orig_left - BBOX_PADDING))
        padded_top = max(0, int(orig_top - BBOX_PADDING))
        padded_right = min(orig_w, int(orig_right + BBOX_PADDING))
        padded_bottom = min(orig_h, int(orig_bottom + BBOX_PADDING))
        
        # ì˜ì—­ ì¤‘ì‹¬ì  ê³„ì‚° (íŒ¨ë”© ì ìš© ì „ bbox ê¸°ì¤€)
        center_x = (orig_left + orig_right) / 2
        center_y = (orig_top + orig_bottom) / 2
        
        connected_regions.append({
            'center_x': center_x,
            'center_y': center_y,
            'score': region["score_sum"],  # ì˜ì—­ ë‚´ ì ìˆ˜ í•©
            'score_mean': region["score_mean"],  # ì˜ì—­ ë‚´ ì ìˆ˜ í‰ê· 
            'size': region["size"],  # íŒ¨ì¹˜ ìˆ˜
            'bbox_original': [int(orig_left), int(orig_top), int(orig_right), int(orig_bottom)],  # íŒ¨ë”© ì „ bbox
            'bbox_padded': [padded_left, padded_top, padded_right, padded_bottom],  # íŒ¨ë”© í›„ bbox (ì‹¤ì œ í¬ë¡­ìš©)
            'region_info': region  # ì›ë³¸ ì˜ì—­ ì •ë³´
        })
    
    connected_regions.sort(key=lambda x: x['score'], reverse=True)
    
    return connected_regions

def create_crops_from_connected_regions(regions, original_image):
    """ì—°ê²°ëœ ì˜ì—­ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ crop"""
    
    if not regions:
        return []
    
    crops = []
    
    for i, region in enumerate(regions):
        bbox = region['bbox_padded']  # íŒ¨ë”©ì´ ì ìš©ëœ bbox ì‚¬ìš©
        crop_img = original_image.crop(bbox)
        
        crops.append({
            'img': crop_img,
            'bbox': bbox,
            'score': region['score'],
            'id': i + 1,
            'region_info': region  # ì›ë³¸ ì˜ì—­ ì •ë³´ í¬í•¨
        })
    
    return crops

def run_stage2_multi_image_inference(crop_list, instruction):
    """Stage 2: multi image inference"""
    
    conversation = create_conversation_stage2(crop_list, instruction)
    pred = multi_image_inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=10)
    
    return pred

def convert_multi_image_results_to_original(multi_pred, crop_list):
    """multi_image_inference ê²°ê³¼ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜"""
    
    converted_results = []
    all_candidates = []
    
    for img_idx, img_result in enumerate(multi_pred['per_image']):
        if img_idx >= len(crop_list):
            continue
            
        crop_info = crop_list[img_idx]
        crop_bbox = crop_info['bbox']  # [left, top, right, bottom]
        crop_width = crop_bbox[2] - crop_bbox[0]
        crop_height = crop_bbox[3] - crop_bbox[1]
        
        crop_candidates = []
        for point_idx, (point, score) in enumerate(zip(img_result['topk_points'], img_result['topk_values'])):
            crop_x = point[0] * crop_width
            crop_y = point[1] * crop_height
            
            original_x = crop_bbox[0] + crop_x
            original_y = crop_bbox[1] + crop_y
            
            candidate = {
                'point': [original_x, original_y],
                'score': score,
                'crop_id': crop_info['id'],
                'crop_bbox': crop_bbox,
                'rank_in_crop': point_idx
            }
            crop_candidates.append(candidate)
            all_candidates.append(candidate)
        
        converted_results.append({
            'crop_id': crop_info['id'],
            'crop_bbox': crop_bbox,
            'candidates': crop_candidates
        })
    
    all_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    return all_candidates

def run_stage1_attention_based(original_image, instruction, gt_bbox):
    """ìƒˆë¡œìš´ ê°„ë‹¨í•œ Stage 1: ì—°ê²°ëœ ì˜ì—­ ê¸°ë°˜ crop ìƒì„±"""
    
    s1_pred, resized_image = run_stage1_attention_inference(original_image, instruction)
    resize_ratio = s1_pred['resize_ratio']
    scaled_gt_bbox = [coord * resize_ratio for coord in gt_bbox]
    regions = find_connected_regions(s1_pred, resized_image, resize_ratio)
    regions = regions[:MAX_CROPS]
    crops = create_crops_from_connected_regions(regions, original_image)
    num_crops = len(crops)
    
    return s1_pred, crops, num_crops, resized_image, scaled_gt_bbox

def get_stage1_score_at_point(point, s1_attn_scores, s1_n_width, s1_n_height, original_size, resize_ratio):
    """íŠ¹ì • ì ì—ì„œì˜ Stage1 ì–´í…ì…˜ ì ìˆ˜ë¥¼ ê³„ì‚°"""
    
    orig_w, orig_h = original_size
    point_x, point_y = point
    
    resized_x = point_x * resize_ratio
    resized_y = point_y * resize_ratio
    
    resized_w = orig_w * resize_ratio
    resized_h = orig_h * resize_ratio
    
    patch_x = int((resized_x / resized_w) * s1_n_width)
    patch_y = int((resized_y / resized_h) * s1_n_height)
    
    patch_x = max(0, min(patch_x, s1_n_width - 1))
    patch_y = max(0, min(patch_y, s1_n_height - 1))
    
    patch_idx = patch_y * s1_n_width + patch_x
    if patch_idx < len(s1_attn_scores):
        return float(s1_attn_scores[patch_idx])
    else:
        return 0.0

def point_in_bbox(point, bbox):
    """ì ì´ bbox ì•ˆì— ìˆëŠ”ì§€ í™•ì¸"""
    if point is None or bbox is None:
        return False
    return bbox[0] <= point[0] <= bbox[2] and bbox[1] <= point[1] <= bbox[3]

#! ================================================================================================

if __name__ == '__main__':

    set_seed(SEED)

    # Model Import
    device_map = "mps" if args.mac else "balanced"

    model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
        device_map=device_map,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH, max_pixels=MAX_PIXELS)
    
    if TFOPS_PROFILING:
        prof = FlopsProfiler(model)

    # warm_up_model(model, tokenizer, processor)

    if TFOPS_PROFILING:
        prof.start_profile()

    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    # ... (rest of the script setup)
    total_samples = 0
    total_crop_success = 0
    total_stage1_success = 0
    total_stage2_success = 0
    total_stage3_success = 0
    total_s1_time = 0.0
    total_s2_time = 0.0
    total_s3_time = 0.0
    total_s1_tflops = 0.0
    total_s2_tflops = 0.0

    csv_headers = [
        "method", "resize_ratio", "region_threshold", "bbox_padding",
        "total_samples", "crop_accuracy", "stage1_accuracy", "stage2_accuracy", "stage3_accuracy",
        "avg_stage1_time", "avg_stage2_time", "avg_stage3_time", "avg_total_time",
        "avg_stage1_tflops", "avg_stage2_tflops", "avg_total_tflops", "timestamp"
    ]

    for task in TASKS:
        init_iter_logger(
            save_dir=save_dir,
            csv_name=f"iter_log_{task}.csv",
            md_name=f"iter_log_{task}.md",
            headers=[
                "idx", "orig_w", "orig_h", "resize_ratio", "num_crop", "crop_hit",
                "s1_time", "s1_tflops", "s1_hit", "s2_time", "s2_tflops", "s2_hit",
                "s3_time", "s3_hit", "total_time", "total_tflops",
                "crop_acc_uptonow", "s1_acc_uptonow", "s2_acc_uptonow", "s3_acc_uptonow",
                "filename", "instruction"
            ],
            write_md=False, use_fsync=True, use_lock=True
        )
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"
        screenspot_data = json.load(open(os.path.join(SCREENSPOT_JSON, dataset), 'r'))
        screenspot_data = screenspot_data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        task_res = []
        num_action = 0
        s1_time_sum = s2_time_sum = s3_time_sum = 0.0
        s1_tflops_sum = s2_tflops_sum = 0.0
        crop_success_count = stage1_success_count = stage2_success_count = stage3_success_count = 0
        data_source_stats = {}

        for j, item in tqdm(enumerate(screenspot_data)):
            s1_tflops = s2_tflops = 0.0
            num_action += 1
            print("\n\n----------------------\n")
            
            filename = item["img_filename"]
            filename_wo_ext, ext = os.path.splitext(filename)
            img_path = os.path.join(SCREENSPOT_IMGS, filename)
            if not os.path.exists(img_path):
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["instruction"]
            original_bbox = item["bbox"]
            original_bbox = [original_bbox[0], original_bbox[1], 
                             original_bbox[0] + original_bbox[2], original_bbox[1] + original_bbox[3]]
            orig_w, orig_h = original_image.size
            data_source = item.get("data_source", "unknown")

            #! ==================================================================
            #! Stage 1 | Attention-based Crop Generation
            #! ==================================================================
            if TFOPS_PROFILING:
                prof.reset_profile()
            s1_start = time.time()
            s1_pred, s1_crop_list, num_crops, resized_image, scaled_gt_bbox = run_stage1_attention_based(
                original_image=original_image, instruction=instruction, gt_bbox=original_bbox
            )
            s1_end = time.time()
            s1_time = s1_end - s1_start
            if TFOPS_PROFILING:
                s1_tflops = prof.get_total_flops() / 1e12

            s1_success = False
            s1_original_point = None
            if s1_pred and "topk_points" in s1_pred and s1_pred["topk_points"]:
                s1_predicted_point = s1_pred["topk_points"][0]
                s1_original_point = [
                    s1_predicted_point[0] * original_image.size[0],
                    s1_predicted_point[1] * original_image.size[1]
                ]
                s1_success = point_in_bbox(s1_original_point, original_bbox)
            
            s1_hit = "âœ…" if s1_success else "âŒ"
            if s1_success:
                stage1_success_count += 1

            crop_success = False
            for crop in s1_crop_list:
                crop_bbox = crop["bbox"]
                left = max(crop_bbox[0], original_bbox[0])
                top = max(crop_bbox[1], original_bbox[1])
                right = min(crop_bbox[2], original_bbox[2])
                bottom = min(crop_bbox[3], original_bbox[3])
                if left < right and top < bottom:
                    crop_success = True
                    break
            
            crop_hit = "âœ…" if crop_success else "âŒ"
            if crop_success:
                crop_success_count += 1

            #! ==================================================================
            #! [Stage 2] Crop Inference
            #! ==================================================================
            s2_tflops = 0.0
            if TFOPS_PROFILING:
                prof.reset_profile()
            s2_inference_start = time.time()
            s2_pred = run_stage2_multi_image_inference(s1_crop_list, instruction)
            s2_all_candidates = convert_multi_image_results_to_original(s2_pred, s1_crop_list)
            
            s2_corrected_point = s2_all_candidates[0]['point']
            stage2_success = point_in_bbox(s2_corrected_point, original_bbox)
            s2_inference_end = time.time()
            s2_time = s2_inference_end - s2_inference_start
            if TFOPS_PROFILING:
                s2_tflops = prof.get_total_flops() / 1e12
            s2_hit = "âœ…" if stage2_success else "âŒ"
            if stage2_success:
                stage2_success_count += 1

            #! ==================================================================
            #! [Stage 3] Ensemble Processing
            #! ==================================================================
            s3_ensemble_point = None
            stage3_success = False
            s3_start = time.time()
            s1_attn_scores = np.array(s1_pred['attn_scores'][0])
            s1_n_width = s1_pred['n_width']
            s1_n_height = s1_pred['n_height']
            s1_resize_ratio = s1_pred['resize_ratio']
            s1_max_score = float(max(s1_attn_scores)) if len(s1_attn_scores) > 0 else 1.0
            s2_topk_scores = [candidate['score'] for candidate in s2_all_candidates]
            s2_max_score = max(s2_topk_scores)
            ensemble_candidates = []
            
            for i, candidate in enumerate(s2_all_candidates):
                s2_original_point = candidate['point']
                s1_raw_score = get_stage1_score_at_point(
                    s2_original_point, s1_attn_scores, s1_n_width, s1_n_height, 
                    original_image.size, s1_resize_ratio
                )
                s1_score = s1_raw_score / s1_max_score
                s2_score = candidate['score'] / s2_max_score
                ensemble_score = STAGE1_ENSEMBLE_RATIO * s1_score + STAGE2_ENSEMBLE_RATIO * s2_score
                
                ensemble_candidates.append({
                    'point': s2_original_point, 'score': ensemble_score, 's1_score': s1_score,
                    's2_score': s2_score, 'crop_id': candidate['crop_id'],
                    'rank_in_crop': candidate['rank_in_crop'], 's2_rank': i + 1
                })
            
            best_candidate = max(ensemble_candidates, key=lambda x: x['score'])
            s3_ensemble_point = best_candidate['point']
            s3_end = time.time()
            s3_time = s3_end - s3_start
            s3_ensemble_candidates = ensemble_candidates
            stage3_success = point_in_bbox(s3_ensemble_point, original_bbox)
            s3_hit = "âœ…" if stage3_success else "âŒ"
            if stage3_success:
                stage3_success_count += 1

            #! ==================================================================
            #! [Visualization - After Time Measurement]
            #! ==================================================================
            
            if VISUALIZE and (not VIS_ONLY_WRONG or not stage3_success):
                inst_dir_name = re.sub(r'\W+', '_', instruction).strip('_')
                result_folder = "correct" if stage3_success else "incorrect"
                inst_dir = os.path.join(save_dir, f"{task}_visualize_{result_folder}", f"{num_action}_{inst_dir_name}")

                # <<< [ìˆ˜ì •] ëª¨ë¸ ì…ë ¥ ì´ë¯¸ì§€ ì €ì¥ì„ ìœ„í•œ ì½”ë“œ ì‹œì‘ >>>
                # ëª¨ë¸ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ë³„ë„ ë””ë ‰í† ë¦¬ ìƒì„±
                model_input_dir = os.path.join(inst_dir, "model_input_images")
                os.makedirs(model_input_dir, exist_ok=True)

                # Stage 1 ëª¨ë¸ ì…ë ¥ ì´ë¯¸ì§€ ì €ì¥ (ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€)
                resized_image.save(os.path.join(model_input_dir, "stage1_input.png"))

                # Stage 2 ëª¨ë¸ ì…ë ¥ ì´ë¯¸ì§€ ì €ì¥ (í¬ë¡­ëœ ì´ë¯¸ì§€ë“¤)
                if s1_crop_list:
                    for crop in s1_crop_list:
                        crop_img = crop['img']
                        crop_id = crop['id']
                        crop_img.save(os.path.join(model_input_dir, f"stage2_input_crop_{crop_id}.png"))
                # <<< [ìˆ˜ì •] ëª¨ë¸ ì…ë ¥ ì´ë¯¸ì§€ ì €ì¥ì„ ìœ„í•œ ì½”ë“œ ì¢…ë£Œ >>>


                # Stage1 ì‹œê°í™”
                visualize_stage1_attention_crops(
                    s1_pred=s1_pred, resized_image=resized_image, 
                    crop_list=s1_crop_list, original_image=original_image,
                    save_dir=inst_dir, instruction=instruction,
                    gt_bbox=original_bbox, s1_predicted_point=s1_original_point
                )
                
                # Stage2 Multi-Image ì‹œê°í™”
                if s2_pred and s1_crop_list:  # Stage2 ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì‹œê°í™”
                    visualize_stage2_multi_attention(
                        s2_pred=s2_pred, crop_list=s1_crop_list,
                        original_image=original_image, save_dir=inst_dir,
                        instruction=instruction, predicted_point=s2_corrected_point
                    )
                
                # Stage3 ì•™ìƒë¸” ì‹œê°í™”
                visualize_stage3_point_ensemble(
                    s3_ensemble_candidates=s3_ensemble_candidates if 's3_ensemble_candidates' in locals() else [],
                    original_image=original_image, crop_list=s1_crop_list,
                    original_bbox=original_bbox, s3_ensemble_point=s3_ensemble_point,
                    s2_corrected_point=s2_corrected_point, s1_original_point=s1_original_point,
                    stage1_ratio=STAGE1_ENSEMBLE_RATIO, stage2_ratio=STAGE2_ENSEMBLE_RATIO,
                    save_dir=inst_dir, vis_only_wrong=VIS_ONLY_WRONG, stage3_success=stage3_success
                )

            # ... (rest of the script for logging and saving results)
            # This part is left unchanged as it correctly handles the statistics.
            s1_time_sum += s1_time; s2_time_sum += s2_time; s3_time_sum += s3_time
            s1_tflops_sum += s1_tflops; s2_tflops_sum += s2_tflops
            total_time = s1_time + s2_time
            if TFOPS_PROFILING: total_tflops_this = s1_tflops + s2_tflops
            num_attention_crops = len(s1_crop_list)
            print(f"Task: {task}, Image: {filename} {orig_w}x{orig_h} (Resize Ratio : {s1_pred['resize_ratio']})")
            print(f"Attention Crops : {num_attention_crops}")
            print(f"Times - S1: {s1_time:.2f}s | S2: {s2_time:.2f}s | Total: {total_time:.2f}s")
            if TFOPS_PROFILING: print(f"FLOPs - S1: {s1_tflops:.2f} | S2: {s2_tflops:.2f} | Total: {total_tflops_this:.2f} TFLOPs")
            print(f"{'âœ… Success' if stage3_success else 'âŒğŸ¯ Fail'}")

            # Update stats... (rest of the original script logic follows)


    # ... The rest of the script for final aggregation and saving remains the same