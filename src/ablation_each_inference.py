import os
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('gpu', type=int, help='GPU number')
parser.add_argument('--resize', type=float, nargs=2, metavar=('MIN_RESIZE', 'MAX_RESIZE'), help='Stage 1 Resize ratio range (min max)')
parser.add_argument('--ensemble', type=float, help='Stage 1 Ensemble ratio')
parser.add_argument('--visualize', action='store_true', help='Whether to save visualization images')
args = parser.parse_args()

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)  # ëª‡ë²ˆ GPU ì‚¬ìš©í• ì§€ argumentë¡œ ì§€ì • : run_gui_actor.py 2 -> 2ë²ˆ GPU

#! Hyperparameter =====================================================================================

ATTN_IMPL = "eager"  # attention implement "eager" "sdpa" "flash" "efficient"

# Image Resize Ratios

MIN_RESIZE = args.resize[0] if args.resize else 0.50  # DYNAMIC_RESIZE ë¹„ìœ¨ ìµœì†Œê°’
MAX_RESIZE = args.resize[1] if args.resize else 0.50  # DYNAMIC_RESIZE ë¹„ìœ¨ ìµœëŒ€ê°’

# Crop Limitations
MAX_CROPS = 3  # ìƒì„±í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ crop ê°œìˆ˜
SELECT_THRESHOLD = 0.50  #! score >= tau * max_score ì¸ ëª¨ë“  crop select
CROP_WIDTH = 1176  # í¬ë¡­í•  ì§ì‚¬ê°í˜• ê°€ë¡œ í¬ê¸° (ì•„ì´í° ì „ì²´ ê°€ë¡œê°€ 1170px)
CROP_HEIGHT = 602  # í¬ë¡­í•  ì§ì‚¬ê°í˜• ì„¸ë¡œ í¬ê¸°

# Ensemble Hyperparameters
# TODO: ì´ê²ƒë„ resizeì²˜ëŸ¼ ë™ì ìœ¼ë¡œ ì¸¡ì •í•´ì„œ ë³€ê²½ ê°€ëŠ¥í•˜ë„ë¡
STAGE1_ENSEMBLE_RATIO = args.ensemble if args.ensemble else 0.50  # Stage1 attention ê°€ì¤‘ì¹˜
STAGE2_ENSEMBLE_RATIO = 1 - STAGE1_ENSEMBLE_RATIO  # Stage2 crop ê°€ì¤‘ì¹˜
ENSEMBLE_TOP_PATCHES = 100                         # Stage2ì—ì„œ ì•™ìƒë¸”ì— ì‚¬ìš©í•  ìƒìœ„ íŒ¨ì¹˜ ê°œìˆ˜

# ìµœëŒ€ PIXELS ì œí•œ
MAX_PIXELS = 3211264  # Processë‹¨ì—ì„œ ì ìš©

method = "dynamic_resize" # csvì— ê¸°ë¡í•  method ì´ë¦„

memo = f"resize_{MIN_RESIZE}~{MAX_RESIZE}_ensemble{STAGE1_ENSEMBLE_RATIO}_crop{CROP_WIDTH}x{CROP_HEIGHT}"

#! Argument ==========================================================================================

SEED = 0

# Dataset & Model
MLLM_PATH = "microsoft/GUI-Actor-3B-Qwen2.5-VL"
SCREENSPOT_IMGS = "../data/screenspotv2_image"  # input image ê²½ë¡œ
SCREENSPOT_JSON = "../data"  # input image jsoníŒŒì¼ ê²½ë¡œ
TASKS = ["mobile", "web", "desktop"]
# TASKS = ["mobile"]
# TASKS = ["web"]
# TASKS = ["desktop"]
SAMPLE_RANGE = slice(None)
# SAMPLE_RANGE = slice(0,2)

# Visualize & Logging
VISUALIZE = args.visualize if args.visualize else False
VIS_ONLY_WRONG = False  # Trueë©´ í‹€ë¦° ê²ƒë§Œ ì‹œê°í™”, Falseë©´ ëª¨ë“  ê²ƒ ì‹œê°í™”
TFOPS_PROFILING = True
MEMORY_EVAL = True
MEMORY_VIS = False

# Save Path
SAVE_DIR = f"../attn_output/" + method + "/" + memo

#! ==================================================================================================

# Standard Library
import os
import sys
import time
import threading
import re
import json
import logging
logging.disable(logging.CRITICAL)  # ëª¨ë“  ë¡œê¹… í˜¸ì¶œ ë¬´ë ¥í™”

# Third-Party Libraries
import numpy as np
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoTokenizer, set_seed

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from util.iter_logger import init_iter_logger, append_iter_log  # log csv ê¸°ë¡ íŒŒì¼
from gui_actor.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer
from gui_actor.inference import inference
from util.visualize_util import visualize_stage1_attention_crops, visualize_stage2_merged_attention, visualize_stage3_ensemble_attention
from util.sharpness_util import get_fft_blur_score
if TFOPS_PROFILING:
    from deepspeed.profiling.flops_profiler import FlopsProfiler

#! ==============================================================================================

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        return super(NpEncoder, self).default(obj)

def resize_image(image, resize_to_pixels):
    image_width, image_height = image.size
    if (resize_to_pixels is not None) and ((image_width * image_height) > resize_to_pixels):
        resize_ratio = (resize_to_pixels / (image_width * image_height)) ** 0.5
        image_width_resized, image_height_resized = int(image_width * resize_ratio), int(image_height * resize_ratio)
        image = image.resize((image_width_resized, image_height_resized))
        print(f"ğŸ”§ Resized image: {image_width}x{image_height} -> {image_width_resized}x{image_height_resized} (ratio: {resize_ratio:.3f})")
    return image

def warm_up_model(model, tokenizer, processor):
    print("ğŸ‹ï¸â€â™‚ï¸ Warming up the model...")
    dummy_instruction = "This is a dummy instruction for warm-up."
    dummy_image = Image.new("RGB", (1000, 1000), color=(255, 255, 255))  # 1000x1000 í°ìƒ‰ ì´ë¯¸ì§€
    dummy_msgs = create_conversation_stage1(image=dummy_image, instruction=dummy_instruction, resize_ratio=1.0)
    
    # ì˜ˆì—´ìš© inference ì‹¤í–‰
    for _ in range(3):  # 3ë²ˆ ë°˜ë³µ
        with torch.no_grad():
            _ = inference(dummy_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    print("ğŸ‹ï¸â€â™‚ï¸ Warm-up complete!")

def create_conversation_stage1(image, instruction, resize_ratio):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    # "text": (
                    #     "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                    #     "your task is to locate the screen element that corresponds to the instruction. "
                    #     "You should output a PyAutoGUI action that performs a click on the correct position. "
                    #     "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                    #     "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    # ),
                    "text": (
                        f"This is a resized screenshot of the whole GUI, scaled by {resize_ratio}. "
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def create_conversation_stage2(image, instruction, crop_cnt):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    # "text": (
                    #     "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                    #     "your task is to locate the screen element that corresponds to the instruction. "
                    #     "You should output a PyAutoGUI action that performs a click on the correct position. "
                    #     "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                    #     "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    # ),
                    "text": (
                        f"This image is a vertical collage of {crop_cnt} cropped regions that were selected as promising. "
                        "Each crop has the same width and fixed height, separated by a red horizontal line. "
                        "You are a GUI agent. Given this collage image and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def run_stage1_attention_inference(original_image, instruction):
    """Stage 1: ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  inference"""

    # ì´ë¯¸ì§€ ë™ì  ë¦¬ì‚¬ì´ì¦ˆ
    orig_w, orig_h = original_image.size
    downsampled = original_image.resize((int(orig_w*0.5), int(orig_h*0.5)))
    resize_ratio = get_fft_blur_score(downsampled, min_resize=MIN_RESIZE, max_resize=MAX_RESIZE)
    resized_w, resized_h = int(orig_w * resize_ratio), int(orig_h * resize_ratio)
    print(f"ğŸ”§ Dynamic Resized image: {orig_w}x{orig_h} -> {resized_w}x{resized_h} (ratio: {resize_ratio:.3f})")
    
    # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ë¡œ inference
    resized_image = original_image.resize((resized_w, resized_h))
    conversation = create_conversation_stage1(resized_image, instruction, resize_ratio)
    pred = inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=1)
    
    # ê²°ê³¼ì— ë¦¬ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ê°€
    pred['resize_ratio'] = resize_ratio
    pred['original_size'] = (orig_w, orig_h)
    pred['resized_size'] = resized_image.size
    
    return pred, resized_image

def find_attention_peaks(pred_result, resized_image, resize_ratio):
    """ì–´í…ì…˜ì—ì„œ ê³ ì ë“¤ ì°¾ê¸° (bbox ìƒì„± í›„ ì¤‘ì‹¬ì  ê±°ë¦¬ë¡œ ì¤‘ë³µ ì œê±°)"""
    
    attn_scores = np.array(pred_result['attn_scores'][0])
    n_width = pred_result['n_width'] 
    n_height = pred_result['n_height']
    
    resized_w, resized_h = resized_image.size
    
    # íŒ¨ì¹˜ë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ë¹„ìœ¨
    patch_w = resized_w / n_width
    patch_h = resized_h / n_height
    
    # ì ìˆ˜ê°€ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬
    sorted_indices = np.argsort(attn_scores)[::-1]
    
    peaks = []
    used_bbox_centers = []  # ì´ë¯¸ ì‚¬ìš©ëœ bbox ì¤‘ì‹¬ì ë“¤
    
    for idx in sorted_indices:
        # íŒ¨ì¹˜ ì¢Œí‘œ ê³„ì‚°
        patch_y = idx // n_width
        patch_x = idx % n_width
        
        # íŒ¨ì¹˜ ì¤‘ì‹¬ì ì˜ í”½ì…€ ì¢Œí‘œ (ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ê¸°ì¤€)
        center_x = (patch_x + 0.5) * patch_w
        center_y = (patch_y + 0.5) * patch_h
        
        # ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
        orig_center_x = center_x / resize_ratio
        orig_center_y = center_y / resize_ratio
        
        # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ bbox ê³„ì‚° (ì‹¤ì œ í¬ë¡­ì´ ìƒì„±ë  ìœ„ì¹˜)
        orig_w = resized_w / resize_ratio
        orig_h = resized_h / resize_ratio
        
        left = max(0, int(orig_center_x - CROP_WIDTH/2))
        top = max(0, int(orig_center_y - CROP_HEIGHT/2))
        right = min(orig_w, int(orig_center_x + CROP_WIDTH/2))
        bottom = min(orig_h, int(orig_center_y + CROP_HEIGHT/2))
        
        # ê²½ê³„ì—ì„œ ì˜ë ¸ì„ ê²½ìš° ì¡°ì •
        if right - left < CROP_WIDTH and right < orig_w:
            right = min(orig_w, left + int(CROP_WIDTH))
        if right - left < CROP_WIDTH and left > 0:
            left = max(0, right - int(CROP_WIDTH))
        if bottom - top < CROP_HEIGHT and bottom < orig_h:
            bottom = min(orig_h, top + int(CROP_HEIGHT))
        if bottom - top < CROP_HEIGHT and top > 0:
            top = max(0, bottom - int(CROP_HEIGHT))
        
        # ì‹¤ì œ bbox ì¤‘ì‹¬ì  ê³„ì‚°
        bbox_center_x = (left + right) / 2
        bbox_center_y = (top + bottom) / 2
        
        # ì´ë¯¸ ì‚¬ìš©ëœ bboxì™€ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸
        skip = False
        for used_center in used_bbox_centers:
            x_distance = abs(bbox_center_x - used_center[0])
            y_distance = abs(bbox_center_y - used_center[1])
            # bbox ì¤‘ì‹¬ì ì´ ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ìŠ¤í‚µ (í¬ë¡­ í¬ê¸°ì˜ 30% ì´ë‚´)
            if x_distance < CROP_WIDTH * 0.3 and y_distance < CROP_HEIGHT * 0.3:  #! í¬ë¡­
                skip = True
                break
        
        if not skip:
            peaks.append({
                'center_x': center_x,  # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ê¸°ì¤€ (ê¸°ì¡´ í˜¸í™˜ì„±)
                'center_y': center_y,
                'score': float(attn_scores[idx]),
                'patch_x': patch_x,
                'patch_y': patch_y,
                'bbox_center': (bbox_center_x, bbox_center_y)  # ì‹¤ì œ bbox ì¤‘ì‹¬ì 
            })
            
            used_bbox_centers.append((bbox_center_x, bbox_center_y))
    
    return peaks

def filter_by_threshold(peaks, threshold=0.7, max_crops=MAX_CROPS):
    """1ë“±ì˜ threshold% ì´ìƒì¸ peakë“¤ë§Œ ë‚¨ê¸°ê³ , ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì ìš©"""
    
    if not peaks:
        return []
    
    max_score = peaks[0]['score']  # ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìŒ
    min_score = max_score * threshold
    
    # threshold ì¡°ê±´ìœ¼ë¡œ ë¨¼ì € í•„í„°ë§
    filtered_peaks = [peak for peak in peaks if peak['score'] >= min_score]
    
    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì ìš© (ìƒìœ„ ì ìˆ˜ìˆœìœ¼ë¡œ)
    if len(filtered_peaks) > max_crops:
        filtered_peaks = filtered_peaks[:max_crops]
    
    print(f"ğŸ¯ Found {len(peaks)} peaks, filtered to {len(filtered_peaks)} (threshold: {threshold}, max_crops: {max_crops})")
    
    return filtered_peaks

def create_crops_from_attention_peaks(peaks, original_image, resize_ratio):
    """Attention peaksë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ crop"""
    
    if not peaks:
        return []
    
    crops = []
    orig_w, orig_h = original_image.size
    
    for i, peak in enumerate(peaks):
        # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ì—ì„œì˜ centerë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³€í™˜
        orig_center_x = peak['center_x'] / resize_ratio
        orig_center_y = peak['center_y'] / resize_ratio
        
        # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œì˜ bbox ê³„ì‚°
        left = max(0, int(orig_center_x - CROP_WIDTH/2))
        top = max(0, int(orig_center_y - CROP_HEIGHT/2))
        right = min(orig_w, int(orig_center_x + CROP_WIDTH/2))
        bottom = min(orig_h, int(orig_center_y + CROP_HEIGHT/2))
        
        # ê²½ê³„ì—ì„œ ì˜ë ¸ì„ ê²½ìš° ì¡°ì •
        if right - left < CROP_WIDTH and right < orig_w:
            right = min(orig_w, left + int(CROP_WIDTH))
        if right - left < CROP_WIDTH and left > 0:
            left = max(0, right - int(CROP_WIDTH))
        if bottom - top < CROP_HEIGHT and bottom < orig_h:
            bottom = min(orig_h, top + int(CROP_HEIGHT))
        if bottom - top < CROP_HEIGHT and top > 0:
            top = max(0, bottom - int(CROP_HEIGHT))
        
        bbox = [left, top, right, bottom]
        crop_img = original_image.crop(bbox)
        
        crops.append({
            'img': crop_img,
            'bbox': bbox,
            'score': peak['score'],
            'id': i + 1
        })
        
        # print(f"ğŸ”§ Crop {i+1}: center=({orig_center_x:.1f}, {orig_center_y:.1f}), bbox={bbox}, size={CROP_WIDTH}x{CROP_HEIGHT}")
    
    return crops

def create_merged_image_for_stage2(crops):
    """Stage 2ìš©: cropë“¤ì„ ì„¸ë¡œë¡œ í•©ì¹˜ê¸° (ë¹¨ê°„ìƒ‰ êµ¬ë¶„ì„  ì´ë¯¸ì§€ë¡œ ë¶„ë¦¬) - bbox yì¢Œí‘œ ìˆœìœ¼ë¡œ ì •ë ¬"""

    if not crops:
        return None, []
    
    # bboxì˜ yì¢Œí‘œ(top) ìˆœìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
    sorted_crops = sorted(crops, key=lambda crop: crop['bbox'][1])
    
    # ì„¸ë¡œë¡œ í•©ì¹  ì´ë¯¸ì§€ë“¤ê³¼ êµ¬ë¶„ì„ ë“¤ì„ ë³„ë„ë¡œ ì¤€ë¹„
    separator_height = 28  # ë¹¨ê°„ìƒ‰ êµ¬ë¶„ì„  ë‘ê»˜
    max_width = max(crop['img'].width for crop in sorted_crops)
    
    # í•©ì¹  ì´ë¯¸ì§€ë“¤ ë¦¬ìŠ¤íŠ¸ (crop ì´ë¯¸ì§€ + êµ¬ë¶„ì„  ì´ë¯¸ì§€ë“¤)
    images_to_merge = []
    crop_y_mappings = []
    current_y = 0
    
    for i, crop in enumerate(sorted_crops):
        # crop ì´ë¯¸ì§€ ì¶”ê°€
        images_to_merge.append(crop['img'])
        
        # ë§¤í•‘ ì •ë³´ ì €ì¥: (merged_y_start, merged_y_end) -> (original_bbox)
        paste_x = (max_width - crop['img'].width) // 2
        crop_y_mappings.append({
            'merged_y_start': current_y,
            'merged_y_end': current_y + crop['img'].height,
            'original_bbox': crop['bbox'],
            'paste_x': paste_x
        })
        
        current_y += crop['img'].height
        
        # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ë¹¨ê°„ìƒ‰ êµ¬ë¶„ì„  ì´ë¯¸ì§€ ì¶”ê°€
        if i < len(sorted_crops) - 1:
            separator_img = Image.new('RGB', (max_width, separator_height), color=(256, 0, 0))
            images_to_merge.append(separator_img)
            current_y += separator_height
    
    # ì´ ë†’ì´ ê³„ì‚°
    total_height = current_y
    
    # í•©ì³ì§„ ì´ë¯¸ì§€ ìƒì„±
    merged_img = Image.new('RGB', (max_width, total_height), color=(0, 0, 0))
    
    # ì´ë¯¸ì§€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë¶™ì´ê¸°
    paste_y = 0
    image_idx = 0
    
    for i, crop in enumerate(sorted_crops):
        # crop ì´ë¯¸ì§€ ë¶™ì´ê¸° (ì¤‘ì•™ ì •ë ¬)
        crop_img = images_to_merge[image_idx]
        paste_x = (max_width - crop_img.width) // 2
        merged_img.paste(crop_img, (paste_x, paste_y))
        paste_y += crop_img.height
        image_idx += 1
        
        # êµ¬ë¶„ì„  ì´ë¯¸ì§€ ë¶™ì´ê¸° (ë§ˆì§€ë§‰ì´ ì•„ë‹Œ ê²½ìš°)
        if i < len(sorted_crops) - 1:
            separator_img = images_to_merge[image_idx]
            merged_img.paste(separator_img, (0, paste_y))
            paste_y += separator_img.height
            image_idx += 1
    
    return merged_img, crop_y_mappings


def run_stage2_merged_inference(merged_img, instruction):
    """Stage 2: í•©ì³ì§„ ì´ë¯¸ì§€ë¡œ inference"""
    
    # í•©ì³ì§„ ì´ë¯¸ì§€ë¡œ inference
    conversation = create_conversation_stage2(merged_img, instruction, 1)

    # topkë¥¼ 10ê°œë¡œ ëŠ˜ë ¤ì„œ ì•™ìƒë¸”ì—ì„œ í™œìš©
    pred = inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=10)
    
    return pred

def convert_merged_point_to_original(point_in_merged, crop_y_mappings, merged_img_size):
    """í•©ì³ì§„ ì´ë¯¸ì§€ì—ì„œì˜ ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜"""
    
    merged_w, merged_h = merged_img_size
    
    # í•©ì³ì§„ ì´ë¯¸ì§€ ë‚´ì—ì„œì˜ í”½ì…€ ì¢Œí‘œ
    merged_x = point_in_merged[0] * merged_w
    merged_y = point_in_merged[1] * merged_h
    
    # ì–´ëŠ cropì— ì†í•˜ëŠ”ì§€ ì°¾ê¸°
    for mapping in crop_y_mappings:
        if mapping['merged_y_start'] <= merged_y < mapping['merged_y_end']:
            # í•´ë‹¹ crop ë‚´ì—ì„œì˜ ìƒëŒ€ ì¢Œí‘œ
            relative_y = merged_y - mapping['merged_y_start']
            relative_x = merged_x - mapping['paste_x']
            
            # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
            original_bbox = mapping['original_bbox']
            original_x = original_bbox[0] + relative_x
            original_y = original_bbox[1] + relative_y
            
            return [original_x, original_y]
    
    # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê²½ìš° ì²« ë²ˆì§¸ cropì˜ ì¤‘ì‹¬ìœ¼ë¡œ ê·¼ì‚¬
    if crop_y_mappings:
        first_bbox = crop_y_mappings[0]['original_bbox']
        center_x = (first_bbox[0] + first_bbox[2]) / 2
        center_y = (first_bbox[1] + first_bbox[3]) / 2
        return [center_x, center_y]
    
    return [0, 0]

def resize_attention_to_original(s1_pred, original_size, resize_ratio):
    """Stage1 ì–´í…ì…˜ ë§µì„ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜"""
    
    attn_scores = np.array(s1_pred['attn_scores'][0])
    n_width = s1_pred['n_width'] 
    n_height = s1_pred['n_height']
    
    orig_w, orig_h = original_size
    
    # ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
    attention_map = np.zeros((orig_h, orig_w))
    
    for idx, score in enumerate(attn_scores):
        # íŒ¨ì¹˜ ì¢Œí‘œ ê³„ì‚°
        patch_y = idx // n_width
        patch_x = idx % n_width
        
        # íŒ¨ì¹˜ ì¤‘ì‹¬ì ì„ ì›ë³¸ ì¢Œí‘œë¡œ ë³€í™˜ (ë¦¬ì‚¬ì´ì¦ˆ ì—­ë³´ì •)
        # íŒ¨ì¹˜ ì¤‘ì‹¬ì  ê³„ì‚° (normalized coordinates)
        patch_center_x_norm = (patch_x + 0.5) / n_width
        patch_center_y_norm = (patch_y + 0.5) / n_height
        
        # ì›ë³¸ ì¢Œí‘œë¡œ ë³€í™˜
        orig_center_x = patch_center_x_norm * orig_w
        orig_center_y = patch_center_y_norm * orig_h
        
        # ì›ë³¸ì—ì„œì˜ íŒ¨ì¹˜ í¬ê¸° ì¶”ì •
        orig_patch_w = orig_w / n_width
        orig_patch_h = orig_h / n_height
        
        # íŒ¨ì¹˜ ì˜ì—­ ê³„ì‚° (ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ)
        orig_x_start = int(max(0, orig_center_x - orig_patch_w/2))
        orig_x_end = int(min(orig_w, orig_center_x + orig_patch_w/2))
        orig_y_start = int(max(0, orig_center_y - orig_patch_h/2))
        orig_y_end = int(min(orig_h, orig_center_y + orig_patch_h/2))
        
        # í•´ë‹¹ ì˜ì—­ì— ì–´í…ì…˜ ìŠ¤ì½”ì–´ í• ë‹¹
        if orig_x_end > orig_x_start and orig_y_end > orig_y_start:
            attention_map[orig_y_start:orig_y_end, orig_x_start:orig_x_end] = score
    
    return attention_map

def create_crop_attention_maps(crops, original_size):
    """ê° í¬ë¡­ì— ëŒ€í•œ ì–´í…ì…˜ ë§µ ìƒì„± (ê· ë“± ë¶„í¬) - fallback í•¨ìˆ˜"""
    
    orig_w, orig_h = original_size
    crop_attention_maps = []
    
    for crop in crops:
        # ê° í¬ë¡­ì— ëŒ€í•´ ê· ë“±í•œ ì–´í…ì…˜ ë§µ ìƒì„±
        crop_map = np.zeros((orig_h, orig_w))
        bbox = crop['bbox']
        
        # í¬ë¡­ ì˜ì—­ì— ê· ë“±í•œ ê°’ í• ë‹¹
        crop_map[bbox[1]:bbox[3], bbox[0]:bbox[2]] = 1.0
        
        crop_attention_maps.append(crop_map)
    
    return crop_attention_maps

def create_stage2_attention_to_original(s2_pred, s2_crop_mappings, s2_merged_img, original_size, use_top_patches=ENSEMBLE_TOP_PATCHES):
    """Stage2 ì–´í…ì…˜ì„ Stage1ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜"""
    
    attn_scores = np.array(s2_pred['attn_scores'][0])
    n_width = s2_pred['n_width'] 
    n_height = s2_pred['n_height']
    
    orig_w, orig_h = original_size
    
    # ìƒìœ„ íŒ¨ì¹˜ë“¤ë§Œ ì„ ë³„ (íš¨ìœ¨ì„±ì„ ìœ„í•´)
    top_patch_indices = np.argsort(attn_scores)[::-1][:use_top_patches]
    
    # Stage2 ì–´í…ì…˜ ë§µì„ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
    stage2_attention_map = np.zeros((orig_h, orig_w))
    
    for patch_idx in top_patch_indices:
        score = attn_scores[patch_idx]
        
        # íŒ¨ì¹˜ ì¢Œí‘œ ê³„ì‚°
        patch_y = patch_idx // n_width
        patch_x = patch_idx % n_width
        
        # íŒ¨ì¹˜ ì¤‘ì‹¬ì  ê³„ì‚° (normalized coordinates)
        patch_center_x_norm = (patch_x + 0.5) / n_width
        patch_center_y_norm = (patch_y + 0.5) / n_height
        
        # í•©ì³ì§„ ì´ë¯¸ì§€ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
        point_original = convert_merged_point_to_original(
            [patch_center_x_norm, patch_center_y_norm], s2_crop_mappings, s2_merged_img.size
        )
        
        # ì›ë³¸ì—ì„œì˜ íŒ¨ì¹˜ í¬ê¸° ì¶”ì • (Stage1ê³¼ ë™ì¼)
        orig_patch_w = orig_w / n_width
        orig_patch_h = orig_h / n_height
        
        # íŒ¨ì¹˜ ì˜ì—­ ê³„ì‚° (ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ)
        orig_center_x, orig_center_y = point_original
        orig_x_start = int(max(0, orig_center_x - orig_patch_w/2))
        orig_x_end = int(min(orig_w, orig_center_x + orig_patch_w/2))
        orig_y_start = int(max(0, orig_center_y - orig_patch_h/2))
        orig_y_end = int(min(orig_h, orig_center_y + orig_patch_h/2))
        
        # í•´ë‹¹ ì˜ì—­ì— ì–´í…ì…˜ ìŠ¤ì½”ì–´ í• ë‹¹ (Stage1ê³¼ ë™ì¼)
        if orig_x_end > orig_x_start and orig_y_end > orig_y_start:
            stage2_attention_map[orig_y_start:orig_y_end, orig_x_start:orig_x_end] += score
    
    return stage2_attention_map

def create_crop_attention_maps_patches(s2_pred, s2_crop_mappings, s2_merged_img, s1_crop_list, original_size, use_top_patches):
    """Stage2ì˜ ìƒìœ„ íŒ¨ì¹˜ë“¤ì„ Stage1ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í¬ë¡­ ì–´í…ì…˜ ë§µ ìƒì„±"""
    
    orig_w, orig_h = original_size
    crop_attention_maps = []
    
    # Stage2ì—ì„œ ê°œë³„ íŒ¨ì¹˜ ì •ë³´ ì¶”ì¶œ
    patch_coords = s2_pred['patch_coords']  # normalized coordinates
    patch_scores = np.array(s2_pred['patch_scores'])
    
    # ìƒìœ„ íŒ¨ì¹˜ë“¤ ì„ ë³„ (ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ)
    top_patch_indices = np.argsort(patch_scores)[::-1][:use_top_patches]
    
    for crop in s1_crop_list:
        crop_map = np.zeros((orig_h, orig_w))
        crop_bbox = crop['bbox']
        
        # ìƒìœ„ íŒ¨ì¹˜ë“¤ë§Œ ì²˜ë¦¬
        for patch_idx in top_patch_indices:
            # íŒ¨ì¹˜ ì¢Œí‘œë¥¼ í•©ì³ì§„ ì´ë¯¸ì§€ì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            patch_coord_norm = patch_coords[patch_idx]  # [x_norm, y_norm]
            point_original = convert_merged_point_to_original(
                patch_coord_norm, s2_crop_mappings, s2_merged_img.size
            )
            
            # ì´ ì ì´ í˜„ì¬ í¬ë¡­ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            if point_in_bbox(point_original, crop_bbox):
                # Stage1ê³¼ ë™ì¼í•˜ê²Œ: íŒ¨ì¹˜ ì˜ì—­ì— í•´ë‹¹í•˜ëŠ” ì ìˆ˜ í• ë‹¹
                # ì›ë³¸ì—ì„œì˜ íŒ¨ì¹˜ í¬ê¸° ì¶”ì •
                n_width = s2_pred['n_width'] 
                n_height = s2_pred['n_height']
                orig_patch_w = orig_w / n_width
                orig_patch_h = orig_h / n_height
                
                # íŒ¨ì¹˜ ì˜ì—­ ê³„ì‚°
                center_x, center_y = point_original
                patch_x_start = int(max(0, center_x - orig_patch_w/2))
                patch_x_end = int(min(orig_w, center_x + orig_patch_w/2))
                patch_y_start = int(max(0, center_y - orig_patch_h/2))
                patch_y_end = int(min(orig_h, center_y + orig_patch_h/2))
                
                # í•´ë‹¹ íŒ¨ì¹˜ ì˜ì—­ì— ì ìˆ˜ í• ë‹¹ (Stage1ê³¼ ë™ì¼)
                if patch_x_end > patch_x_start and patch_y_end > patch_y_start:
                    crop_map[patch_y_start:patch_y_end, patch_x_start:patch_x_end] += patch_scores[patch_idx]
        
        # í¬ë¡­ì— íŒ¨ì¹˜ê°€ ì—†ìœ¼ë©´ ë‚®ì€ ê· ë“± ë¶„í¬
        if crop_map.max() == 0:
            crop_map[crop_bbox[1]:crop_bbox[3], crop_bbox[0]:crop_bbox[2]] = 0.1
        
        crop_attention_maps.append(crop_map)
    
    return crop_attention_maps

def create_crop_point_based_attention_maps(s2_corrected_point, s1_crop_list, original_size):
    """Stage2ì˜ ì˜ˆì¸¡ ì ì„ ê¸°ë°˜ìœ¼ë¡œ í¬ë¡­ ì–´í…ì…˜ ë§µ ìƒì„± (ë²¡í„°í™”ëœ ê³ ì† ë²„ì „)"""
    
    orig_w, orig_h = original_size
    crop_attention_maps = []
    
    for crop in s1_crop_list:
        crop_map = np.zeros((orig_h, orig_w))
        crop_bbox = crop['bbox']
        
        # Stage2 ì˜ˆì¸¡ì ì´ ì´ í¬ë¡­ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
        if point_in_bbox(s2_corrected_point, crop_bbox):
            # ì˜ˆì¸¡ì  ì£¼ë³€ì— ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ì–´í…ì…˜ í• ë‹¹ (ë²¡í„°í™”)
            center_x, center_y = s2_corrected_point
            
            # í¬ë¡­ ì˜ì—­ì˜ ì¢Œí‘œ ê·¸ë¦¬ë“œ ìƒì„±
            y_min, y_max = max(0, crop_bbox[1]), min(orig_h, crop_bbox[3])
            x_min, x_max = max(0, crop_bbox[0]), min(orig_w, crop_bbox[2])
            
            if y_max > y_min and x_max > x_min:
                # ë©”ì‹œ ê·¸ë¦¬ë“œ ìƒì„± (ë²¡í„°í™”)
                y_coords, x_coords = np.meshgrid(
                    np.arange(y_min, y_max), 
                    np.arange(x_min, x_max), 
                    indexing='ij'
                )
                
                # ê±°ë¦¬ ê³„ì‚° (ë²¡í„°í™”)
                dist = np.sqrt((x_coords - center_x)**2 + (y_coords - center_y)**2)
                
                # ì‹œê·¸ë§ˆ ê³„ì‚°
                sigma = min(crop_bbox[2] - crop_bbox[0], crop_bbox[3] - crop_bbox[1]) / 6
                
                # ê°€ìš°ì‹œì•ˆ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë²¡í„°í™”)
                weights = np.exp(-(dist**2) / (2 * sigma**2))
                
                # í¬ë¡­ ë§µì— í• ë‹¹
                crop_map[y_min:y_max, x_min:x_max] = weights
        else:
            # ì˜ˆì¸¡ì ì´ í¬ë¡­ ë°–ì— ìˆìœ¼ë©´ ê· ë“± ë¶„í¬ (ë‚®ì€ ê°’)
            crop_map[crop_bbox[1]:crop_bbox[3], crop_bbox[0]:crop_bbox[2]] = 0.1
        
        crop_attention_maps.append(crop_map)
    
    return crop_attention_maps

def ensemble_attention_maps(stage1_attention_map, crop_attention_maps, attention_ratio=STAGE1_ENSEMBLE_RATIO, crop_ratio=STAGE2_ENSEMBLE_RATIO):
    """Stage1 ì–´í…ì…˜ê³¼ í¬ë¡­ ì–´í…ì…˜ë“¤ì„ ì•™ìƒë¸” (ë²¡í„°í™”ëœ ê³ ì† ë²„ì „)"""
    
    orig_h, orig_w = stage1_attention_map.shape
    
    # Stage1 ì–´í…ì…˜ ì •ê·œí™” (0~1)
    s1_max = stage1_attention_map.max()
    if s1_max > 0:
        stage1_normalized = stage1_attention_map / s1_max
    else:
        stage1_normalized = stage1_attention_map
    
    # í¬ë¡­ ì–´í…ì…˜ë“¤ì„ ìŠ¤íƒìœ¼ë¡œ í•©ì¹˜ê¸° (ë²¡í„°í™”)
    if crop_attention_maps:
        crop_stack = np.stack(crop_attention_maps, axis=0)  # (num_crops, H, W)
        crop_sum = np.sum(crop_stack, axis=0)  # (H, W)
        overlap_count = np.sum(crop_stack > 0, axis=0)  # (H, W)
    else:
        crop_sum = np.zeros((orig_h, orig_w))
        overlap_count = np.zeros((orig_h, orig_w))
    
    # Stage2 í¬ë¡­ ì–´í…ì…˜ ì •ê·œí™” (0~1)
    crop_max = crop_sum.max()
    if crop_max > 0:
        crop_normalized = crop_sum / crop_max
    else:
        crop_normalized = crop_sum
    
    # ìµœì¢… ì•™ìƒë¸” ë§µ ê³„ì‚° (ë²¡í„°í™”)
    ensemble_map = attention_ratio * stage1_normalized
    
    # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì²˜ë¦¬ (ë²¡í„°í™”)
    valid_mask = overlap_count > 0
    if np.any(valid_mask):
        adjusted_crop_weights = np.where(
            valid_mask,
            crop_ratio / overlap_count,
            0
        )
        ensemble_map += adjusted_crop_weights * crop_normalized
    
    return ensemble_map

def find_ensemble_best_point(ensemble_map):
    """ì•™ìƒë¸” ë§µì—ì„œ ìµœê³ ì  ì°¾ê¸° (ì„œë¸Œí”½ì…€ ì •í™•ë„ë¡œ)"""
    
    # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
    max_idx = np.argmax(ensemble_map)
    orig_h, orig_w = ensemble_map.shape
    
    best_y = max_idx // orig_w
    best_x = max_idx % orig_w
    best_score = ensemble_map[best_y, best_x]
    
    # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¥¼ ìœ„í•œ ë³´ê°„ (ì£¼ë³€ í”½ì…€ë“¤ì˜ ê°€ì¤‘í‰ê· )
    # 3x3 ì˜ì—­ì—ì„œ ê°€ì¤‘ì¤‘ì‹¬ ê³„ì‚°
    y_start = max(0, best_y - 1)
    y_end = min(orig_h, best_y + 2)
    x_start = max(0, best_x - 1) 
    x_end = min(orig_w, best_x + 2)
    
    # ì£¼ë³€ ì˜ì—­ ì¶”ì¶œ
    region = ensemble_map[y_start:y_end, x_start:x_end]
    
    if region.size > 1:
        # ì¢Œí‘œ ë©”ì‹œ ìƒì„±
        y_coords, x_coords = np.meshgrid(
            np.arange(y_start, y_end),
            np.arange(x_start, x_end),
            indexing='ij'
        )
        
        # ê°€ì¤‘í‰ê· ìœ¼ë¡œ ì„œë¸Œí”½ì…€ ìœ„ì¹˜ ê³„ì‚°
        total_weight = np.sum(region)
        if total_weight > 0:
            refined_y = np.sum(y_coords * region) / total_weight
            refined_x = np.sum(x_coords * region) / total_weight
        else:
            refined_y = float(best_y)
            refined_x = float(best_x)
    else:
        refined_y = float(best_y)
        refined_x = float(best_x)
    
    return [refined_x, refined_y], float(best_score)

def run_stage1_attention_based(original_image, instruction, gt_bbox):
    """ìƒˆë¡œìš´ ê°„ë‹¨í•œ Stage 1: Attention ê¸°ë°˜ crop ìƒì„±"""
    
    # 1. ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  inference
    print("ğŸ” Stage 1: Running attention-based inference...")
    s1_pred, resized_image = run_stage1_attention_inference(original_image, instruction)
    
    # 2. GT bboxë„ ë¦¬ì‚¬ì´ì¦ˆ ë¹„ìœ¨ì— ë§ì¶° ì¡°ì •
    resize_ratio = s1_pred['resize_ratio']
    scaled_gt_bbox = [coord * resize_ratio for coord in gt_bbox]
    
    # 3. Attention ê³ ì ë“¤ ì°¾ê¸°
    peaks = find_attention_peaks(s1_pred, resized_image, resize_ratio)
    
    if not peaks:
        print("âš ï¸ No attention peaks found")
        return s1_pred, [], 0, resized_image, scaled_gt_bbox
    
    # 4. 1ë“±ì˜ 70% ì´ìƒë§Œ ë‚¨ê¸°ê³  ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì ìš©
    filtered_peaks = filter_by_threshold(peaks, threshold=SELECT_THRESHOLD, max_crops=MAX_CROPS)
    
    if not filtered_peaks:
        print("âš ï¸ No peaks passed threshold")
        return s1_pred, [], 0, resized_image, scaled_gt_bbox
    
    # 5. ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ crop ìƒì„±
    crops = create_crops_from_attention_peaks(filtered_peaks, original_image, resize_ratio)
    
    num_crops = len(crops)
    
    return s1_pred, crops, num_crops, resized_image, scaled_gt_bbox

def point_in_bbox(point, bbox):
    """ì ì´ bbox ì•ˆì— ìˆëŠ”ì§€ í™•ì¸"""
    if point is None or bbox is None:
        return False
    return bbox[0] <= point[0] <= bbox[2] and bbox[1] <= point[1] <= bbox[3]

def monitor_memory(interval=0.1):
    start = time.time()
    while not stop_flag:
        mem = torch.cuda.memory_allocated() / 1024**3
        now = time.time() - start
        mem_log.append(mem)
        time_log.append(now)
        time.sleep(interval)

#! ================================================================================================

if __name__ == '__main__':

    set_seed(SEED)

    # Model Import (NVIDIA CUDA)
    model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
        device_map="balanced",
        # max_memory=max_memory, 
        low_cpu_mem_usage=True
    )
    # Model Import (Mac)
    # model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
    #     MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
    #     device_map="mps", # Mac
    #     # max_memory=max_memory, 
    #     low_cpu_mem_usage=False
    # )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH, max_pixels=MAX_PIXELS)
    prof = FlopsProfiler(model)

    warm_up_model(model, tokenizer, processor)
    if TFOPS_PROFILING:
        prof.start_profile()

    # save_dir í´ë”ëª…ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„± (save_dir -> save_dir_1 -> save_dir_2)
    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    # Process
    for task in TASKS:
        # ê° taskë³„ë¡œ ë³„ë„ì˜ ë¡œê·¸ íŒŒì¼ ìƒì„±
        init_iter_logger(  
            save_dir=save_dir,
            csv_name=f"iter_log_{task}.csv",
            md_name=f"iter_log_{task}.md",
            headers=[  # ìˆœì„œ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°
                "idx", "orig_w", "orig_h", "resize_ratio",
                "num_crop", "crop_hit",
                "s1_time", "s1_tflops", "s1_hit", 
                "s2_time", "s2_tflops", "s2_hit", 
                "s3_ensemble_time", "s3_ensemble_hit",
                "total_time", "total_tflops", "peak_memory_gb", 
                "crop_acc_uptonow", "s1_acc_uptonow", "s2_acc_uptonow", "s3_ensemble_acc_uptonow",
                "filename", "instruction"
            ],
            write_md=False, use_fsync=True, use_lock=True
        )
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"
        screenspot_data = json.load(open(os.path.join(SCREENSPOT_JSON, dataset), 'r'))
        screenspot_data = screenspot_data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        # í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        task_res = []
        num_action = 0
        s1_time_sum = s2_time_sum = s3_time_sum = 0.0
        s1_tflops_sum = s2_tflops_sum = 0.0
        stage1_success_count = stage2_success_count = stage3_ensemble_success_count = 0
        crop_success_count = 0  # ìƒˆë¡œìš´ crop ì„±ê³µ ì¹´ìš´í„° ì¶”ê°€
        peak_memory_sum = 0.0  # í”¼í¬ ë©”ëª¨ë¦¬ í•©ê³„ ì¶”ê°€
        
        # data_sourceë³„ í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        data_source_stats = {}

        if MEMORY_VIS:
            memory_dir = os.path.join(save_dir, "gpu_usage", task)
            os.makedirs(memory_dir, exist_ok=True)

        for j, item in tqdm(enumerate(screenspot_data)):

            if MEMORY_EVAL:
                mem_log = []
                time_log = []
                stop_flag = False
                monitor_thread = threading.Thread(target=monitor_memory, daemon=True)
                monitor_thread.start()
                torch.cuda.reset_peak_memory_stats()

            s1_tflops = s2_tflops = 0.0

            print("\n\n----------------------\n")

            num_action += 1
            
            # íŒŒì¼ ë° ë°ì´í„° ë¡œë“œ
            filename = item["img_filename"]
            filename_wo_ext, ext = os.path.splitext(filename)
            img_path = os.path.join(SCREENSPOT_IMGS, filename)
            if not os.path.exists(img_path):
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["instruction"]
            original_bbox = item["bbox"]
            original_bbox = [original_bbox[0], original_bbox[1], 
                           original_bbox[0] + original_bbox[2], original_bbox[1] + original_bbox[3]]

            orig_w, orig_h = original_image.size

            # data_source ì •ë³´ ì¶”ì¶œ (ì—†ìœ¼ë©´ "unknown"ìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •)
            data_source = item.get("data_source", "unknown")

            #! ==================================================================
            #! Stage 1 | Attention-based Crop Generation
            #! ==================================================================

            if TFOPS_PROFILING:
                prof.reset_profile()

            s1_start = time.time()
            
            # ìƒˆë¡œìš´ attention ê¸°ë°˜ ë°©ì‹ (ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©)
            s1_pred, s1_crop_list, num_crops, resized_image, scaled_gt_bbox = run_stage1_attention_based(
                original_image=original_image,
                instruction=instruction,
                gt_bbox=original_bbox
            )
            
            s1_infence_end = time.time()
            s1_time = s1_infence_end - s1_start

            if TFOPS_PROFILING:
                s1_tflops = prof.get_total_flops() / 1e12

            # Stage1 Grounding ì„±ê³µ ì—¬ë¶€ í™•ì¸ (ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼)
            s1_success = False
            s1_original_point = None
            if s1_pred and "topk_points" in s1_pred and s1_pred["topk_points"]:
                s1_predicted_point = s1_pred["topk_points"][0]  # ì •ê·œí™”ëœ ì¢Œí‘œ (0~1)
                # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                s1_original_point = [
                    s1_predicted_point[0] * original_image.size[0],
                    s1_predicted_point[1] * original_image.size[1]
                ]
                s1_success = point_in_bbox(s1_original_point, original_bbox)
            
            s1_hit = "âœ…" if s1_success else "âŒ"
            if s1_success:
                stage1_success_count += 1

            # Crop ìƒì„± ì„±ê³µ ì—¬ë¶€ í™•ì¸ (GTê°€ ìƒì„±ëœ cropë“¤ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸)
            crop_success = False
            if num_crops > 0:
                gt_center = [(original_bbox[0] + original_bbox[2])/2, (original_bbox[1] + original_bbox[3])/2]
                for crop in s1_crop_list:
                    crop_bbox = crop["bbox"]
                    if point_in_bbox(gt_center, crop_bbox):
                        crop_success = True
                        break
            
            crop_hit = "âœ…" if crop_success else "âŒ"
            if crop_success:
                crop_success_count += 1

            #! ==================================================================
            #! [Stage 2] Merged Crop Inference
            #! ==================================================================
            
            s2_tflops = 0.0
            s2_pred = s2_merged_img = s2_crop_mappings = s2_corrected_point = None  # ì‹œê°í™”ìš© ë³€ìˆ˜ ì´ˆê¸°í™”
            stage2_success = False

            s2_inference_start = time.time()
            
            if TFOPS_PROFILING:
                prof.reset_profile()
            
            # 1. cropë“¤ì„ ì„¸ë¡œë¡œ í•©ì¹˜ê¸°
            s2_merged_img, s2_crop_mappings = create_merged_image_for_stage2(s1_crop_list)
            
            # 2. í•©ì³ì§„ ì´ë¯¸ì§€ë¡œ inference
            s2_pred = run_stage2_merged_inference(s2_merged_img, instruction)
            
            # 3. ê²°ê³¼ ì²˜ë¦¬
            top_point_normalized = s2_pred["topk_points"][0]

            # í•©ì³ì§„ ì´ë¯¸ì§€ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
            s2_corrected_point = convert_merged_point_to_original(
                top_point_normalized, s2_crop_mappings, s2_merged_img.size
            )
            
            # Stage2 ì„±ê³µ ì—¬ë¶€ í™•ì¸
            stage2_success = point_in_bbox(s2_corrected_point, original_bbox)

            s2_inference_end = time.time()
            s2_time = s2_inference_end - s2_inference_start
            
            if TFOPS_PROFILING:
                s2_tflops = prof.get_total_flops() / 1e12

            s2_hit = "âœ…" if stage2_success else "âŒ"
            if stage2_success:
                stage2_success_count += 1

            #! ==================================================================
            #! [Stage 3] Ensemble Processing
            #! ==================================================================
            
            s3_ensemble_point = None
            stage3_ensemble_success = False
            skip_stage3 = False
            
            # í¬ë¡­ ë©´ì ì´ ì›ë³¸ ì´ë¯¸ì§€ ë©´ì ì˜ 50%ë¥¼ ë„˜ëŠ”ì§€ í™•ì¸
            crop_area = CROP_WIDTH * CROP_HEIGHT
            original_area = orig_w * orig_h
            crop_area_ratio = crop_area / original_area
            
            if crop_area_ratio > 0.5:
                # Stage3 ê±´ë„ˆë›°ê³  Stage2 ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                skip_stage3 = True
                s3_ensemble_point = s2_corrected_point
                stage3_ensemble_success = stage2_success
                s3_ensemble_time = 0.0
                
                print(f"ğŸ”„ Skipping Stage3: Crop area ratio {crop_area_ratio:.3f} > 0.5")
            else:
                s3_ensemble_start = time.time()
                
                # Stage1 ì–´í…ì…˜ì„ ì›ë³¸ í¬ê¸°ë¡œ ë³€í™˜
                stage1_attention_map = resize_attention_to_original(
                    s1_pred, original_image.size, s1_pred['resize_ratio']
                )
                
                # Stage2 ì–´í…ì…˜ì„ ì›ë³¸ í¬ê¸°ë¡œ ë³€í™˜ (Stage1ê³¼ ë™ì¼í•œ ë°©ì‹)
                stage2_attention_map = create_stage2_attention_to_original(
                    s2_pred, s2_crop_mappings, s2_merged_img, original_image.size, use_top_patches=ENSEMBLE_TOP_PATCHES
                )
                
                # ë‘ ì–´í…ì…˜ ë§µì„ ì§ì ‘ ì•™ìƒë¸” (0~1 ì •ê·œí™” í›„)
                # Stage1 ì •ê·œí™”
                s1_max = stage1_attention_map.max()
                if s1_max > 0:
                    stage1_normalized = stage1_attention_map / s1_max
                else:
                    stage1_normalized = np.zeros_like(stage1_attention_map)
                
                # Stage2 ì •ê·œí™”
                s2_max = stage2_attention_map.max()
                if s2_max > 0:
                    stage2_normalized = stage2_attention_map / s2_max
                else:
                    stage2_normalized = np.zeros_like(stage2_attention_map)
                
                # ìµœì¢… ì•™ìƒë¸” ë§µ ê³„ì‚°
                ensemble_map = (STAGE1_ENSEMBLE_RATIO * stage1_normalized + 
                               STAGE2_ENSEMBLE_RATIO * stage2_normalized)
                
                # ì•™ìƒë¸” ê²°ê³¼ì—ì„œ ìµœì ì  ì°¾ê¸°
                s3_ensemble_point, ensemble_score = find_ensemble_best_point(ensemble_map)
                
                # ì•™ìƒë¸” ê²°ê³¼ë¡œ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                stage3_ensemble_success = point_in_bbox(s3_ensemble_point, original_bbox)
                
                s3_ensemble_end = time.time()
                s3_ensemble_time = s3_ensemble_end - s3_ensemble_start
            
            s3_ensemble_hit = "âœ…" if stage3_ensemble_success else "âŒ"
            if stage3_ensemble_success:
                stage3_ensemble_success_count += 1

            #! ==================================================================
            #! [Common Processing]
            #! ==================================================================
            
            # ê³µí†µ í†µê³„ ì—…ë°ì´íŠ¸
            s1_time_sum += s1_time
            s2_time_sum += s2_time
            s3_time_sum += s3_ensemble_time
            s1_tflops_sum += s1_tflops
            s2_tflops_sum += s2_tflops
                
            # ì„±ëŠ¥ ë¡œê¹…
            total_time = s1_time + s2_time + s3_ensemble_time
            if TFOPS_PROFILING:
                total_tflops_this = s1_tflops + s2_tflops  # Stage3ëŠ” FLOPs ì œì™¸

            #! ==================================================================
            #! [Visualization - After Time Measurement]
            #! ==================================================================
            
            # ì‹œê°í™”ìš© ë””ë ‰í† ë¦¬ ì„¤ì • (stage3 ê²°ê³¼ì— ë”°ë¼)
            if VISUALIZE and (not VIS_ONLY_WRONG or not stage3_ensemble_success):
                inst_dir_name = re.sub(r'\W+', '_', instruction).strip('_')
                result_folder = "correct" if stage3_ensemble_success else "incorrect"
                inst_dir = os.path.join(save_dir, f"{task}_visualize_{result_folder}", f"{num_action}_{inst_dir_name}")

                # Stage1 ì‹œê°í™”
                from util.visualize_util import visualize_stage1_attention_crops
                visualize_stage1_attention_crops(
                    s1_pred=s1_pred,
                    resized_image=resized_image, 
                    crop_list=s1_crop_list,
                    original_image=original_image,
                    save_dir=inst_dir,
                    instruction=instruction,
                    gt_bbox=original_bbox,
                    s1_predicted_point=s1_original_point
                )
                
                # Stage2 ì‹œê°í™”
                from util.visualize_util import visualize_stage2_merged_attention
                visualize_stage2_merged_attention(
                    s2_pred=s2_pred,
                    merged_img=s2_merged_img,
                    save_dir=inst_dir,
                    instruction=instruction,
                    predicted_point=s2_corrected_point
                )
                
                # Stage3 ì•™ìƒë¸” ì‹œê°í™” (Stage3ê°€ ì‹¤í–‰ëœ ê²½ìš°ë§Œ)
                if not skip_stage3:
                    visualize_stage3_ensemble_attention(
                        ensemble_map=ensemble_map,
                        original_image=original_image,
                        crop_list=s1_crop_list,
                        original_bbox=original_bbox,
                        s3_ensemble_point=s3_ensemble_point,
                        s2_corrected_point=s2_corrected_point,
                        s1_original_point=s1_original_point,
                        stage1_ratio=STAGE1_ENSEMBLE_RATIO,
                        stage2_ratio=STAGE2_ENSEMBLE_RATIO,
                        save_dir=inst_dir,
                        vis_only_wrong=VIS_ONLY_WRONG,
                        stage3_success=stage3_ensemble_success
                    )
                else:
                    # Stage3ë¥¼ ê±´ë„ˆë›´ ê²½ìš°, Stage2 ê²°ê³¼ë¥¼ Stage3 ê²°ê³¼ë¡œ í‘œì‹œí•˜ëŠ” ê°„ë‹¨í•œ ì‹œê°í™”
                    print(f"ğŸ“ Stage3 skipped (crop area ratio: {crop_area_ratio:.3f}), using Stage2 result as final result")

            num_attention_crops = len(s1_crop_list)
            print(f"âœ‚ï¸  Attention Crops : {num_attention_crops}")
            print(f"ğŸ•– Times - S1: {s1_time:.2f}s | S2: {s2_time:.2f}s | S3: {s3_ensemble_time:.2f}s | Total: {total_time:.2f}s")
            if TFOPS_PROFILING:
                print(f"ğŸ”¥ FLOPs - S1: {s1_tflops:.2f} | S2: {s2_tflops:.2f} | Total: {total_tflops_this:.2f} TFLOPs")
            print(f"{'âœ… Success' if stage3_ensemble_success else 'âŒğŸ¯ Fail'}")

            #! ==================================================================
            #! [End]
            #! ==================================================================

            if MEMORY_EVAL:
                time.sleep(0.1)
                stop_flag = True
                monitor_thread.join()

                # í”¼í¬ ë©”ëª¨ë¦¬ ê³„ì‚° (GB ë‹¨ìœ„, ì†Œìˆ˜ì  3ìë¦¬)
                peak_memory = max(mem_log) if mem_log else 0.0
                peak_memory_gb = round(peak_memory, 3)

                if MEMORY_VIS:
                    import matplotlib.pyplot as plt
                    plt.figure(figsize=(10, 4))
                    plt.plot(time_log, mem_log)
                    plt.xlabel("Time (s)")
                    plt.ylabel("GPU Memory Allocated (GB)")
                    plt.title("GPU Memory Usage Over Time")
                    plt.grid(True)
                    plt.savefig(f"{memory_dir}/{num_action}_{filename}")
                    plt.close()  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ close ì¶”ê°€

            if MEMORY_EVAL:
                peak_memory_sum += peak_memory_gb

            # data_sourceë³„ í†µê³„ ì—…ë°ì´íŠ¸
            if data_source not in data_source_stats:
                data_source_stats[data_source] = {
                    'num_action': 0,
                    's1_time_sum': 0.0,
                    's2_time_sum': 0.0,
                    's3_time_sum': 0.0,
                    's1_tflops_sum': 0.0,
                    's2_tflops_sum': 0.0,
                    'total_tflops': 0.0,
                    'stage1_success_count': 0,
                    'crop_success_count': 0,
                    'stage2_success_count': 0,
                    'stage3_ensemble_success_count': 0,
                    'peak_memory_sum': 0.0
                }
            
            stats = data_source_stats[data_source]
            stats['num_action'] += 1
            stats['s1_time_sum'] += s1_time
            stats['s2_time_sum'] += s2_time
            stats['s3_time_sum'] += s3_ensemble_time
            if TFOPS_PROFILING:
                stats['s1_tflops_sum'] += s1_tflops
                stats['s2_tflops_sum'] += s2_tflops
                stats['total_tflops'] += total_tflops_this
            if MEMORY_EVAL:
                stats['peak_memory_sum'] += peak_memory_gb
            if s1_success:
                stats['stage1_success_count'] += 1
            if crop_success:
                stats['crop_success_count'] += 1
            if stage2_success:
                stats['stage2_success_count'] += 1
            if stage3_ensemble_success:
                stats['stage3_ensemble_success_count'] += 1

            up2now_s1_score = stage1_success_count / num_action * 100
            up2now_crop_score = crop_success_count / num_action * 100
            up2now_s2_score = stage2_success_count / num_action * 100
            up2now_s3_ensemble_score = stage3_ensemble_success_count / num_action * 100
            # print(f"Up2Now Crop Accuracy: {up2now_crop_score:.2f}%")
            print(f"Up2Now Stage1 Accuracy: {up2now_s1_score:.2f}%")
            print(f"Up2Now Stage2 Accuracy: {up2now_s2_score:.2f}%")
            print(f"Up2Now Stage3 Ensemble Accuracy: {up2now_s3_ensemble_score:.2f}%")

            # Iter log - ê°œì„ ëœ ë¡œê¹…
            append_iter_log(
                idx=j+1,
                orig_w=original_image.size[0],
                orig_h=original_image.size[1],
                resize_ratio=s1_pred['resize_ratio'],
                num_crop=num_attention_crops,
                crop_hit=crop_hit,
                s1_time=f"{s1_time:.3f}",
                s1_tflops=f"{s1_tflops:.2f}",
                s1_hit=s1_hit,
                s2_time=f"{s2_time:.3f}",
                s2_tflops=f"{s2_tflops:.2f}",
                s2_hit=s2_hit,
                s3_ensemble_time=f"{s3_ensemble_time:.3f}",
                s3_ensemble_hit=s3_ensemble_hit,
                total_time=f"{total_time:.3f}",
                total_tflops=f"{total_tflops_this:.2f}",
                peak_memory_gb=f"{peak_memory_gb:.3f}" if MEMORY_EVAL else "N/A",
                crop_acc_uptonow=f"{up2now_crop_score:.2f}",
                s1_acc_uptonow=f"{up2now_s1_score:.2f}",
                s2_acc_uptonow=f"{up2now_s2_score:.2f}",
                s3_ensemble_acc_uptonow=f"{up2now_s3_ensemble_score:.2f}",
                filename=filename_wo_ext,
                instruction=instruction[:50] + "..." if len(instruction) > 50 else instruction
            )

            # JSON ê¸°ë¡ - í•µì‹¬ ì •ë³´ë§Œ
            item_res = {
                'filename': filename,
                'orig_w': original_image.size[0],
                'orig_h': original_image.size[1],
                'instruction': instruction,
                'gt_bbox': original_bbox,
                'data_source': data_source,
                'num_crop': num_attention_crops,
                'crop_success': crop_success,
                'stage1_success': s1_success,
                'stage2_success': stage2_success,
                'stage3_ensemble_success': stage3_ensemble_success,
                's1_hit': s1_hit,
                'crop_hit': crop_hit,
                's2_hit': s2_hit,
                's3_ensemble_hit': s3_ensemble_hit,
                's3_ensemble_point': s3_ensemble_point,
                's1_original_point': s1_original_point,
                's2_original_point': s2_corrected_point,
                's1_time': s1_time,
                's2_time': s2_time,
                's3_ensemble_time': s3_ensemble_time,
                'total_time': total_time,
                's1_tflops': s1_tflops,
                's2_tflops': s2_tflops,
                'total_tflops': s1_tflops+s2_tflops,
                'peak_memory_gb': peak_memory_gb if MEMORY_EVAL else None,
                'ensemble_config': {
                    'attention_ratio': STAGE1_ENSEMBLE_RATIO,
                    'crop_ratio': STAGE2_ENSEMBLE_RATIO
                }
            }
            task_res.append(item_res)

        #! ==================================================
        # ê²°ê³¼ Json ì •ë¦¬
        os.makedirs(os.path.join(save_dir, "json"), exist_ok=True)
        with open(os.path.join(save_dir, "json", dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        # ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            "task": task,
            "total_samples": num_action,
            "crop_accuracy": crop_success_count / num_action * 100,
            "stage1_accuracy": stage1_success_count / num_action * 100,
            "stage2_accuracy": stage2_success_count / num_action * 100,
            "stage3_accuracy": stage3_ensemble_success_count / num_action * 100,
            "avg_times": {
                "stage1": s1_time_sum / num_action,
                "stage2": s2_time_sum / num_action,
                "stage3_ensemble": s3_time_sum / num_action,
                "total": (s1_time_sum + s2_time_sum + s3_time_sum) / num_action
            },
            "avg_flops_tflops": {
                "stage1": s1_tflops_sum / num_action,
                "stage2": s2_tflops_sum / num_action,
                "total": (s1_tflops_sum + s2_tflops_sum) / num_action
            },
            "avg_peak_memory_gb": round(peak_memory_sum / num_action, 3) if MEMORY_EVAL else None,
            "hyperparameters": {
                "select_threshold": SELECT_THRESHOLD,
                "attn_impl": ATTN_IMPL,
                "STAGE1_ensemble_ratio": STAGE1_ENSEMBLE_RATIO,
                "STAGE2_ensemble_ratio": STAGE2_ENSEMBLE_RATIO
            }
        }

        with open(os.path.join(save_dir, f"results_{task}.json"), "w") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=4)

        # data_sourceë³„ ë©”íŠ¸ë¦­ ì €ì¥
        data_source_metrics = {}
        for ds, stats in data_source_stats.items():
            if stats['num_action'] > 0:
                data_source_metrics[ds] = {
                    "task": task,
                    "data_source": ds,
                    "total_samples": stats['num_action'],
                    "crop_accuracy": stats['crop_success_count'] / stats['num_action'] * 100,
                    "stage1_accuracy": stats['stage1_success_count'] / stats['num_action'] * 100,
                    "stage2_accuracy": stats['stage2_success_count'] / stats['num_action'] * 100,
                    "stage3_accuracy": stats['stage3_ensemble_success_count'] / stats['num_action'] * 100,
                    "avg_times": {
                        "stage1": stats['s1_time_sum'] / stats['num_action'],
                        "stage2": stats['s2_time_sum'] / stats['num_action'],
                        "stage3_ensemble": stats['s3_time_sum'] / stats['num_action'],
                        "total": (stats['s1_time_sum'] + stats['s2_time_sum'] + stats['s3_time_sum']) / stats['num_action']
                    },
                    "avg_flops_tflops": {
                        "stage1": stats['s1_tflops_sum'] / stats['num_action'],
                        "stage2": stats['s2_tflops_sum'] / stats['num_action'],
                        "total": (stats['s1_tflops_sum'] + stats['s2_tflops_sum']) / stats['num_action']
                    },
                    "avg_peak_memory_gb": round(stats['peak_memory_sum'] / stats['num_action'], 3) if MEMORY_EVAL else None,
                    "hyperparameters": {
                        "select_threshold": SELECT_THRESHOLD,
                        "attn_impl": ATTN_IMPL,
                        "STAGE1_ensemble_ratio": STAGE1_ENSEMBLE_RATIO,
                        "STAGE2_ensemble_ratio": STAGE2_ENSEMBLE_RATIO
                    }
                }
        
        with open(os.path.join(save_dir, f"source_results_{task}.json"), "w") as dsf:
            json.dump(data_source_metrics, dsf, ensure_ascii=False, indent=4)

        # ì „ì²´ ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— í•œ ì¤„ ì¶”ê°€
        results_csv_path = "../_results"
        os.makedirs(results_csv_path, exist_ok=True)
        csv_file_path = os.path.join(results_csv_path, f"results_{task}.csv")
        
        # CSV í—¤ë” ì •ì˜
        csv_headers = [
            "method",
            "min_resize", "max_resize", "select_threshold", "stage1_ensemble_ratio", "crop_width", "crop_height",
            "total_samples", "crop_accuracy", "stage1_accuracy", "stage2_accuracy", "stage3_accuracy",
            "avg_stage1_time", "avg_stage2_time", "avg_stage3_time", "avg_total_time",
            "avg_stage1_tflops", "avg_stage2_tflops", "avg_total_tflops", "avg_peak_memory_gb",
            "timestamp"
        ]
        
        # CSV ë°ì´í„° í–‰ ìƒì„±
        import datetime
        csv_row = [
            method,
            MIN_RESIZE, MAX_RESIZE, SELECT_THRESHOLD, STAGE1_ENSEMBLE_RATIO, CROP_WIDTH, CROP_HEIGHT,
            num_action, 
            round(metrics['crop_accuracy'], 2),
            round(metrics['stage1_accuracy'], 2),
            round(metrics['stage2_accuracy'], 2), 
            round(metrics['stage3_accuracy'], 2),
            round(metrics['avg_times']['stage1'], 4),
            round(metrics['avg_times']['stage2'], 4),
            round(metrics['avg_times']['stage3_ensemble'], 4),
            round(metrics['avg_times']['total'], 4),
            round(metrics['avg_flops_tflops']['stage1'], 2),
            round(metrics['avg_flops_tflops']['stage2'], 2),
            round(metrics['avg_flops_tflops']['total'], 2),
            metrics['avg_peak_memory_gb'] if metrics['avg_peak_memory_gb'] else 0.0,
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        ]
        
        # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±, ìˆìœ¼ë©´ ë°ì´í„° í–‰ë§Œ ì¶”ê°€
        import csv
        file_exists = os.path.exists(csv_file_path)
        
        with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ í—¤ë” ì¶”ê°€
            if not file_exists or os.path.getsize(csv_file_path) == 0:
                writer.writerow(csv_headers)
            
            # ë°ì´í„° í–‰ ì¶”ê°€
            writer.writerow(csv_row)
        
        print(f"ğŸ“ Results saved to CSV: {csv_file_path}")

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("=" * 60)
        print(f"ğŸ“Š Final Results for {task}:")
        print(f"Total Samples: {num_action}")
        print(f"Crop Accuracy: {metrics['crop_accuracy']:.2f}%")
        print(f"Stage1 Accuracy: {metrics['stage1_accuracy']:.2f}%")
        print(f"Stage2 Accuracy: {metrics['stage2_accuracy']:.2f}%")
        print(f"Stage3 Ensemble Accuracy: {metrics['stage3_accuracy']:.2f}%")
        print(f"Avg Times: S1 {metrics['avg_times']['stage1']:.3f}s | S2 {metrics['avg_times']['stage2']:.3f}s | S3 {metrics['avg_times']['stage3_ensemble']:.3f}s | Total {metrics['avg_times']['total']:.3f}s")
        print(f"Avg FLOPs: S1 {metrics['avg_flops_tflops']['stage1']:.2f} | S2 {metrics['avg_flops_tflops']['stage2']:.2f} | Total {metrics['avg_flops_tflops']['total']:.2f} TFLOPs")
        if MEMORY_EVAL and metrics['avg_peak_memory_gb'] is not None:
            print(f"Avg Peak Memory: {metrics['avg_peak_memory_gb']:.3f} GB")
        print(f"Ensemble Config: Attention {STAGE1_ENSEMBLE_RATIO:.1f}, Crop {STAGE2_ENSEMBLE_RATIO:.1f}")
        
        print("=" * 60)