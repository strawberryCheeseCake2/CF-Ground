import os
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('gpu', type=int, help='GPU number')
parser.add_argument('--r', type=float, nargs=2, metavar=('MIN_RESIZE', 'MAX_RESIZE'), help='Stage 1 Resize ratio range (min max)')
parser.add_argument('--e', type=float, help='Stage 1 Ensemble ratio')
parser.add_argument('--v', action='store_true', help='Whether to save visualization images')
args = parser.parse_args()

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)  # ëª‡ë²ˆ GPU ì‚¬ìš©í• ì§€ argumentë¡œ ì§€ì • : run_gui_actor.py 2 -> 2ë²ˆ GPU

#! Hyperparameter =====================================================================================

ATTN_IMPL = "eager"  # attention implement "eager" "sdpa" "flash" "efficient"

# Image Resize Ratios

MIN_RESIZE = args.r[0] if args.r else 0.50  # DYNAMIC_RESIZE ë¹„ìœ¨ ìµœì†Œê°’
MAX_RESIZE = args.r[1] if args.r else 0.50  # DYNAMIC_RESIZE ë¹„ìœ¨ ìµœëŒ€ê°’

# Crop Limitations
MAX_CROPS = 3  # ìƒì„±í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ crop ê°œìˆ˜
SELECT_THRESHOLD = 0.50  #! score >= tau * max_score ì¸ ëª¨ë“  crop select
CROP_WIDTH = 1176  # í¬ë¡­í•  ì§ì‚¬ê°í˜• ê°€ë¡œ í¬ê¸° (ì•„ì´í° ì „ì²´ ê°€ë¡œê°€ 1170px)
CROP_HEIGHT = 602  # í¬ë¡­í•  ì§ì‚¬ê°í˜• ì„¸ë¡œ í¬ê¸°

# Ensemble Hyperparameters
# TODO: ì´ê²ƒë„ resizeì²˜ëŸ¼ ë™ì ìœ¼ë¡œ ì¸¡ì •í•´ì„œ ë³€ê²½ ê°€ëŠ¥í•˜ë„ë¡
STAGE1_ENSEMBLE_RATIO = args.e if args.e else 0.50  # Stage1 attention ê°€ì¤‘ì¹˜
STAGE2_ENSEMBLE_RATIO = 1 - STAGE1_ENSEMBLE_RATIO  # Stage2 crop ê°€ì¤‘ì¹˜
ENSEMBLE_TOP_PATCHES = 100                         # Stage2ì—ì„œ ì•™ìƒë¸”ì— ì‚¬ìš©í•  ìƒìœ„ íŒ¨ì¹˜ ê°œìˆ˜

# ìµœëŒ€ PIXELS ì œí•œ
MAX_PIXELS = 3211264  # Processë‹¨ì—ì„œ ì ìš©

# csvì— ê¸°ë¡í•  method ì´ë¦„
# method = "dynamic_resize"
method = "multi_fixed_resize"

memo = f"resize_{MIN_RESIZE:.2f}~{MAX_RESIZE:.2f}_ensemble{STAGE1_ENSEMBLE_RATIO}_crop{CROP_WIDTH}x{CROP_HEIGHT}"

#! Argument ==========================================================================================

SEED = 0

# Dataset & Model
MLLM_PATH = "microsoft/GUI-Actor-3B-Qwen2.5-VL"
SCREENSPOT_IMGS = "../data/screenspotv2_image"  # input image ê²½ë¡œ
SCREENSPOT_JSON = "../data"  # input image jsoníŒŒì¼ ê²½ë¡œ
TASKS = ["mobile", "web", "desktop"]
# TASKS = ["mobile"]
# TASKS = ["web"]
# TASKS = ["desktop"]
SAMPLE_RANGE = slice(None)
# SAMPLE_RANGE = slice(0,2)

# Visualize & Logging
VISUALIZE = args.v if args.v else False
VIS_ONLY_WRONG = False  # Trueë©´ í‹€ë¦° ê²ƒë§Œ ì‹œê°í™”, Falseë©´ ëª¨ë“  ê²ƒ ì‹œê°í™”
TFOPS_PROFILING = True
MEMORY_EVAL = True
MEMORY_VIS = False

# Save Path
SAVE_DIR = f"../attn_output/" + method + "/" + memo

#! ==================================================================================================

# Standard Library
import os
import sys
import time
import threading
import re
import json
import logging
logging.disable(logging.CRITICAL)  # ëª¨ë“  ë¡œê¹… í˜¸ì¶œ ë¬´ë ¥í™”

# Third-Party Libraries
import numpy as np
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoTokenizer, set_seed

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from util.iter_logger import init_iter_logger, append_iter_log  # log csv ê¸°ë¡ íŒŒì¼
from gui_actor.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer
from gui_actor.inference import inference
from gui_actor.multi_image_inference import multi_image_inference
from util.visualize_util import visualize_stage1_attention_crops, visualize_stage2_merged_attention, visualize_stage2_multi_attention, visualize_stage3_ensemble_attention, visualize_stage3_point_ensemble
from util.sharpness_util import get_fft_blur_score
if TFOPS_PROFILING:
    from deepspeed.profiling.flops_profiler import FlopsProfiler

#! ==============================================================================================

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        return super(NpEncoder, self).default(obj)

def resize_image(image, resize_to_pixels):
    image_width, image_height = image.size
    if (resize_to_pixels is not None) and ((image_width * image_height) > resize_to_pixels):
        resize_ratio = (resize_to_pixels / (image_width * image_height)) ** 0.5
        image_width_resized, image_height_resized = int(image_width * resize_ratio), int(image_height * resize_ratio)
        image = image.resize((image_width_resized, image_height_resized))
        print(f"ğŸ”§ Resized image: {image_width}x{image_height} -> {image_width_resized}x{image_height_resized} (ratio: {resize_ratio:.3f})")
    return image

def warm_up_model(model, tokenizer, processor):
    print("ğŸ‹ï¸â€â™‚ï¸ Warming up the model...")
    dummy_instruction = "This is a dummy instruction for warm-up."
    dummy_image = Image.new("RGB", (1000, 1000), color=(255, 255, 255))  # 1000x1000 í°ìƒ‰ ì´ë¯¸ì§€
    dummy_msgs = create_conversation_stage1(image=dummy_image, instruction=dummy_instruction, resize_ratio=1.0)
    
    # ì˜ˆì—´ìš© inference ì‹¤í–‰
    for _ in range(3):  # 3ë²ˆ ë°˜ë³µ
        with torch.no_grad():
            _ = inference(dummy_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    print("ğŸ‹ï¸â€â™‚ï¸ Warm-up complete!")

def create_conversation_stage1(image, instruction, resize_ratio):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    # "text": (
                    #     "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                    #     "your task is to locate the screen element that corresponds to the instruction. "
                    #     "You should output a PyAutoGUI action that performs a click on the correct position. "
                    #     "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                    #     "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    # ),
                    "text": (
                        f"This is a resized screenshot of the whole GUI, scaled by {resize_ratio}. "
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def create_conversation_stage2(image, instruction, crop_cnt):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    # "text": (
                    #     "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                    #     "your task is to locate the screen element that corresponds to the instruction. "
                    #     "You should output a PyAutoGUI action that performs a click on the correct position. "
                    #     "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                    #     "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    # ),
                    "text": (
                        f"This image is a vertical collage of {crop_cnt} cropped regions that were selected as promising. "
                        "Each crop has the same width and fixed height, separated by a red horizontal line. "
                        "You are a GUI agent. Given this collage image and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def run_stage1_attention_inference(original_image, instruction):
    """Stage 1: ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  inference"""

    orig_w, orig_h = original_image.size
    # ì´ë¯¸ì§€ ê³ ì • ë¦¬ì‚¬ì´ì¦ˆ
    if MIN_RESIZE == MAX_RESIZE:
        resize_ratio = MIN_RESIZE
        resized_w, resized_h = int(orig_w * resize_ratio), int(orig_h * resize_ratio)
        print(f"ğŸ”§ Fixed Resized image: {orig_w}x{orig_h} -> {resized_w}x{resized_h} (ratio: {resize_ratio:.3f})")
    
    # ì´ë¯¸ì§€ ë™ì  ë¦¬ì‚¬ì´ì¦ˆ
    else:
        downsampled = original_image.resize((int(orig_w*0.5), int(orig_h*0.5)))
        resize_ratio = get_fft_blur_score(downsampled, min_resize=MIN_RESIZE, max_resize=MAX_RESIZE)
        resized_w, resized_h = int(orig_w * resize_ratio), int(orig_h * resize_ratio)
        print(f"ğŸ”§ Dynamic Resized image: {orig_w}x{orig_h} -> {resized_w}x{resized_h} (ratio: {resize_ratio:.3f})")
    
    # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ë¡œ inference
    resized_image = original_image.resize((resized_w, resized_h))
    conversation = create_conversation_stage1(resized_image, instruction, resize_ratio)
    pred = inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=1)
    
    # ê²°ê³¼ì— ë¦¬ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ê°€
    pred['resize_ratio'] = resize_ratio
    pred['original_size'] = (orig_w, orig_h)
    pred['resized_size'] = resized_image.size
    
    return pred, resized_image

def find_attention_peaks(pred_result, resized_image, resize_ratio):
    """ì–´í…ì…˜ì—ì„œ ê³ ì ë“¤ ì°¾ê¸° (bbox ìƒì„± í›„ ì¤‘ì‹¬ì  ê±°ë¦¬ë¡œ ì¤‘ë³µ ì œê±°)"""
    
    attn_scores = np.array(pred_result['attn_scores'][0])
    n_width = pred_result['n_width'] 
    n_height = pred_result['n_height']
    
    resized_w, resized_h = resized_image.size
    
    # íŒ¨ì¹˜ë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ë¹„ìœ¨
    patch_w = resized_w / n_width
    patch_h = resized_h / n_height
    
    # ì ìˆ˜ê°€ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬
    sorted_indices = np.argsort(attn_scores)[::-1]
    
    peaks = []
    used_bbox_centers = []  # ì´ë¯¸ ì‚¬ìš©ëœ bbox ì¤‘ì‹¬ì ë“¤
    
    for idx in sorted_indices:
        # íŒ¨ì¹˜ ì¢Œí‘œ ê³„ì‚°
        patch_y = idx // n_width
        patch_x = idx % n_width
        
        # íŒ¨ì¹˜ ì¤‘ì‹¬ì ì˜ í”½ì…€ ì¢Œí‘œ (ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ê¸°ì¤€)
        center_x = (patch_x + 0.5) * patch_w
        center_y = (patch_y + 0.5) * patch_h
        
        # ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
        orig_center_x = center_x / resize_ratio
        orig_center_y = center_y / resize_ratio
        
        # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ bbox ê³„ì‚° (ì‹¤ì œ í¬ë¡­ì´ ìƒì„±ë  ìœ„ì¹˜)
        orig_w = resized_w / resize_ratio
        orig_h = resized_h / resize_ratio
        
        left = max(0, int(orig_center_x - CROP_WIDTH/2))
        top = max(0, int(orig_center_y - CROP_HEIGHT/2))
        right = min(orig_w, int(orig_center_x + CROP_WIDTH/2))
        bottom = min(orig_h, int(orig_center_y + CROP_HEIGHT/2))
        
        # ê²½ê³„ì—ì„œ ì˜ë ¸ì„ ê²½ìš° ì¡°ì •
        if right - left < CROP_WIDTH and right < orig_w:
            right = min(orig_w, left + int(CROP_WIDTH))
        if right - left < CROP_WIDTH and left > 0:
            left = max(0, right - int(CROP_WIDTH))
        if bottom - top < CROP_HEIGHT and bottom < orig_h:
            bottom = min(orig_h, top + int(CROP_HEIGHT))
        if bottom - top < CROP_HEIGHT and top > 0:
            top = max(0, bottom - int(CROP_HEIGHT))
        
        # ì‹¤ì œ bbox ì¤‘ì‹¬ì  ê³„ì‚°
        bbox_center_x = (left + right) / 2
        bbox_center_y = (top + bottom) / 2
        
        # ì´ë¯¸ ì‚¬ìš©ëœ bboxì™€ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸
        skip = False
        for used_center in used_bbox_centers:
            x_distance = abs(bbox_center_x - used_center[0])
            y_distance = abs(bbox_center_y - used_center[1])
            # bbox ì¤‘ì‹¬ì ì´ ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ìŠ¤í‚µ (í¬ë¡­ í¬ê¸°ì˜ 30% ì´ë‚´)
            if x_distance < CROP_WIDTH * 0.3 and y_distance < CROP_HEIGHT * 0.3:  #! í¬ë¡­
                skip = True
                break
        
        if not skip:
            peaks.append({
                'center_x': center_x,  # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ê¸°ì¤€ (ê¸°ì¡´ í˜¸í™˜ì„±)
                'center_y': center_y,
                'score': float(attn_scores[idx]),
                'patch_x': patch_x,
                'patch_y': patch_y,
                'bbox_center': (bbox_center_x, bbox_center_y)  # ì‹¤ì œ bbox ì¤‘ì‹¬ì 
            })
            
            used_bbox_centers.append((bbox_center_x, bbox_center_y))
    
    return peaks

def filter_by_threshold(peaks, threshold=0.7, max_crops=MAX_CROPS):
    """1ë“±ì˜ threshold% ì´ìƒì¸ peakë“¤ë§Œ ë‚¨ê¸°ê³ , ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì ìš©"""
    
    if not peaks:
        return []
    
    max_score = peaks[0]['score']  # ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìŒ
    min_score = max_score * threshold
    
    # threshold ì¡°ê±´ìœ¼ë¡œ ë¨¼ì € í•„í„°ë§
    filtered_peaks = [peak for peak in peaks if peak['score'] >= min_score]
    
    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì ìš© (ìƒìœ„ ì ìˆ˜ìˆœìœ¼ë¡œ)
    if len(filtered_peaks) > max_crops:
        filtered_peaks = filtered_peaks[:max_crops]
    
    print(f"ğŸ¯ Found {len(peaks)} peaks, filtered to {len(filtered_peaks)} (threshold: {threshold}, max_crops: {max_crops})")
    
    return filtered_peaks

def create_crops_from_attention_peaks(peaks, original_image, resize_ratio):
    """Attention peaksë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ crop"""
    
    if not peaks:
        return []
    
    crops = []
    orig_w, orig_h = original_image.size
    
    for i, peak in enumerate(peaks):
        # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ì—ì„œì˜ centerë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³€í™˜
        orig_center_x = peak['center_x'] / resize_ratio
        orig_center_y = peak['center_y'] / resize_ratio
        
        # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œì˜ bbox ê³„ì‚°
        left = max(0, int(orig_center_x - CROP_WIDTH/2))
        top = max(0, int(orig_center_y - CROP_HEIGHT/2))
        right = min(orig_w, int(orig_center_x + CROP_WIDTH/2))
        bottom = min(orig_h, int(orig_center_y + CROP_HEIGHT/2))
        
        # ê²½ê³„ì—ì„œ ì˜ë ¸ì„ ê²½ìš° ì¡°ì •
        if right - left < CROP_WIDTH and right < orig_w:
            right = min(orig_w, left + int(CROP_WIDTH))
        if right - left < CROP_WIDTH and left > 0:
            left = max(0, right - int(CROP_WIDTH))
        if bottom - top < CROP_HEIGHT and bottom < orig_h:
            bottom = min(orig_h, top + int(CROP_HEIGHT))
        if bottom - top < CROP_HEIGHT and top > 0:
            top = max(0, bottom - int(CROP_HEIGHT))
        
        bbox = [left, top, right, bottom]
        crop_img = original_image.crop(bbox)
        
        crops.append({
            'img': crop_img,
            'bbox': bbox,
            'score': peak['score'],
            'id': i + 1
        })
        
        # print(f"ğŸ”§ Crop {i+1}: center=({orig_center_x:.1f}, {orig_center_y:.1f}), bbox={bbox}, size={CROP_WIDTH}x{CROP_HEIGHT}")
    
    return crops

def create_merged_image_for_stage2(crops):
    """Stage 2ìš©: cropë“¤ì„ ì„¸ë¡œë¡œ í•©ì¹˜ê¸° (ë¹¨ê°„ìƒ‰ êµ¬ë¶„ì„  ì´ë¯¸ì§€ë¡œ ë¶„ë¦¬) - bbox yì¢Œí‘œ ìˆœìœ¼ë¡œ ì •ë ¬"""

    if not crops:
        return None, []
    
    # bboxì˜ yì¢Œí‘œ(top) ìˆœìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
    sorted_crops = sorted(crops, key=lambda crop: crop['bbox'][1])
    
    # ì„¸ë¡œë¡œ í•©ì¹  ì´ë¯¸ì§€ë“¤ê³¼ êµ¬ë¶„ì„ ë“¤ì„ ë³„ë„ë¡œ ì¤€ë¹„
    separator_height = 28  # ë¹¨ê°„ìƒ‰ êµ¬ë¶„ì„  ë‘ê»˜
    max_width = max(crop['img'].width for crop in sorted_crops)
    
    # í•©ì¹  ì´ë¯¸ì§€ë“¤ ë¦¬ìŠ¤íŠ¸ (crop ì´ë¯¸ì§€ + êµ¬ë¶„ì„  ì´ë¯¸ì§€ë“¤)
    images_to_merge = []
    crop_y_mappings = []
    current_y = 0
    
    for i, crop in enumerate(sorted_crops):
        # crop ì´ë¯¸ì§€ ì¶”ê°€
        images_to_merge.append(crop['img'])
        
        # ë§¤í•‘ ì •ë³´ ì €ì¥: (merged_y_start, merged_y_end) -> (original_bbox)
        paste_x = (max_width - crop['img'].width) // 2
        crop_y_mappings.append({
            'merged_y_start': current_y,
            'merged_y_end': current_y + crop['img'].height,
            'original_bbox': crop['bbox'],
            'paste_x': paste_x
        })
        
        current_y += crop['img'].height
        
        # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ë¹¨ê°„ìƒ‰ êµ¬ë¶„ì„  ì´ë¯¸ì§€ ì¶”ê°€
        if i < len(sorted_crops) - 1:
            separator_img = Image.new('RGB', (max_width, separator_height), color=(256, 0, 0))
            images_to_merge.append(separator_img)
            current_y += separator_height
    
    # ì´ ë†’ì´ ê³„ì‚°
    total_height = current_y
    
    # í•©ì³ì§„ ì´ë¯¸ì§€ ìƒì„±
    merged_img = Image.new('RGB', (max_width, total_height), color=(0, 0, 0))
    
    # ì´ë¯¸ì§€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë¶™ì´ê¸°
    paste_y = 0
    image_idx = 0
    
    for i, crop in enumerate(sorted_crops):
        # crop ì´ë¯¸ì§€ ë¶™ì´ê¸° (ì¤‘ì•™ ì •ë ¬)
        crop_img = images_to_merge[image_idx]
        paste_x = (max_width - crop_img.width) // 2
        merged_img.paste(crop_img, (paste_x, paste_y))
        paste_y += crop_img.height
        image_idx += 1
        
        # êµ¬ë¶„ì„  ì´ë¯¸ì§€ ë¶™ì´ê¸° (ë§ˆì§€ë§‰ì´ ì•„ë‹Œ ê²½ìš°)
        if i < len(sorted_crops) - 1:
            separator_img = images_to_merge[image_idx]
            merged_img.paste(separator_img, (0, paste_y))
            paste_y += separator_img.height
            image_idx += 1
    
    return merged_img, crop_y_mappings


def create_multi_image_msgs(crop_list, instruction):
    user_content = []
    for crop in crop_list:
        img = crop["img"]  # "resized_img" -> "img" ìˆ˜ì •
        user_content.append({"type": "image", "image": img})

    user_content.append({
        "type": "text",
        "text": instruction,
    })
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": user_content,
        },
    ]
    return conversation

def run_stage2_multi_image_inference(crop_list, instruction):
    """Stage 2: multi image inference - ê° cropë³„ë¡œ ê°œë³„ inference"""
    
    # multi image inferenceìš© ëŒ€í™” ìƒì„±
    conversation = create_multi_image_msgs(crop_list, instruction)
    
    # multi image inference ì‹¤í–‰ (ê° ì´ë¯¸ì§€ë³„ ê²°ê³¼ ë°˜í™˜)
    pred = multi_image_inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=10)
    
    return pred

def convert_multi_image_results_to_original(multi_pred, crop_list):
    """multi_image_inference ê²°ê³¼ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜í•˜ê³  ì•™ìƒë¸”"""
    
    if not multi_pred.get('per_image') or not crop_list:
        return None, []
    
    # ê° cropë³„ ê²°ê³¼ë¥¼ ì›ë³¸ ì¢Œí‘œë¡œ ë³€í™˜
    converted_results = []
    all_candidates = []
    
    for img_idx, img_result in enumerate(multi_pred['per_image']):
        if img_idx >= len(crop_list):
            continue
            
        crop_info = crop_list[img_idx]
        crop_bbox = crop_info['bbox']  # [left, top, right, bottom]
        crop_width = crop_bbox[2] - crop_bbox[0]
        crop_height = crop_bbox[3] - crop_bbox[1]
        
        # í•´ë‹¹ ì´ë¯¸ì§€ì˜ topk ê²°ê³¼ë“¤ì„ ì›ë³¸ ì¢Œí‘œë¡œ ë³€í™˜
        crop_candidates = []
        for point_idx, (point, score) in enumerate(zip(img_result['topk_points'], img_result['topk_values'])):
            # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ crop ë‚´ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            crop_x = point[0] * crop_width
            crop_y = point[1] * crop_height
            
            # crop ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
            original_x = crop_bbox[0] + crop_x
            original_y = crop_bbox[1] + crop_y
            
            candidate = {
                'point': [original_x, original_y],
                'score': score,
                'crop_id': crop_info['id'],
                'crop_bbox': crop_bbox,
                'rank_in_crop': point_idx
            }
            crop_candidates.append(candidate)
            all_candidates.append(candidate)
        
        converted_results.append({
            'crop_id': crop_info['id'],
            'crop_bbox': crop_bbox,
            'candidates': crop_candidates
        })
    
    # ëª¨ë“  í›„ë³´ë“¤ì„ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìµœì¢… ì•™ìƒë¸” ê²°ê³¼ ìƒì„±
    all_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    # ìµœê³  ì ìˆ˜ í›„ë³´ë¥¼ ìµœì¢… ì˜ˆì¸¡ìœ¼ë¡œ ì„ íƒ
    best_candidate = all_candidates[0] if all_candidates else None
    
    return best_candidate, all_candidates

def create_stage2_attention_to_original_multi(multi_pred, crop_list, original_size):
    """multi_image_inference ê²°ê³¼ë¡œë¶€í„° ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ì˜ ì–´í…ì…˜ ë§µ ìƒì„±"""
    
    if not multi_pred.get('per_image') or not crop_list:
        return None
    
    orig_w, orig_h = original_size
    stage2_attention_map = np.zeros((orig_h, orig_w), dtype=np.float32)
    overlap_count_map = np.zeros((orig_h, orig_w), dtype=np.int32)
    
    # ê° cropì˜ ìƒìœ„ 100ê°œ íŒ¨ì¹˜ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ì— ë§¤í•‘
    for img_idx, img_result in enumerate(multi_pred['per_image']):
        if img_idx >= len(crop_list):
            continue
            
        crop_info = crop_list[img_idx]
        crop_bbox = crop_info['bbox']  # [left, top, right, bottom]
        crop_width = crop_bbox[2] - crop_bbox[0]
        crop_height = crop_bbox[3] - crop_bbox[1]
        
        # í•´ë‹¹ ì´ë¯¸ì§€ì˜ ì–´í…ì…˜ ìŠ¤ì½”ì–´ì™€ ê·¸ë¦¬ë“œ ì •ë³´
        attn_scores = np.array(img_result['attn_scores'][0])
        n_width = img_result['n_width']
        n_height = img_result['n_height']
        
        # ìƒìœ„ 100ê°œ íŒ¨ì¹˜ ì„ íƒ
        top_indices = np.argsort(attn_scores)[-100:][::-1]  # ìƒìœ„ 100ê°œ
        
        for patch_idx in top_indices:
            # íŒ¨ì¹˜ ì¢Œí‘œ ê³„ì‚° (crop ë‚´ì—ì„œ)
            patch_y = patch_idx // n_width
            patch_x = patch_idx % n_width
            
            # íŒ¨ì¹˜ ì¤‘ì‹¬ì ì˜ ì •ê·œí™”ëœ ì¢Œí‘œ (crop ë‚´ì—ì„œ)
            norm_x = (patch_x + 0.5) / n_width
            norm_y = (patch_y + 0.5) / n_height
            
            # crop ë‚´ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            crop_pixel_x = norm_x * crop_width
            crop_pixel_y = norm_y * crop_height
            
            # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
            orig_pixel_x = crop_bbox[0] + crop_pixel_x
            orig_pixel_y = crop_bbox[1] + crop_pixel_y
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë²”ìœ„ ë‚´ë¡œ í´ë¦¬í•‘
            orig_pixel_x = max(0, min(orig_w - 1, orig_pixel_x))
            orig_pixel_y = max(0, min(orig_h - 1, orig_pixel_y))
            
            # ì •ìˆ˜ ì¢Œí‘œë¡œ ë³€í™˜
            x_int = int(orig_pixel_x)
            y_int = int(orig_pixel_y)
            
            # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ëˆ„ì  ë° ê²¹ì¹¨ ì¹´ìš´íŠ¸
            stage2_attention_map[y_int, x_int] += attn_scores[patch_idx]
            overlap_count_map[y_int, x_int] += 1
    
    # ê²¹ì¹˜ëŠ” ë¶€ë¶„ì€ ê²¹ì¹˜ëŠ” íšŸìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ê°€ì¤‘í‰ê·  ì²˜ë¦¬
    valid_mask = overlap_count_map > 0
    stage2_attention_map[valid_mask] = stage2_attention_map[valid_mask] / overlap_count_map[valid_mask]
    
    return stage2_attention_map

def run_stage1_attention_based(original_image, instruction, gt_bbox):
    """ìƒˆë¡œìš´ ê°„ë‹¨í•œ Stage 1: Attention ê¸°ë°˜ crop ìƒì„±"""
    
    # 1. ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  inference
    print("ğŸ” Stage 1: Running attention-based inference...")
    s1_pred, resized_image = run_stage1_attention_inference(original_image, instruction)
    
    # 2. GT bboxë„ ë¦¬ì‚¬ì´ì¦ˆ ë¹„ìœ¨ì— ë§ì¶° ì¡°ì •
    resize_ratio = s1_pred['resize_ratio']
    scaled_gt_bbox = [coord * resize_ratio for coord in gt_bbox]
    
    # 3. Attention ê³ ì ë“¤ ì°¾ê¸°
    peaks = find_attention_peaks(s1_pred, resized_image, resize_ratio)
    
    if not peaks:
        print("âš ï¸ No attention peaks found")
        return s1_pred, [], 0, resized_image, scaled_gt_bbox
    
    # 4. 1ë“±ì˜ 70% ì´ìƒë§Œ ë‚¨ê¸°ê³  ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì ìš©
    filtered_peaks = filter_by_threshold(peaks, threshold=SELECT_THRESHOLD, max_crops=MAX_CROPS)
    
    if not filtered_peaks:
        print("âš ï¸ No peaks passed threshold")
        return s1_pred, [], 0, resized_image, scaled_gt_bbox
    
    # 5. ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ crop ìƒì„±
    crops = create_crops_from_attention_peaks(filtered_peaks, original_image, resize_ratio)
    
    num_crops = len(crops)
    
    return s1_pred, crops, num_crops, resized_image, scaled_gt_bbox

def get_stage1_score_at_point(point, s1_attn_scores, s1_n_width, s1_n_height, original_size, resize_ratio):
    """íŠ¹ì • ì ì—ì„œì˜ Stage1 ì–´í…ì…˜ ì ìˆ˜ë¥¼ ê³„ì‚°"""
    
    orig_w, orig_h = original_size
    point_x, point_y = point
    
    # ì›ë³¸ ì¢Œí‘œë¥¼ ë¦¬ì‚¬ì´ì¦ˆëœ ì¢Œí‘œë¡œ ë³€í™˜
    resized_x = point_x * resize_ratio
    resized_y = point_y * resize_ratio
    
    # ë¦¬ì‚¬ì´ì¦ˆëœ ì¢Œí‘œë¥¼ íŒ¨ì¹˜ ì¢Œí‘œë¡œ ë³€í™˜
    resized_w = orig_w * resize_ratio
    resized_h = orig_h * resize_ratio
    
    patch_x = int((resized_x / resized_w) * s1_n_width)
    patch_y = int((resized_y / resized_h) * s1_n_height)
    
    # íŒ¨ì¹˜ ì¢Œí‘œê°€ ìœ íš¨í•œ ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
    patch_x = max(0, min(patch_x, s1_n_width - 1))
    patch_y = max(0, min(patch_y, s1_n_height - 1))
    
    # í•´ë‹¹ íŒ¨ì¹˜ì˜ ì–´í…ì…˜ ì ìˆ˜ ë°˜í™˜
    patch_idx = patch_y * s1_n_width + patch_x
    if patch_idx < len(s1_attn_scores):
        return float(s1_attn_scores[patch_idx])
    else:
        return 0.0

def point_in_bbox(point, bbox):
    """ì ì´ bbox ì•ˆì— ìˆëŠ”ì§€ í™•ì¸"""
    if point is None or bbox is None:
        return False
    return bbox[0] <= point[0] <= bbox[2] and bbox[1] <= point[1] <= bbox[3]

def monitor_memory(interval=0.1):
    start = time.time()
    while not stop_flag:
        mem = torch.cuda.memory_allocated() / 1024**3
        now = time.time() - start
        mem_log.append(mem)
        time_log.append(now)
        time.sleep(interval)

#! ================================================================================================

if __name__ == '__main__':

    set_seed(SEED)

    # Model Import (NVIDIA CUDA)
    model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
        device_map="balanced",
        # max_memory=max_memory, 
        low_cpu_mem_usage=True
    )
    # Model Import (Mac)
    # model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
    #     MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
    #     device_map="mps", # Mac
    #     # max_memory=max_memory, 
    #     low_cpu_mem_usage=False
    # )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH, max_pixels=MAX_PIXELS)
    prof = FlopsProfiler(model)

    warm_up_model(model, tokenizer, processor)

    if TFOPS_PROFILING:
        prof.start_profile()

    # save_dir í´ë”ëª…ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„± (save_dir -> save_dir_1 -> save_dir_2)
    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    # Process
    for task in TASKS:
        # ê° taskë³„ë¡œ ë³„ë„ì˜ ë¡œê·¸ íŒŒì¼ ìƒì„±
        init_iter_logger(  
            save_dir=save_dir,
            csv_name=f"iter_log_{task}.csv",
            md_name=f"iter_log_{task}.md",
            headers=[  # ìˆœì„œ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°
                "idx", "orig_w", "orig_h", "resize_ratio",
                "num_crop", "crop_hit",
                "s1_time", "s1_tflops", "s1_hit", 
                "s2_time", "s2_tflops", "s2_hit", 
                "s3_time", "s3_hit",
                "total_time", "total_tflops", "peak_memory_gb", 
                "crop_acc_uptonow", "s1_acc_uptonow", "s2_acc_uptonow", "s3_acc_uptonow",
                "filename", "instruction"
            ],
            write_md=False, use_fsync=True, use_lock=True
        )
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"
        screenspot_data = json.load(open(os.path.join(SCREENSPOT_JSON, dataset), 'r'))
        screenspot_data = screenspot_data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        # í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        task_res = []
        num_action = 0
        s1_time_sum = s2_time_sum = s3_time_sum = 0.0
        s1_tflops_sum = s2_tflops_sum = 0.0
        stage1_success_count = stage2_success_count = stage3_success_count = 0
        crop_success_count = 0  # ìƒˆë¡œìš´ crop ì„±ê³µ ì¹´ìš´í„° ì¶”ê°€
        peak_memory_sum = 0.0  # í”¼í¬ ë©”ëª¨ë¦¬ í•©ê³„ ì¶”ê°€
        
        # data_sourceë³„ í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        data_source_stats = {}

        if MEMORY_VIS:
            memory_dir = os.path.join(save_dir, "gpu_usage", task)
            os.makedirs(memory_dir, exist_ok=True)

        for j, item in tqdm(enumerate(screenspot_data)):

            if MEMORY_EVAL:
                mem_log = []
                time_log = []
                stop_flag = False
                monitor_thread = threading.Thread(target=monitor_memory, daemon=True)
                monitor_thread.start()
                torch.cuda.reset_peak_memory_stats()

            s1_tflops = s2_tflops = 0.0

            print("\n\n----------------------\n")

            num_action += 1
            
            # íŒŒì¼ ë° ë°ì´í„° ë¡œë“œ
            filename = item["img_filename"]
            filename_wo_ext, ext = os.path.splitext(filename)
            img_path = os.path.join(SCREENSPOT_IMGS, filename)
            if not os.path.exists(img_path):
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["instruction"]
            original_bbox = item["bbox"]
            original_bbox = [original_bbox[0], original_bbox[1], 
                           original_bbox[0] + original_bbox[2], original_bbox[1] + original_bbox[3]]

            orig_w, orig_h = original_image.size

            # data_source ì •ë³´ ì¶”ì¶œ (ì—†ìœ¼ë©´ "unknown"ìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •)
            data_source = item.get("data_source", "unknown")

            #! ==================================================================
            #! Stage 1 | Attention-based Crop Generation
            #! ==================================================================

            if TFOPS_PROFILING:
                prof.reset_profile()

            s1_start = time.time()
            
            # ìƒˆë¡œìš´ attention ê¸°ë°˜ ë°©ì‹ (ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©)
            s1_pred, s1_crop_list, num_crops, resized_image, scaled_gt_bbox = run_stage1_attention_based(
                original_image=original_image,
                instruction=instruction,
                gt_bbox=original_bbox
            )
            
            s1_infence_end = time.time()
            s1_time = s1_infence_end - s1_start

            if TFOPS_PROFILING:
                s1_tflops = prof.get_total_flops() / 1e12

            # Stage1 Grounding ì„±ê³µ ì—¬ë¶€ í™•ì¸ (ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼)
            s1_success = False
            s1_original_point = None
            if s1_pred and "topk_points" in s1_pred and s1_pred["topk_points"]:
                s1_predicted_point = s1_pred["topk_points"][0]  # ì •ê·œí™”ëœ ì¢Œí‘œ (0~1)
                # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                s1_original_point = [
                    s1_predicted_point[0] * original_image.size[0],
                    s1_predicted_point[1] * original_image.size[1]
                ]
                s1_success = point_in_bbox(s1_original_point, original_bbox)
            
            s1_hit = "âœ…" if s1_success else "âŒ"
            if s1_success:
                stage1_success_count += 1

            # Crop ìƒì„± ì„±ê³µ ì—¬ë¶€ í™•ì¸ (GTê°€ ìƒì„±ëœ cropë“¤ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸)
            crop_success = False
            if num_crops > 0:
                gt_center = [(original_bbox[0] + original_bbox[2])/2, (original_bbox[1] + original_bbox[3])/2]
                for crop in s1_crop_list:
                    crop_bbox = crop["bbox"]
                    if point_in_bbox(gt_center, crop_bbox):
                        crop_success = True
                        break
            
            crop_hit = "âœ…" if crop_success else "âŒ"
            if crop_success:
                crop_success_count += 1

            #! ==================================================================
            #! [Stage 2] Merged Crop Inference
            #! ==================================================================
            
            s2_tflops = 0.0
            s2_pred = s2_corrected_point = None  # ì‹œê°í™”ìš© ë³€ìˆ˜ ì´ˆê¸°í™” (multi-image ë°©ì‹)
            stage2_success = False

            s2_inference_start = time.time()
            
            if TFOPS_PROFILING:
                prof.reset_profile()
            
            # ë©€í‹° ì´ë¯¸ì§€ë¡œ inference
            s2_pred = run_stage2_multi_image_inference(s1_crop_list, instruction)

            # Stage2 multi-image ê²°ê³¼ë¥¼ ì›ë³¸ ì¢Œí‘œë¡œ ë³€í™˜
            s2_best_candidate, s2_all_candidates = convert_multi_image_results_to_original(s2_pred, s1_crop_list)
            
            # Stage2 ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if s2_best_candidate:
                s2_corrected_point = s2_best_candidate['point']
                stage2_success = point_in_bbox(s2_corrected_point, original_bbox)
            else:
                s2_corrected_point = [0, 0]
                stage2_success = False


            s2_inference_end = time.time()
            s2_time = s2_inference_end - s2_inference_start
            
            if TFOPS_PROFILING:
                s2_tflops = prof.get_total_flops() / 1e12

            s2_hit = "âœ…" if stage2_success else "âŒ"
            if stage2_success:
                stage2_success_count += 1

            #! ==================================================================
            #! [Stage 3] Ensemble Processing
            #! ==================================================================
            
            s3_ensemble_point = None
            stage3_success = False
            
            # í¬ë¡­ ë©´ì ì´ ì›ë³¸ ì´ë¯¸ì§€ ë©´ì ì˜ 50%ë¥¼ ë„˜ëŠ”ì§€ í™•ì¸
            crop_area = CROP_WIDTH * CROP_HEIGHT
            original_area = orig_w * orig_h
            crop_area_ratio = crop_area / original_area
            
            s3_start = time.time()
            # Multi-image ì•™ìƒë¸” ë°©ë²•: Stage2 multi-image ê²°ê³¼ì™€ í•´ë‹¹ ìœ„ì¹˜ì˜ Stage1 ì ìˆ˜ ì¡°í•©
            # Stage1 ì–´í…ì…˜ ì •ë³´
            s1_attn_scores = np.array(s1_pred['attn_scores'][0])
            s1_n_width = s1_pred['n_width']
            s1_n_height = s1_pred['n_height']
            s1_resize_ratio = s1_pred['resize_ratio']
            
            # Stage1 attention ì ìˆ˜ë“¤ ì •ê·œí™” (1ë“± ê¸°ì¤€)
            s1_max_score = float(max(s1_attn_scores)) if len(s1_attn_scores) > 0 else 1.0
            
            # Stage2ì—ì„œ topk í›„ë³´ë“¤ë§Œ ì„ ë³„ (run_gui_actorì™€ ë™ì¼)
            if s2_all_candidates:
                # ì ìˆ˜ ìƒìœ„ 10ê°œë§Œ ì„ íƒ (run_gui_actorì˜ topk=10ê³¼ ë™ì¼)
                s2_topk_candidates = sorted(s2_all_candidates, key=lambda x: x['score'], reverse=True)[:10]
                s2_topk_scores = [candidate['score'] for candidate in s2_topk_candidates]
                
                # topk ì ìˆ˜ë“¤ë§Œìœ¼ë¡œ ì •ê·œí™” (run_gui_actorì™€ ë™ì¼)
                if s2_topk_scores:
                    s2_max_score = max(s2_topk_scores)
                    if s2_max_score > 0:
                        s2_normalized_scores = [score / s2_max_score for score in s2_topk_scores]
                    else:
                        s2_normalized_scores = [0.0] * len(s2_topk_scores)
                else:
                    s2_normalized_scores = []
            else:
                s2_topk_candidates = []
                s2_normalized_scores = []
            
            # ê° Stage2 topk ì ì— ëŒ€í•´ ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚°
            ensemble_candidates = []
            
            for i, candidate in enumerate(s2_topk_candidates):
                s2_original_point = candidate['point']
                
                # í•´ë‹¹ ì ì—ì„œì˜ Stage1 ì ìˆ˜ ê³„ì‚° (ì •ê·œí™”ëœ ê°’)
                s1_raw_score = get_stage1_score_at_point(
                    s2_original_point, s1_attn_scores, s1_n_width, s1_n_height, 
                    original_image.size, s1_resize_ratio
                )
                s1_score = s1_raw_score / s1_max_score if s1_max_score > 0 else 0.0
                
                # Stage2 ì ìˆ˜ëŠ” ì •ê·œí™”ëœ ì ìˆ˜ ì‚¬ìš© (run_gui_actorì™€ ë™ì¼)
                s2_score = s2_normalized_scores[i] if i < len(s2_normalized_scores) else 0.0
                
                # ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚°
                ensemble_score = STAGE1_ENSEMBLE_RATIO * s1_score + STAGE2_ENSEMBLE_RATIO * s2_score
                
                ensemble_candidates.append({
                    'point': s2_original_point,
                    'score': ensemble_score,
                    's1_score': s1_score,
                    's2_score': s2_score,
                    'crop_id': candidate['crop_id'],
                    'rank_in_crop': candidate['rank_in_crop'],
                    's2_rank': i + 1  # topk ë‚´ì—ì„œì˜ ìˆœìœ„
                })
            
            # ìµœê³  ì ìˆ˜ë¥¼ ê°€ì§„ ì  ì„ íƒ
            if ensemble_candidates:
                best_candidate = max(ensemble_candidates, key=lambda x: x['score'])
                s3_ensemble_point = best_candidate['point']
            else:
                # Stage2 ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Stage1 ê²°ê³¼ë¥¼ ì‚¬ìš©
                if s1_original_point:
                    s3_ensemble_point = s1_original_point
                else:
                    s3_ensemble_point = [0, 0]

            s3_end = time.time()
            s3_time = s3_end - s3_start
            
            # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥ (ìƒìœ„ 3ê°œë§Œ) - run_gui_actorì™€ ë™ì¼í•œ í˜•íƒœ
            if ensemble_candidates:
                print(f"ğŸ¯ Stage3 Ensemble Candidates (Top 3):")
                for i, candidate in enumerate(sorted(ensemble_candidates, key=lambda x: x['score'], reverse=True)[:3]):
                    print(f"  Rank {i+1}: S2_rank={candidate['s2_rank']}, S1={candidate['s1_score']:.3f}, S2={candidate['s2_score']:.3f}, Ensemble={candidate['score']:.3f}")
            else:
                print(f"ğŸ¯ Stage3: No ensemble candidates, using Stage1 result")
            
            # ì‹œê°í™”ë¥¼ ìœ„í•´ í›„ë³´ë“¤ ì €ì¥
            s3_ensemble_candidates = ensemble_candidates
            
            # ì•™ìƒë¸” ê²°ê³¼ë¡œ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            stage3_success = point_in_bbox(s3_ensemble_point, original_bbox)
            
            s3_hit = "âœ…" if stage3_success else "âŒ"
            if stage3_success:
                stage3_success_count += 1

            #! ==================================================================
            #! [Common Processing]
            #! ==================================================================
            
            # ê³µí†µ í†µê³„ ì—…ë°ì´íŠ¸
            s1_time_sum += s1_time
            s2_time_sum += s2_time
            s3_time_sum += s3_time
            s1_tflops_sum += s1_tflops
            s2_tflops_sum += s2_tflops
                
            # ì„±ëŠ¥ ë¡œê¹…
            total_time = s1_time + s2_time
            if TFOPS_PROFILING:
                total_tflops_this = s1_tflops + s2_tflops  # Stage3ëŠ” FLOPs ì œì™¸

            #! ==================================================================
            #! [Visualization - After Time Measurement]
            #! ==================================================================
            
            # ì‹œê°í™”ìš© ë””ë ‰í† ë¦¬ ì„¤ì • (stage3 ê²°ê³¼ì— ë”°ë¼)
            if VISUALIZE and (not VIS_ONLY_WRONG or not stage3_success):
                inst_dir_name = re.sub(r'\W+', '_', instruction).strip('_')
                result_folder = "correct" if stage3_success else "incorrect"
                inst_dir = os.path.join(save_dir, f"{task}_visualize_{result_folder}", f"{num_action}_{inst_dir_name}")

                # Stage1 ì‹œê°í™”
                visualize_stage1_attention_crops(
                    s1_pred=s1_pred,
                    resized_image=resized_image, 
                    crop_list=s1_crop_list,
                    original_image=original_image,
                    save_dir=inst_dir,
                    instruction=instruction,
                    gt_bbox=original_bbox,
                    s1_predicted_point=s1_original_point
                )
                
                # Stage2 Multi-Image ì‹œê°í™”
                if s2_pred and s1_crop_list:  # Stage2 ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì‹œê°í™”
                    visualize_stage2_multi_attention(
                        s2_pred=s2_pred,
                        crop_list=s1_crop_list,
                        original_image=original_image,
                        save_dir=inst_dir,
                        instruction=instruction,
                        predicted_point=s2_corrected_point
                    )
                
                # Stage3 ì•™ìƒë¸” ì‹œê°í™”
                visualize_stage3_point_ensemble(
                    s3_ensemble_candidates=s3_ensemble_candidates if 's3_ensemble_candidates' in locals() else [],
                    original_image=original_image,
                    crop_list=s1_crop_list,
                    original_bbox=original_bbox,
                    s3_ensemble_point=s3_ensemble_point,
                    s2_corrected_point=s2_corrected_point,
                    s1_original_point=s1_original_point,
                    stage1_ratio=STAGE1_ENSEMBLE_RATIO,
                    stage2_ratio=STAGE2_ENSEMBLE_RATIO,
                    save_dir=inst_dir,
                    vis_only_wrong=VIS_ONLY_WRONG,
                    stage3_success=stage3_success
                )


            num_attention_crops = len(s1_crop_list)
            print(f"âœ‚ï¸  Attention Crops : {num_attention_crops}")
            print(f"ğŸ•– Times - S1: {s1_time:.2f}s | S2: {s2_time:.2f}s | Total: {total_time:.2f}s")
            if TFOPS_PROFILING:
                print(f"ğŸ”¥ FLOPs - S1: {s1_tflops:.2f} | S2: {s2_tflops:.2f} | Total: {total_tflops_this:.2f} TFLOPs")
            print(f"{'âœ… Success' if stage3_success else 'âŒğŸ¯ Fail'}")

            #! ==================================================================
            #! [End]
            #! ==================================================================

            if MEMORY_EVAL:
                time.sleep(0.1)
                stop_flag = True
                monitor_thread.join()

                # í”¼í¬ ë©”ëª¨ë¦¬ ê³„ì‚° (GB ë‹¨ìœ„, ì†Œìˆ˜ì  3ìë¦¬)
                peak_memory = max(mem_log) if mem_log else 0.0
                peak_memory_gb = round(peak_memory, 3)

                if MEMORY_VIS:
                    import matplotlib.pyplot as plt
                    plt.figure(figsize=(10, 4))
                    plt.plot(time_log, mem_log)
                    plt.xlabel("Time (s)")
                    plt.ylabel("GPU Memory Allocated (GB)")
                    plt.title("GPU Memory Usage Over Time")
                    plt.grid(True)
                    plt.savefig(f"{memory_dir}/{num_action}_{filename}")
                    plt.close()  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ close ì¶”ê°€

            if MEMORY_EVAL:
                peak_memory_sum += peak_memory_gb

            # data_sourceë³„ í†µê³„ ì—…ë°ì´íŠ¸
            if data_source not in data_source_stats:
                data_source_stats[data_source] = {
                    'num_action': 0,
                    's1_time_sum': 0.0,
                    's2_time_sum': 0.0,
                    's3_time_sum': 0.0,
                    's1_tflops_sum': 0.0,
                    's2_tflops_sum': 0.0,
                    'total_tflops': 0.0,
                    'stage1_success_count': 0,
                    'crop_success_count': 0,
                    'stage2_success_count': 0,
                    'stage3_success_count': 0,
                    'peak_memory_sum': 0.0
                }
            
            stats = data_source_stats[data_source]
            stats['num_action'] += 1
            stats['s1_time_sum'] += s1_time
            stats['s2_time_sum'] += s2_time
            stats['s3_time_sum'] += s3_time
            if TFOPS_PROFILING:
                stats['s1_tflops_sum'] += s1_tflops
                stats['s2_tflops_sum'] += s2_tflops
                stats['total_tflops'] += total_tflops_this
            if MEMORY_EVAL:
                stats['peak_memory_sum'] += peak_memory_gb
            if s1_success:
                stats['stage1_success_count'] += 1
            if crop_success:
                stats['crop_success_count'] += 1
            if stage2_success:
                stats['stage2_success_count'] += 1
            if stage3_success:
                stats['stage3_success_count'] += 1

            up2now_s1_score = stage1_success_count / num_action * 100
            up2now_crop_score = crop_success_count / num_action * 100
            up2now_s2_score = stage2_success_count / num_action * 100
            up2now_s3_ensemble_score = stage3_success_count / num_action * 100
            # print(f"Up2Now Crop Accuracy: {up2now_crop_score:.2f}%")
            print(f"Up2Now Stage1 Accuracy: {up2now_s1_score:.2f}%")
            print(f"Up2Now Stage2 Accuracy: {up2now_s2_score:.2f}%")
            print(f"Up2Now Stage3 Ensemble Accuracy: {up2now_s3_ensemble_score:.2f}%")

            # Iter log - ê°œì„ ëœ ë¡œê¹…
            append_iter_log(
                idx=j+1,
                orig_w=original_image.size[0],
                orig_h=original_image.size[1],
                resize_ratio=s1_pred['resize_ratio'],
                num_crop=num_attention_crops,
                crop_hit=crop_hit,
                s1_time=f"{s1_time:.3f}",
                s1_tflops=f"{s1_tflops:.2f}",
                s1_hit=s1_hit,
                s2_time=f"{s2_time:.3f}",
                s2_tflops=f"{s2_tflops:.2f}",
                s2_hit=s2_hit,
                s3_time=f"{s3_time:.3f}",
                s3_hit=s3_hit,
                total_time=f"{total_time:.3f}",
                total_tflops=f"{total_tflops_this:.2f}",
                peak_memory_gb=f"{peak_memory_gb:.3f}" if MEMORY_EVAL else "N/A",
                crop_acc_uptonow=f"{up2now_crop_score:.2f}",
                s1_acc_uptonow=f"{up2now_s1_score:.2f}",
                s2_acc_uptonow=f"{up2now_s2_score:.2f}",
                s3_acc_uptonow=f"{up2now_s3_ensemble_score:.2f}",
                filename=filename_wo_ext,
                instruction=instruction[:50] + "..." if len(instruction) > 50 else instruction
            )

            # JSON ê¸°ë¡ - í•µì‹¬ ì •ë³´ë§Œ
            item_res = {
                'filename': filename,
                'orig_w': original_image.size[0],
                'orig_h': original_image.size[1],
                'instruction': instruction,
                'gt_bbox': original_bbox,
                'data_source': data_source,
                'num_crop': num_attention_crops,
                'crop_success': crop_success,
                'stage1_success': s1_success,
                'stage2_success': stage2_success,
                'stage3_success': stage3_success,
                's1_hit': s1_hit,
                'crop_hit': crop_hit,
                's2_hit': s2_hit,
                's3_hit': s3_hit,
                's3_ensemble_point': s3_ensemble_point,
                's1_original_point': s1_original_point,
                's2_original_point': s2_corrected_point,
                's1_time': s1_time,
                's2_time': s2_time,
                's3_time': s3_time,
                'total_time': total_time,
                's1_tflops': s1_tflops,
                's2_tflops': s2_tflops,
                'total_tflops': s1_tflops+s2_tflops,
                'peak_memory_gb': peak_memory_gb if MEMORY_EVAL else None,
                'ensemble_config': {
                    'attention_ratio': STAGE1_ENSEMBLE_RATIO,
                    'crop_ratio': STAGE2_ENSEMBLE_RATIO
                }
            }
            task_res.append(item_res)

        #! ==================================================
        # ê²°ê³¼ Json ì •ë¦¬
        os.makedirs(os.path.join(save_dir, "json"), exist_ok=True)
        with open(os.path.join(save_dir, "json", dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        # ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            "task": task,
            "total_samples": num_action,
            "crop_accuracy": crop_success_count / num_action * 100,
            "stage1_accuracy": stage1_success_count / num_action * 100,
            "stage2_accuracy": stage2_success_count / num_action * 100,
            "stage3_accuracy": stage3_success_count / num_action * 100,
            "avg_times": {
                "stage1": s1_time_sum / num_action,
                "stage2": s2_time_sum / num_action,
                "stage3": s3_time_sum / num_action,
                "total": (s1_time_sum + s2_time_sum + s3_time_sum) / num_action
            },
            "avg_flops_tflops": {
                "stage1": s1_tflops_sum / num_action,
                "stage2": s2_tflops_sum / num_action,
                "total": (s1_tflops_sum + s2_tflops_sum) / num_action
            },
            "avg_peak_memory_gb": round(peak_memory_sum / num_action, 3) if MEMORY_EVAL else None,
            "hyperparameters": {
                "select_threshold": SELECT_THRESHOLD,
                "attn_impl": ATTN_IMPL,
                "STAGE1_ensemble_ratio": STAGE1_ENSEMBLE_RATIO,
                "STAGE2_ensemble_ratio": STAGE2_ENSEMBLE_RATIO
            }
        }

        with open(os.path.join(save_dir, f"results_{task}.json"), "w") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=4)

        # data_sourceë³„ ë©”íŠ¸ë¦­ ì €ì¥
        data_source_metrics = {}
        for ds, stats in data_source_stats.items():
            if stats['num_action'] > 0:
                data_source_metrics[ds] = {
                    "task": task,
                    "data_source": ds,
                    "total_samples": stats['num_action'],
                    "crop_accuracy": stats['crop_success_count'] / stats['num_action'] * 100,
                    "stage1_accuracy": stats['stage1_success_count'] / stats['num_action'] * 100,
                    "stage2_accuracy": stats['stage2_success_count'] / stats['num_action'] * 100,
                    "stage3_accuracy": stats['stage3_success_count'] / stats['num_action'] * 100,
                    "avg_times": {
                        "stage1": stats['s1_time_sum'] / stats['num_action'],
                        "stage2": stats['s2_time_sum'] / stats['num_action'],
                        "stage3": stats['s3_time_sum'] / stats['num_action'],
                        "total": (stats['s1_time_sum'] + stats['s2_time_sum'] + stats['s3_time_sum']) / stats['num_action']
                    },
                    "avg_flops_tflops": {
                        "stage1": stats['s1_tflops_sum'] / stats['num_action'],
                        "stage2": stats['s2_tflops_sum'] / stats['num_action'],
                        "total": (stats['s1_tflops_sum'] + stats['s2_tflops_sum']) / stats['num_action']
                    },
                    "avg_peak_memory_gb": round(stats['peak_memory_sum'] / stats['num_action'], 3) if MEMORY_EVAL else None,
                    "hyperparameters": {
                        "select_threshold": SELECT_THRESHOLD,
                        "attn_impl": ATTN_IMPL,
                        "STAGE1_ensemble_ratio": STAGE1_ENSEMBLE_RATIO,
                        "STAGE2_ensemble_ratio": STAGE2_ENSEMBLE_RATIO
                    }
                }
        
        with open(os.path.join(save_dir, f"source_results_{task}.json"), "w") as dsf:
            json.dump(data_source_metrics, dsf, ensure_ascii=False, indent=4)

        # ì „ì²´ ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— í•œ ì¤„ ì¶”ê°€
        results_csv_path = "../_results"
        os.makedirs(results_csv_path, exist_ok=True)
        csv_file_path = os.path.join(results_csv_path, f"results_{task}.csv")
        
        # CSV í—¤ë” ì •ì˜
        csv_headers = [
            "method",
            "min_resize", "max_resize", "select_threshold", "stage1_ensemble_ratio", "crop_width", "crop_height",
            "total_samples", "crop_accuracy", "stage1_accuracy", "stage2_accuracy", "stage3_accuracy",
            "avg_stage1_time", "avg_stage2_time", "avg_stage3_time", "avg_total_time",
            "avg_stage1_tflops", "avg_stage2_tflops", "avg_total_tflops", "avg_peak_memory_gb",
            "timestamp"
        ]
        
        # CSV ë°ì´í„° í–‰ ìƒì„±
        import datetime
        csv_row = [
            method,
            MIN_RESIZE, MAX_RESIZE, SELECT_THRESHOLD, STAGE1_ENSEMBLE_RATIO, CROP_WIDTH, CROP_HEIGHT,
            num_action, 
            round(metrics['crop_accuracy'], 2),
            round(metrics['stage1_accuracy'], 2),
            round(metrics['stage2_accuracy'], 2), 
            round(metrics['stage3_accuracy'], 2),
            round(metrics['avg_times']['stage1'], 4),
            round(metrics['avg_times']['stage2'], 4),
            round(metrics['avg_times']['stage3'], 4),
            round(metrics['avg_times']['total'], 4),
            round(metrics['avg_flops_tflops']['stage1'], 2),
            round(metrics['avg_flops_tflops']['stage2'], 2),
            round(metrics['avg_flops_tflops']['total'], 2),
            metrics['avg_peak_memory_gb'] if metrics['avg_peak_memory_gb'] else 0.0,
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        ]
        
        # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±, ìˆìœ¼ë©´ ë°ì´í„° í–‰ë§Œ ì¶”ê°€
        import csv
        file_exists = os.path.exists(csv_file_path)
        
        with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ í—¤ë” ì¶”ê°€
            if not file_exists or os.path.getsize(csv_file_path) == 0:
                writer.writerow(csv_headers)
            
            # ë°ì´í„° í–‰ ì¶”ê°€
            writer.writerow(csv_row)
        
        print(f"ğŸ“ Results saved to CSV: {csv_file_path}")

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("=" * 60)
        print(f"ğŸ“Š Final Results for {task}:")
        print(f"Total Samples: {num_action}")
        print(f"Crop Accuracy: {metrics['crop_accuracy']:.2f}%")
        print(f"Stage1 Accuracy: {metrics['stage1_accuracy']:.2f}%")
        print(f"Stage2 Accuracy: {metrics['stage2_accuracy']:.2f}%")
        print(f"Stage3 Ensemble Accuracy: {metrics['stage3_accuracy']:.2f}%")
        print(f"Avg Times: S1 {metrics['avg_times']['stage1']:.3f}s | S2 {metrics['avg_times']['stage2']:.3f}s | S3 {metrics['avg_times']['stage3']:.3f}s | Total {metrics['avg_times']['total']:.3f}s")
        print(f"Avg FLOPs: S1 {metrics['avg_flops_tflops']['stage1']:.2f} | S2 {metrics['avg_flops_tflops']['stage2']:.2f} | Total {metrics['avg_flops_tflops']['total']:.2f} TFLOPs")
        if MEMORY_EVAL and metrics['avg_peak_memory_gb'] is not None:
            print(f"Avg Peak Memory: {metrics['avg_peak_memory_gb']:.3f} GB")
        print(f"Ensemble Config: Attention {STAGE1_ENSEMBLE_RATIO:.1f}, Crop {STAGE2_ENSEMBLE_RATIO:.1f}")
        
        print("=" * 60)