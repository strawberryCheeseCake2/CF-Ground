import os
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('gpu', type=int, help='GPU number')
parser.add_argument('--r', type=float, nargs=2, metavar=('MIN_RESIZE', 'MAX_RESIZE'), help='Stage 1 Resize ratio range (min max)')
parser.add_argument('--e', type=float, help='Stage 1 Ensemble ratio')
parser.add_argument('--v', action='store_true', help='Whether to save visualization images')
args = parser.parse_args()

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)  # Î™áÎ≤à GPU ÏÇ¨Ïö©Ìï†ÏßÄ argumentÎ°ú ÏßÄÏ†ï : run_gui_actor.py 2 -> 2Î≤à GPU

#! Hyperparameter =====================================================================================

ATTN_IMPL = "eager"  # attention implement "eager" "sdpa" "flash" "efficient"

# Image Resize Ratios

MIN_RESIZE = args.r[0] if args.r else 0.50  # DYNAMIC_RESIZE ÎπÑÏú® ÏµúÏÜåÍ∞í
MAX_RESIZE = args.r[1] if args.r else 0.50  # DYNAMIC_RESIZE ÎπÑÏú® ÏµúÎåÄÍ∞í

# Crop Limitations
MAX_CROPS = 3  # ÏÉùÏÑ±Ìï† Ïàò ÏûàÎäî ÏµúÎåÄ crop Í∞úÏàò
SELECT_THRESHOLD = 0.50  #! score >= tau * max_score Ïù∏ Î™®Îì† crop select
CROP_WIDTH = 1176  # ÌÅ¨Î°≠Ìï† ÏßÅÏÇ¨Í∞ÅÌòï Í∞ÄÎ°ú ÌÅ¨Í∏∞ (ÏïÑÏù¥Ìè∞ Ï†ÑÏ≤¥ Í∞ÄÎ°úÍ∞Ä 1170px)
CROP_HEIGHT = 602  # ÌÅ¨Î°≠Ìï† ÏßÅÏÇ¨Í∞ÅÌòï ÏÑ∏Î°ú ÌÅ¨Í∏∞

# Ensemble Hyperparameters
# TODO: Ïù¥Í≤ÉÎèÑ resizeÏ≤òÎüº ÎèôÏ†ÅÏúºÎ°ú Ï∏°Ï†ïÌï¥ÏÑú Î≥ÄÍ≤Ω Í∞ÄÎä•ÌïòÎèÑÎ°ù
STAGE1_ENSEMBLE_RATIO = args.e if args.e else 0.50  # Stage1 attention Í∞ÄÏ§ëÏπò
STAGE2_ENSEMBLE_RATIO = 1 - STAGE1_ENSEMBLE_RATIO  # Stage2 crop Í∞ÄÏ§ëÏπò
ENSEMBLE_TOP_PATCHES = 100                         # Stage2ÏóêÏÑú ÏïôÏÉÅÎ∏îÏóê ÏÇ¨Ïö©Ìï† ÏÉÅÏúÑ Ìå®Ïπò Í∞úÏàò

# ÏµúÎåÄ PIXELS Ï†úÌïú
MAX_PIXELS = 3211264  # ProcessÎã®ÏóêÏÑú Ï†ÅÏö©

# csvÏóê Í∏∞Î°ùÌï† method Ïù¥Î¶Ñ
# method = "dynamic_resize"
method = "multi_fixed_resize"

memo = f"resize_{MIN_RESIZE:.2f}~{MAX_RESIZE:.2f}_ensemble{STAGE1_ENSEMBLE_RATIO}_crop{CROP_WIDTH}x{CROP_HEIGHT}"

#! Argument ==========================================================================================

SEED = 0

# Dataset & Model
MLLM_PATH = "microsoft/GUI-Actor-3B-Qwen2.5-VL"
SCREENSPOT_IMGS = "../data/screenspotv2_image"  # input image Í≤ΩÎ°ú
SCREENSPOT_JSON = "../data"  # input image jsonÌååÏùº Í≤ΩÎ°ú
TASKS = ["mobile", "web", "desktop"]
# TASKS = ["mobile"]
# TASKS = ["web"]
# TASKS = ["desktop"]
SAMPLE_RANGE = slice(None)
# SAMPLE_RANGE = slice(0,2)

# Visualize & Logging
VISUALIZE = args.v if args.v else False
VIS_ONLY_WRONG = False  # TrueÎ©¥ ÌãÄÎ¶∞ Í≤ÉÎßå ÏãúÍ∞ÅÌôî, FalseÎ©¥ Î™®Îì† Í≤É ÏãúÍ∞ÅÌôî
TFOPS_PROFILING = True
MEMORY_EVAL = True
MEMORY_VIS = False

# Save Path
SAVE_DIR = f"../attn_output/" + method + "/" + memo

#! ==================================================================================================

# Standard Library
import os
import sys
import time
import threading
import re
import json
import logging
logging.disable(logging.CRITICAL)  # Î™®Îì† Î°úÍπÖ Ìò∏Ï∂ú Î¨¥Î†•Ìôî

# Third-Party Libraries
import numpy as np
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoTokenizer, set_seed

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from util.iter_logger import init_iter_logger, append_iter_log  # log csv Í∏∞Î°ù ÌååÏùº
from gui_actor.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer
from gui_actor.inference import inference
from gui_actor.multi_image_inference import multi_image_inference
from util.visualize_util import visualize_stage1_attention_crops, visualize_stage2_merged_attention, visualize_stage2_multi_attention, visualize_stage3_ensemble_attention, visualize_stage3_point_ensemble
from util.sharpness_util import get_fft_blur_score
if TFOPS_PROFILING:
    from deepspeed.profiling.flops_profiler import FlopsProfiler

#! ==============================================================================================

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        return super(NpEncoder, self).default(obj)

def resize_image(image, resize_to_pixels):
    image_width, image_height = image.size
    if (resize_to_pixels is not None) and ((image_width * image_height) > resize_to_pixels):
        resize_ratio = (resize_to_pixels / (image_width * image_height)) ** 0.5
        image_width_resized, image_height_resized = int(image_width * resize_ratio), int(image_height * resize_ratio)
        image = image.resize((image_width_resized, image_height_resized))
        print(f"üîß Resized image: {image_width}x{image_height} -> {image_width_resized}x{image_height_resized} (ratio: {resize_ratio:.3f})")
    return image

def warm_up_model(model, tokenizer, processor):
    print("üèãÔ∏è‚Äç‚ôÇÔ∏è Warming up the model...")
    dummy_instruction = "This is a dummy instruction for warm-up."
    dummy_image = Image.new("RGB", (1000, 1000), color=(255, 255, 255))  # 1000x1000 Ìù∞ÏÉâ Ïù¥ÎØ∏ÏßÄ
    dummy_msgs = create_conversation_stage1(image=dummy_image, instruction=dummy_instruction, resize_ratio=1.0)
    
    # ÏòàÏó¥Ïö© inference Ïã§Ìñâ
    for _ in range(3):  # 3Î≤à Î∞òÎ≥µ
        with torch.no_grad():
            _ = inference(dummy_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    print("üèãÔ∏è‚Äç‚ôÇÔ∏è Warm-up complete!")

def create_conversation_stage1(image, instruction, resize_ratio):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    # "text": (
                    #     "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                    #     "your task is to locate the screen element that corresponds to the instruction. "
                    #     "You should output a PyAutoGUI action that performs a click on the correct position. "
                    #     "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                    #     "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    # ),
                    "text": (
                        f"This is a resized screenshot of the whole GUI, scaled by {resize_ratio}. "
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def create_conversation_stage2(image, instruction, crop_cnt):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    # "text": (
                    #     "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                    #     "your task is to locate the screen element that corresponds to the instruction. "
                    #     "You should output a PyAutoGUI action that performs a click on the correct position. "
                    #     "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                    #     "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    # ),
                    "text": (
                        f"This image is a vertical collage of {crop_cnt} cropped regions that were selected as promising. "
                        "Each crop has the same width and fixed height, separated by a red horizontal line. "
                        "You are a GUI agent. Given this collage image and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation

def run_stage1_attention_inference(original_image, instruction):
    """Stage 1: Î¶¨ÏÇ¨Ïù¥Ï¶àÌïòÍ≥† inference"""

    orig_w, orig_h = original_image.size
    # Ïù¥ÎØ∏ÏßÄ Í≥†Ï†ï Î¶¨ÏÇ¨Ïù¥Ï¶à
    if MIN_RESIZE == MAX_RESIZE:
        resize_ratio = MIN_RESIZE
        resized_w, resized_h = int(orig_w * resize_ratio), int(orig_h * resize_ratio)
        print(f"üîß Fixed Resized image: {orig_w}x{orig_h} -> {resized_w}x{resized_h} (ratio: {resize_ratio:.3f})")
    
    # Ïù¥ÎØ∏ÏßÄ ÎèôÏ†Å Î¶¨ÏÇ¨Ïù¥Ï¶à
    else:
        downsampled = original_image.resize((int(orig_w*0.5), int(orig_h*0.5)))
        resize_ratio = get_fft_blur_score(downsampled, min_resize=MIN_RESIZE, max_resize=MAX_RESIZE)
        resized_w, resized_h = int(orig_w * resize_ratio), int(orig_h * resize_ratio)
        print(f"üîß Dynamic Resized image: {orig_w}x{orig_h} -> {resized_w}x{resized_h} (ratio: {resize_ratio:.3f})")
    
    # Î¶¨ÏÇ¨Ïù¥Ï¶àÎêú Ïù¥ÎØ∏ÏßÄÎ°ú inference
    resized_image = original_image.resize((resized_w, resized_h))
    conversation = create_conversation_stage1(resized_image, instruction, resize_ratio)
    pred = inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=1)
    
    # Í≤∞Í≥ºÏóê Î¶¨ÏÇ¨Ïù¥Ï¶à Ï†ïÎ≥¥ Ï∂îÍ∞Ä
    pred['resize_ratio'] = resize_ratio
    pred['original_size'] = (orig_w, orig_h)
    pred['resized_size'] = resized_image.size
    
    return pred, resized_image

def find_attention_peaks(pred_result, resized_image, resize_ratio):
    """Ïñ¥ÌÖêÏÖòÏóêÏÑú Í≥†Ï†êÎì§ Ï∞æÍ∏∞ (bbox ÏÉùÏÑ± ÌõÑ Ï§ëÏã¨Ï†ê Í±∞Î¶¨Î°ú Ï§ëÎ≥µ Ï†úÍ±∞)"""
    
    attn_scores = np.array(pred_result['attn_scores'][0])
    n_width = pred_result['n_width'] 
    n_height = pred_result['n_height']
    
    resized_w, resized_h = resized_image.size
    
    # Ìå®ÏπòÎ•º ÌîΩÏÖÄ Ï¢åÌëúÎ°ú Î≥ÄÌôòÌïòÍ∏∞ ÏúÑÌïú ÎπÑÏú®
    patch_w = resized_w / n_width
    patch_h = resized_h / n_height
    
    # Ï†êÏàòÍ∞Ä ÎÜíÏùÄ ÏàúÏÑúÎ°ú Ï†ïÎ†¨
    sorted_indices = np.argsort(attn_scores)[::-1]
    
    peaks = []
    used_bbox_centers = []  # Ïù¥ÎØ∏ ÏÇ¨Ïö©Îêú bbox Ï§ëÏã¨Ï†êÎì§
    
    for idx in sorted_indices:
        # Ìå®Ïπò Ï¢åÌëú Í≥ÑÏÇ∞
        patch_y = idx // n_width
        patch_x = idx % n_width
        
        # Ìå®Ïπò Ï§ëÏã¨Ï†êÏùò ÌîΩÏÖÄ Ï¢åÌëú (Î¶¨ÏÇ¨Ïù¥Ï¶àÎêú Ïù¥ÎØ∏ÏßÄ Í∏∞Ï§Ä)
        center_x = (patch_x + 0.5) * patch_w
        center_y = (patch_y + 0.5) * patch_h
        
        # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Í∏∞Ï§ÄÏúºÎ°ú Î≥ÄÌôò
        orig_center_x = center_x / resize_ratio
        orig_center_y = center_y / resize_ratio
        
        # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏóêÏÑú bbox Í≥ÑÏÇ∞ (Ïã§Ï†ú ÌÅ¨Î°≠Ïù¥ ÏÉùÏÑ±Îê† ÏúÑÏπò)
        orig_w = resized_w / resize_ratio
        orig_h = resized_h / resize_ratio
        
        left = max(0, int(orig_center_x - CROP_WIDTH/2))
        top = max(0, int(orig_center_y - CROP_HEIGHT/2))
        right = min(orig_w, int(orig_center_x + CROP_WIDTH/2))
        bottom = min(orig_h, int(orig_center_y + CROP_HEIGHT/2))
        
        # Í≤ΩÍ≥ÑÏóêÏÑú ÏûòÎ†∏ÏùÑ Í≤ΩÏö∞ Ï°∞Ï†ï
        if right - left < CROP_WIDTH and right < orig_w:
            right = min(orig_w, left + int(CROP_WIDTH))
        if right - left < CROP_WIDTH and left > 0:
            left = max(0, right - int(CROP_WIDTH))
        if bottom - top < CROP_HEIGHT and bottom < orig_h:
            bottom = min(orig_h, top + int(CROP_HEIGHT))
        if bottom - top < CROP_HEIGHT and top > 0:
            top = max(0, bottom - int(CROP_HEIGHT))
        
        # Ïã§Ï†ú bbox Ï§ëÏã¨Ï†ê Í≥ÑÏÇ∞
        bbox_center_x = (left + right) / 2
        bbox_center_y = (top + bottom) / 2
        
        # Ïù¥ÎØ∏ ÏÇ¨Ïö©Îêú bboxÏôÄ Ï§ëÎ≥µÎêòÎäîÏßÄ ÌôïÏù∏
        skip = False
        for used_center in used_bbox_centers:
            x_distance = abs(bbox_center_x - used_center[0])
            y_distance = abs(bbox_center_y - used_center[1])
            # bbox Ï§ëÏã¨Ï†êÏù¥ ÎÑàÎ¨¥ Í∞ÄÍπåÏö∞Î©¥ Ïä§ÌÇµ (ÌÅ¨Î°≠ ÌÅ¨Í∏∞Ïùò 30% Ïù¥ÎÇ¥)
            if x_distance < CROP_WIDTH * 0.3 and y_distance < CROP_HEIGHT * 0.3:  #! ÌÅ¨Î°≠
                skip = True
                break
        
        if not skip:
            peaks.append({
                'center_x': center_x,  # Î¶¨ÏÇ¨Ïù¥Ï¶àÎêú Ïù¥ÎØ∏ÏßÄ Í∏∞Ï§Ä (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±)
                'center_y': center_y,
                'score': float(attn_scores[idx]),
                'patch_x': patch_x,
                'patch_y': patch_y,
                'bbox_center': (bbox_center_x, bbox_center_y)  # Ïã§Ï†ú bbox Ï§ëÏã¨Ï†ê
            })
            
            used_bbox_centers.append((bbox_center_x, bbox_center_y))
    
    return peaks

def filter_by_threshold(peaks, threshold=0.7, max_crops=MAX_CROPS):
    """1Îì±Ïùò threshold% Ïù¥ÏÉÅÏù∏ peakÎì§Îßå ÎÇ®Í∏∞Í≥†, ÏµúÎåÄ Í∞úÏàò Ï†úÌïú Ï†ÅÏö©"""
    
    if not peaks:
        return []
    
    max_score = peaks[0]['score']  # Ïù¥ÎØ∏ Ï†ïÎ†¨ÎêòÏñ¥ ÏûàÏùå
    min_score = max_score * threshold
    
    # threshold Ï°∞Í±¥ÏúºÎ°ú Î®ºÏ†Ä ÌïÑÌÑ∞ÎßÅ
    filtered_peaks = [peak for peak in peaks if peak['score'] >= min_score]
    
    # ÏµúÎåÄ Í∞úÏàò Ï†úÌïú Ï†ÅÏö© (ÏÉÅÏúÑ Ï†êÏàòÏàúÏúºÎ°ú)
    if len(filtered_peaks) > max_crops:
        filtered_peaks = filtered_peaks[:max_crops]
    
    print(f"üéØ Found {len(peaks)} peaks, filtered to {len(filtered_peaks)} (threshold: {threshold}, max_crops: {max_crops})")
    
    return filtered_peaks

def create_crops_from_attention_peaks(peaks, original_image, resize_ratio):
    """Attention peaksÎ•º Í∏∞Î∞òÏúºÎ°ú ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÏßÅÏ†ë crop"""
    
    if not peaks:
        return []
    
    crops = []
    orig_w, orig_h = original_image.size
    
    for i, peak in enumerate(peaks):
        # Î¶¨ÏÇ¨Ïù¥Ï¶àÎêú Ïù¥ÎØ∏ÏßÄÏóêÏÑúÏùò centerÎ•º ÏõêÎ≥∏ ÌÅ¨Í∏∞Î°ú Î≥ÄÌôò
        orig_center_x = peak['center_x'] / resize_ratio
        orig_center_y = peak['center_y'] / resize_ratio
        
        # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏóêÏÑúÏùò bbox Í≥ÑÏÇ∞
        left = max(0, int(orig_center_x - CROP_WIDTH/2))
        top = max(0, int(orig_center_y - CROP_HEIGHT/2))
        right = min(orig_w, int(orig_center_x + CROP_WIDTH/2))
        bottom = min(orig_h, int(orig_center_y + CROP_HEIGHT/2))
        
        # Í≤ΩÍ≥ÑÏóêÏÑú ÏûòÎ†∏ÏùÑ Í≤ΩÏö∞ Ï°∞Ï†ï
        if right - left < CROP_WIDTH and right < orig_w:
            right = min(orig_w, left + int(CROP_WIDTH))
        if right - left < CROP_WIDTH and left > 0:
            left = max(0, right - int(CROP_WIDTH))
        if bottom - top < CROP_HEIGHT and bottom < orig_h:
            bottom = min(orig_h, top + int(CROP_HEIGHT))
        if bottom - top < CROP_HEIGHT and top > 0:
            top = max(0, bottom - int(CROP_HEIGHT))
        
        bbox = [left, top, right, bottom]
        crop_img = original_image.crop(bbox)
        
        crops.append({
            'img': crop_img,
            'bbox': bbox,
            'score': peak['score'],
            'id': i + 1
        })
        
        # print(f"üîß Crop {i+1}: center=({orig_center_x:.1f}, {orig_center_y:.1f}), bbox={bbox}, size={CROP_WIDTH}x{CROP_HEIGHT}")
    
    return crops

def create_merged_image_for_stage2(crops):
    """Stage 2Ïö©: cropÎì§ÏùÑ ÏÑ∏Î°úÎ°ú Ìï©ÏπòÍ∏∞ (Îπ®Í∞ÑÏÉâ Íµ¨Î∂ÑÏÑ† Ïù¥ÎØ∏ÏßÄÎ°ú Î∂ÑÎ¶¨) - bbox yÏ¢åÌëú ÏàúÏúºÎ°ú Ï†ïÎ†¨"""

    if not crops:
        return None, []
    
    # bboxÏùò yÏ¢åÌëú(top) ÏàúÏúºÎ°ú Ï†ïÎ†¨ (ÏúÑÏóêÏÑú ÏïÑÎûòÎ°ú)
    sorted_crops = sorted(crops, key=lambda crop: crop['bbox'][1])
    
    # ÏÑ∏Î°úÎ°ú Ìï©Ïπ† Ïù¥ÎØ∏ÏßÄÎì§Í≥º Íµ¨Î∂ÑÏÑ†Îì§ÏùÑ Î≥ÑÎèÑÎ°ú Ï§ÄÎπÑ
    separator_height = 28  # Îπ®Í∞ÑÏÉâ Íµ¨Î∂ÑÏÑ† ÎëêÍªò
    max_width = max(crop['img'].width for crop in sorted_crops)
    
    # Ìï©Ïπ† Ïù¥ÎØ∏ÏßÄÎì§ Î¶¨Ïä§Ìä∏ (crop Ïù¥ÎØ∏ÏßÄ + Íµ¨Î∂ÑÏÑ† Ïù¥ÎØ∏ÏßÄÎì§)
    images_to_merge = []
    crop_y_mappings = []
    current_y = 0
    
    for i, crop in enumerate(sorted_crops):
        # crop Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä
        images_to_merge.append(crop['img'])
        
        # Îß§Ìïë Ï†ïÎ≥¥ Ï†ÄÏû•: (merged_y_start, merged_y_end) -> (original_bbox)
        paste_x = (max_width - crop['img'].width) // 2
        crop_y_mappings.append({
            'merged_y_start': current_y,
            'merged_y_end': current_y + crop['img'].height,
            'original_bbox': crop['bbox'],
            'paste_x': paste_x
        })
        
        current_y += crop['img'].height
        
        # ÎßàÏßÄÎßâÏù¥ ÏïÑÎãàÎ©¥ Îπ®Í∞ÑÏÉâ Íµ¨Î∂ÑÏÑ† Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä
        if i < len(sorted_crops) - 1:
            separator_img = Image.new('RGB', (max_width, separator_height), color=(256, 0, 0))
            images_to_merge.append(separator_img)
            current_y += separator_height
    
    # Ï¥ù ÎÜíÏù¥ Í≥ÑÏÇ∞
    total_height = current_y
    
    # Ìï©Ï≥êÏßÑ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
    merged_img = Image.new('RGB', (max_width, total_height), color=(0, 0, 0))
    
    # Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ ÏàúÏÑúÎåÄÎ°ú Î∂ôÏù¥Í∏∞
    paste_y = 0
    image_idx = 0
    
    for i, crop in enumerate(sorted_crops):
        # crop Ïù¥ÎØ∏ÏßÄ Î∂ôÏù¥Í∏∞ (Ï§ëÏïô Ï†ïÎ†¨)
        crop_img = images_to_merge[image_idx]
        paste_x = (max_width - crop_img.width) // 2
        merged_img.paste(crop_img, (paste_x, paste_y))
        paste_y += crop_img.height
        image_idx += 1
        
        # Íµ¨Î∂ÑÏÑ† Ïù¥ÎØ∏ÏßÄ Î∂ôÏù¥Í∏∞ (ÎßàÏßÄÎßâÏù¥ ÏïÑÎãå Í≤ΩÏö∞)
        if i < len(sorted_crops) - 1:
            separator_img = images_to_merge[image_idx]
            merged_img.paste(separator_img, (0, paste_y))
            paste_y += separator_img.height
            image_idx += 1
    
    return merged_img, crop_y_mappings


def create_multi_image_msgs(crop_list, instruction):
    user_content = []
    for crop in crop_list:
        img = crop["img"]  # "resized_img" -> "img" ÏàòÏ†ï
        user_content.append({"type": "image", "image": img})

    user_content.append({
        "type": "text",
        "text": instruction,
    })
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": user_content,
        },
    ]
    return conversation

def run_stage2_multi_image_inference(crop_list, instruction):
    """Stage 2: multi image inference - Í∞Å cropÎ≥ÑÎ°ú Í∞úÎ≥Ñ inference"""
    
    # multi image inferenceÏö© ÎåÄÌôî ÏÉùÏÑ±
    conversation = create_multi_image_msgs(crop_list, instruction)
    
    # multi image inference Ïã§Ìñâ (Í∞Å Ïù¥ÎØ∏ÏßÄÎ≥Ñ Í≤∞Í≥º Î∞òÌôò)
    pred = multi_image_inference(conversation, model, tokenizer, processor, use_placeholder=True, topk=10)
    
    return pred

def convert_multi_image_results_to_original(multi_pred, crop_list):
    """multi_image_inference Í≤∞Í≥ºÎ•º ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï¢åÌëúÎ°ú Î≥ÄÌôòÌïòÍ≥† ÏïôÏÉÅÎ∏î"""
    
    if not multi_pred.get('per_image') or not crop_list:
        return None, []
    
    # Í∞Å cropÎ≥Ñ Í≤∞Í≥ºÎ•º ÏõêÎ≥∏ Ï¢åÌëúÎ°ú Î≥ÄÌôò
    converted_results = []
    all_candidates = []
    
    for img_idx, img_result in enumerate(multi_pred['per_image']):
        if img_idx >= len(crop_list):
            continue
            
        crop_info = crop_list[img_idx]
        crop_bbox = crop_info['bbox']  # [left, top, right, bottom]
        crop_width = crop_bbox[2] - crop_bbox[0]
        crop_height = crop_bbox[3] - crop_bbox[1]
        
        # Ìï¥Îãπ Ïù¥ÎØ∏ÏßÄÏùò topk Í≤∞Í≥ºÎì§ÏùÑ ÏõêÎ≥∏ Ï¢åÌëúÎ°ú Î≥ÄÌôò
        crop_candidates = []
        for point_idx, (point, score) in enumerate(zip(img_result['topk_points'], img_result['topk_values'])):
            # Ï†ïÍ∑úÌôîÎêú Ï¢åÌëúÎ•º crop ÎÇ¥ ÌîΩÏÖÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
            crop_x = point[0] * crop_width
            crop_y = point[1] * crop_height
            
            # crop Ï¢åÌëúÎ•º ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
            original_x = crop_bbox[0] + crop_x
            original_y = crop_bbox[1] + crop_y
            
            candidate = {
                'point': [original_x, original_y],
                'score': score,
                'crop_id': crop_info['id'],
                'crop_bbox': crop_bbox,
                'rank_in_crop': point_idx
            }
            crop_candidates.append(candidate)
            all_candidates.append(candidate)
        
        converted_results.append({
            'crop_id': crop_info['id'],
            'crop_bbox': crop_bbox,
            'candidates': crop_candidates
        })
    
    # Î™®Îì† ÌõÑÎ≥¥Îì§ÏùÑ Ï†êÏàòÏàúÏúºÎ°ú Ï†ïÎ†¨ÌïòÏó¨ ÏµúÏ¢Ö ÏïôÏÉÅÎ∏î Í≤∞Í≥º ÏÉùÏÑ±
    all_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    # ÏµúÍ≥† Ï†êÏàò ÌõÑÎ≥¥Î•º ÏµúÏ¢Ö ÏòàÏ∏°ÏúºÎ°ú ÏÑ†ÌÉù
    best_candidate = all_candidates[0] if all_candidates else None
    
    return best_candidate, all_candidates

def create_stage2_attention_to_original_multi(multi_pred, crop_list, original_size):
    """multi_image_inference Í≤∞Í≥ºÎ°úÎ∂ÄÌÑ∞ ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Ïùò Ïñ¥ÌÖêÏÖò Îßµ ÏÉùÏÑ±"""
    
    if not multi_pred.get('per_image') or not crop_list:
        return None
    
    orig_w, orig_h = original_size
    stage2_attention_map = np.zeros((orig_h, orig_w), dtype=np.float32)
    overlap_count_map = np.zeros((orig_h, orig_w), dtype=np.int32)
    
    # Í∞Å cropÏùò ÏÉÅÏúÑ 100Í∞ú Ìå®ÏπòÎ•º ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏóê Îß§Ìïë
    for img_idx, img_result in enumerate(multi_pred['per_image']):
        if img_idx >= len(crop_list):
            continue
            
        crop_info = crop_list[img_idx]
        crop_bbox = crop_info['bbox']  # [left, top, right, bottom]
        crop_width = crop_bbox[2] - crop_bbox[0]
        crop_height = crop_bbox[3] - crop_bbox[1]
        
        # Ìï¥Îãπ Ïù¥ÎØ∏ÏßÄÏùò Ïñ¥ÌÖêÏÖò Ïä§ÏΩîÏñ¥ÏôÄ Í∑∏Î¶¨Îìú Ï†ïÎ≥¥
        attn_scores = np.array(img_result['attn_scores'][0])
        n_width = img_result['n_width']
        n_height = img_result['n_height']
        
        # ÏÉÅÏúÑ 100Í∞ú Ìå®Ïπò ÏÑ†ÌÉù
        top_indices = np.argsort(attn_scores)[-100:][::-1]  # ÏÉÅÏúÑ 100Í∞ú
        
        for patch_idx in top_indices:
            # Ìå®Ïπò Ï¢åÌëú Í≥ÑÏÇ∞ (crop ÎÇ¥ÏóêÏÑú)
            patch_y = patch_idx // n_width
            patch_x = patch_idx % n_width
            
            # Ìå®Ïπò Ï§ëÏã¨Ï†êÏùò Ï†ïÍ∑úÌôîÎêú Ï¢åÌëú (crop ÎÇ¥ÏóêÏÑú)
            norm_x = (patch_x + 0.5) / n_width
            norm_y = (patch_y + 0.5) / n_height
            
            # crop ÎÇ¥ ÌîΩÏÖÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
            crop_pixel_x = norm_x * crop_width
            crop_pixel_y = norm_y * crop_height
            
            # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
            orig_pixel_x = crop_bbox[0] + crop_pixel_x
            orig_pixel_y = crop_bbox[1] + crop_pixel_y
            
            # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î≤îÏúÑ ÎÇ¥Î°ú ÌÅ¥Î¶¨Ìïë
            orig_pixel_x = max(0, min(orig_w - 1, orig_pixel_x))
            orig_pixel_y = max(0, min(orig_h - 1, orig_pixel_y))
            
            # Ï†ïÏàò Ï¢åÌëúÎ°ú Î≥ÄÌôò
            x_int = int(orig_pixel_x)
            y_int = int(orig_pixel_y)
            
            # Ïñ¥ÌÖêÏÖò Ïä§ÏΩîÏñ¥ ÎàÑÏ†Å Î∞è Í≤πÏπ® Ïπ¥Ïö¥Ìä∏
            stage2_attention_map[y_int, x_int] += attn_scores[patch_idx]
            overlap_count_map[y_int, x_int] += 1
    
    # Í≤πÏπòÎäî Î∂ÄÎ∂ÑÏùÄ Í≤πÏπòÎäî ÌöüÏàòÎ°ú ÎÇòÎàÑÏñ¥ Í∞ÄÏ§ëÌèâÍ∑† Ï≤òÎ¶¨
    valid_mask = overlap_count_map > 0
    stage2_attention_map[valid_mask] = stage2_attention_map[valid_mask] / overlap_count_map[valid_mask]
    
    return stage2_attention_map

def run_stage1_attention_based(original_image, instruction, gt_bbox):
    """ÏÉàÎ°úÏö¥ Í∞ÑÎã®Ìïú Stage 1: Attention Í∏∞Î∞ò crop ÏÉùÏÑ±"""
    
    # 1. Î¶¨ÏÇ¨Ïù¥Ï¶àÌïòÍ≥† inference
    print("üîç Stage 1: Running attention-based inference...")
    s1_pred, resized_image = run_stage1_attention_inference(original_image, instruction)
    
    # 2. GT bboxÎèÑ Î¶¨ÏÇ¨Ïù¥Ï¶à ÎπÑÏú®Ïóê ÎßûÏ∂∞ Ï°∞Ï†ï
    resize_ratio = s1_pred['resize_ratio']
    scaled_gt_bbox = [coord * resize_ratio for coord in gt_bbox]
    
    # 3. Attention Í≥†Ï†êÎì§ Ï∞æÍ∏∞
    peaks = find_attention_peaks(s1_pred, resized_image, resize_ratio)
    
    if not peaks:
        print("‚ö†Ô∏è No attention peaks found")
        return s1_pred, [], 0, resized_image, scaled_gt_bbox
    
    # 4. 1Îì±Ïùò 70% Ïù¥ÏÉÅÎßå ÎÇ®Í∏∞Í≥† ÏµúÎåÄ Í∞úÏàò Ï†úÌïú Ï†ÅÏö©
    filtered_peaks = filter_by_threshold(peaks, threshold=SELECT_THRESHOLD, max_crops=MAX_CROPS)
    
    if not filtered_peaks:
        print("‚ö†Ô∏è No peaks passed threshold")
        return s1_pred, [], 0, resized_image, scaled_gt_bbox
    
    # 5. ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÏßÅÏ†ë crop ÏÉùÏÑ±
    crops = create_crops_from_attention_peaks(filtered_peaks, original_image, resize_ratio)
    
    num_crops = len(crops)
    
    return s1_pred, crops, num_crops, resized_image, scaled_gt_bbox

def get_stage1_score_at_point(point, s1_attn_scores, s1_n_width, s1_n_height, original_size, resize_ratio):
    """ÌäπÏ†ï Ï†êÏóêÏÑúÏùò Stage1 Ïñ¥ÌÖêÏÖò Ï†êÏàòÎ•º Í≥ÑÏÇ∞"""
    
    orig_w, orig_h = original_size
    point_x, point_y = point
    
    # ÏõêÎ≥∏ Ï¢åÌëúÎ•º Î¶¨ÏÇ¨Ïù¥Ï¶àÎêú Ï¢åÌëúÎ°ú Î≥ÄÌôò
    resized_x = point_x * resize_ratio
    resized_y = point_y * resize_ratio
    
    # Î¶¨ÏÇ¨Ïù¥Ï¶àÎêú Ï¢åÌëúÎ•º Ìå®Ïπò Ï¢åÌëúÎ°ú Î≥ÄÌôò
    resized_w = orig_w * resize_ratio
    resized_h = orig_h * resize_ratio
    
    patch_x = int((resized_x / resized_w) * s1_n_width)
    patch_y = int((resized_y / resized_h) * s1_n_height)
    
    # Ìå®Ïπò Ï¢åÌëúÍ∞Ä Ïú†Ìö®Ìïú Î≤îÏúÑ ÎÇ¥Ïù∏ÏßÄ ÌôïÏù∏
    patch_x = max(0, min(patch_x, s1_n_width - 1))
    patch_y = max(0, min(patch_y, s1_n_height - 1))
    
    # Ìï¥Îãπ Ìå®ÏπòÏùò Ïñ¥ÌÖêÏÖò Ï†êÏàò Î∞òÌôò
    patch_idx = patch_y * s1_n_width + patch_x
    if patch_idx < len(s1_attn_scores):
        return float(s1_attn_scores[patch_idx])
    else:
        return 0.0

def point_in_bbox(point, bbox):
    """Ï†êÏù¥ bbox ÏïàÏóê ÏûàÎäîÏßÄ ÌôïÏù∏"""
    if point is None or bbox is None:
        return False
    return bbox[0] <= point[0] <= bbox[2] and bbox[1] <= point[1] <= bbox[3]

def monitor_memory(interval=0.1):
    start = time.time()
    while not stop_flag:
        mem = torch.cuda.memory_allocated() / 1024**3
        now = time.time() - start
        mem_log.append(mem)
        time_log.append(now)
        time.sleep(interval)

#! ================================================================================================

if __name__ == '__main__':

    set_seed(SEED)

    # Model Import (NVIDIA CUDA)
    model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
        device_map="balanced",
        # max_memory=max_memory, 
        low_cpu_mem_usage=True
    )
    # Model Import (Mac)
    # model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
    #     MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
    #     device_map="mps", # Mac
    #     # max_memory=max_memory, 
    #     low_cpu_mem_usage=False
    # )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH, max_pixels=MAX_PIXELS)
    prof = FlopsProfiler(model)

    warm_up_model(model, tokenizer, processor)

    if TFOPS_PROFILING:
        prof.start_profile()

    # save_dir Ìè¥ÎçîÎ™ÖÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎ©¥ Í≥†Ïú†Ìïú Ïù¥Î¶Ñ ÏÉùÏÑ± (save_dir -> save_dir_1 -> save_dir_2)
    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    # Process
    for task in TASKS:
        # Í∞Å taskÎ≥ÑÎ°ú Î≥ÑÎèÑÏùò Î°úÍ∑∏ ÌååÏùº ÏÉùÏÑ±
        init_iter_logger(  
            save_dir=save_dir,
            csv_name=f"iter_log_{task}.csv",
            md_name=f"iter_log_{task}.md",
            headers=[  # ÏàúÏÑú Í∑∏ÎåÄÎ°ú Îì§Ïñ¥Í∞ê
                "idx", "orig_w", "orig_h", "resize_ratio",
                "num_crop", "crop_hit",
                "s1_time", "s1_tflops", "s1_hit", 
                "s2_time", "s2_tflops", "s2_hit", 
                "s3_time", "s3_hit",
                "total_time", "total_tflops", "peak_memory_gb", 
                "crop_acc_uptonow", "s1_acc_uptonow", "s2_acc_uptonow", "s3_acc_uptonow",
                "filename", "instruction"
            ],
            write_md=False, use_fsync=True, use_lock=True
        )
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"
        screenspot_data = json.load(open(os.path.join(SCREENSPOT_JSON, dataset), 'r'))
        screenspot_data = screenspot_data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        # ÌÜµÍ≥Ñ Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
        task_res = []
        num_action = 0
        s1_time_sum = s2_time_sum = s3_time_sum = 0.0
        s1_tflops_sum = s2_tflops_sum = 0.0
        stage1_success_count = stage2_success_count = stage3_success_count = 0
        crop_success_count = 0  # ÏÉàÎ°úÏö¥ crop ÏÑ±Í≥µ Ïπ¥Ïö¥ÌÑ∞ Ï∂îÍ∞Ä
        peak_memory_sum = 0.0  # ÌîºÌÅ¨ Î©îÎ™®Î¶¨ Ìï©Í≥Ñ Ï∂îÍ∞Ä
        
        # data_sourceÎ≥Ñ ÌÜµÍ≥Ñ Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
        data_source_stats = {}

        if MEMORY_VIS:
            memory_dir = os.path.join(save_dir, "gpu_usage", task)
            os.makedirs(memory_dir, exist_ok=True)

        for j, item in tqdm(enumerate(screenspot_data)):

            if MEMORY_EVAL:
                mem_log = []
                time_log = []
                stop_flag = False
                monitor_thread = threading.Thread(target=monitor_memory, daemon=True)
                monitor_thread.start()
                torch.cuda.reset_peak_memory_stats()

            s1_tflops = s2_tflops = 0.0

            print("\n\n----------------------\n")

            num_action += 1
            
            # ÌååÏùº Î∞è Îç∞Ïù¥ÌÑ∞ Î°úÎìú
            filename = item["img_filename"]
            filename_wo_ext, ext = os.path.splitext(filename)
            img_path = os.path.join(SCREENSPOT_IMGS, filename)
            if not os.path.exists(img_path):
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["instruction"]
            original_bbox = item["bbox"]
            original_bbox = [original_bbox[0], original_bbox[1], 
                           original_bbox[0] + original_bbox[2], original_bbox[1] + original_bbox[3]]

            orig_w, orig_h = original_image.size

            # data_source Ï†ïÎ≥¥ Ï∂îÏ∂ú (ÏóÜÏúºÎ©¥ "unknown"ÏúºÎ°ú Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï)
            data_source = item.get("data_source", "unknown")

            #! ==================================================================
            #! Stage 1 | Attention-based Crop Generation
            #! ==================================================================

            if TFOPS_PROFILING:
                prof.reset_profile()

            s1_start = time.time()
            
            # ÏÉàÎ°úÏö¥ attention Í∏∞Î∞ò Î∞©Ïãù (ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©)
            s1_pred, s1_crop_list, num_crops, resized_image, scaled_gt_bbox = run_stage1_attention_based(
                original_image=original_image,
                instruction=instruction,
                gt_bbox=original_bbox
            )
            
            s1_infence_end = time.time()
            s1_time = s1_infence_end - s1_start

            if TFOPS_PROFILING:
                s1_tflops = prof.get_total_flops() / 1e12

            # Stage1 Grounding ÏÑ±Í≥µ Ïó¨Î∂Ä ÌôïÏù∏ (Ïã§Ï†ú ÏòàÏ∏° Í≤∞Í≥º)
            s1_success = False
            s1_original_point = None
            if s1_pred and "topk_points" in s1_pred and s1_pred["topk_points"]:
                s1_predicted_point = s1_pred["topk_points"][0]  # Ï†ïÍ∑úÌôîÎêú Ï¢åÌëú (0~1)
                # Ï†ïÍ∑úÌôîÎêú Ï¢åÌëúÎ•º ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌîΩÏÖÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
                s1_original_point = [
                    s1_predicted_point[0] * original_image.size[0],
                    s1_predicted_point[1] * original_image.size[1]
                ]
                s1_success = point_in_bbox(s1_original_point, original_bbox)
            
            s1_hit = "‚úÖ" if s1_success else "‚ùå"
            if s1_success:
                stage1_success_count += 1

            # Crop ÏÉùÏÑ± ÏÑ±Í≥µ Ïó¨Î∂Ä ÌôïÏù∏ (GTÍ∞Ä ÏÉùÏÑ±Îêú cropÎì§ Ï§ë ÌïòÎÇòÎùºÎèÑ Ìè¨Ìï®ÎêòÎäîÏßÄ ÌôïÏù∏)
            crop_success = False
            if num_crops > 0:
                gt_center = [(original_bbox[0] + original_bbox[2])/2, (original_bbox[1] + original_bbox[3])/2]
                for crop in s1_crop_list:
                    crop_bbox = crop["bbox"]
                    if point_in_bbox(gt_center, crop_bbox):
                        crop_success = True
                        break
            
            crop_hit = "‚úÖ" if crop_success else "‚ùå"
            if crop_success:
                crop_success_count += 1

            #! ==================================================================
            #! [Stage 2] Merged Crop Inference
            #! ==================================================================
            
            s2_tflops = 0.0
            s2_pred = s2_corrected_point = None  # ÏãúÍ∞ÅÌôîÏö© Î≥ÄÏàò Ï¥àÍ∏∞Ìôî (multi-image Î∞©Ïãù)
            stage2_success = False

            s2_inference_start = time.time()
            
            if TFOPS_PROFILING:
                prof.reset_profile()
            
            # Î©ÄÌã∞ Ïù¥ÎØ∏ÏßÄÎ°ú inference
            s2_pred = run_stage2_multi_image_inference(s1_crop_list, instruction)

            # Stage2 multi-image Í≤∞Í≥ºÎ•º ÏõêÎ≥∏ Ï¢åÌëúÎ°ú Î≥ÄÌôò
            s2_best_candidate, s2_all_candidates = convert_multi_image_results_to_original(s2_pred, s1_crop_list)
            
            # Stage2 ÏÑ±Í≥µ Ïó¨Î∂Ä ÌôïÏù∏
            if s2_best_candidate:
                s2_corrected_point = s2_best_candidate['point']
                stage2_success = point_in_bbox(s2_corrected_point, original_bbox)
            else:
                s2_corrected_point = [0, 0]
                stage2_success = False


            s2_inference_end = time.time()
            s2_time = s2_inference_end - s2_inference_start
            
            if TFOPS_PROFILING:
                s2_tflops = prof.get_total_flops() / 1e12

            s2_hit = "‚úÖ" if stage2_success else "‚ùå"
            if stage2_success:
                stage2_success_count += 1

            #! ==================================================================
            #! [Stage 3] Ensemble Processing
            #! ==================================================================
            
            s3_ensemble_point = None
            stage3_success = False
            
            # ÌÅ¨Î°≠ Î©¥Ï†ÅÏù¥ ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î©¥Ï†ÅÏùò 50%Î•º ÎÑòÎäîÏßÄ ÌôïÏù∏
            crop_area = CROP_WIDTH * CROP_HEIGHT
            original_area = orig_w * orig_h
            crop_area_ratio = crop_area / original_area
            
            s3_start = time.time()
            # Multi-image ÏïôÏÉÅÎ∏î Î∞©Î≤ï: Stage2 multi-image Í≤∞Í≥ºÏôÄ Ìï¥Îãπ ÏúÑÏπòÏùò Stage1 Ï†êÏàò Ï°∞Ìï©
            # Stage1 Ïñ¥ÌÖêÏÖò Ï†ïÎ≥¥
            s1_attn_scores = np.array(s1_pred['attn_scores'][0])
            s1_n_width = s1_pred['n_width']
            s1_n_height = s1_pred['n_height']
            s1_resize_ratio = s1_pred['resize_ratio']
            
            # Stage1 attention Ï†êÏàòÎì§ Ï†ïÍ∑úÌôî (1Îì± Í∏∞Ï§Ä)
            s1_max_score = float(max(s1_attn_scores)) if len(s1_attn_scores) > 0 else 1.0
            
            # Stage2ÏóêÏÑú topk ÌõÑÎ≥¥Îì§Îßå ÏÑ†Î≥Ñ (run_gui_actorÏôÄ ÎèôÏùº)
            if s2_all_candidates:
                # Ï†êÏàò ÏÉÅÏúÑ 10Í∞úÎßå ÏÑ†ÌÉù (run_gui_actorÏùò topk=10Í≥º ÎèôÏùº)
                s2_topk_candidates = sorted(s2_all_candidates, key=lambda x: x['score'], reverse=True)[:10]
                s2_topk_scores = [candidate['score'] for candidate in s2_topk_candidates]
                
                # topk Ï†êÏàòÎì§ÎßåÏúºÎ°ú Ï†ïÍ∑úÌôî (run_gui_actorÏôÄ ÎèôÏùº)
                if s2_topk_scores:
                    s2_max_score = max(s2_topk_scores)
                    if s2_max_score > 0:
                        s2_normalized_scores = [score / s2_max_score for score in s2_topk_scores]
                    else:
                        s2_normalized_scores = [0.0] * len(s2_topk_scores)
                else:
                    s2_normalized_scores = []
            else:
                s2_topk_candidates = []
                s2_normalized_scores = []
            
            # Í∞Å Stage2 topk Ï†êÏóê ÎåÄÌï¥ ÏïôÏÉÅÎ∏î Ï†êÏàò Í≥ÑÏÇ∞
            ensemble_candidates = []
            
            for i, candidate in enumerate(s2_topk_candidates):
                s2_original_point = candidate['point']
                
                # Ìï¥Îãπ Ï†êÏóêÏÑúÏùò Stage1 Ï†êÏàò Í≥ÑÏÇ∞ (Ï†ïÍ∑úÌôîÎêú Í∞í)
                s1_raw_score = get_stage1_score_at_point(
                    s2_original_point, s1_attn_scores, s1_n_width, s1_n_height, 
                    original_image.size, s1_resize_ratio
                )
                s1_score = s1_raw_score / s1_max_score if s1_max_score > 0 else 0.0
                
                # Stage2 Ï†êÏàòÎäî Ï†ïÍ∑úÌôîÎêú Ï†êÏàò ÏÇ¨Ïö© (run_gui_actorÏôÄ ÎèôÏùº)
                s2_score = s2_normalized_scores[i] if i < len(s2_normalized_scores) else 0.0
                
                # ÏïôÏÉÅÎ∏î Ï†êÏàò Í≥ÑÏÇ∞
                ensemble_score = STAGE1_ENSEMBLE_RATIO * s1_score + STAGE2_ENSEMBLE_RATIO * s2_score
                
                ensemble_candidates.append({
                    'point': s2_original_point,
                    'score': ensemble_score,
                    's1_score': s1_score,
                    's2_score': s2_score,
                    'crop_id': candidate['crop_id'],
                    'rank_in_crop': candidate['rank_in_crop'],
                    's2_rank': i + 1  # topk ÎÇ¥ÏóêÏÑúÏùò ÏàúÏúÑ
                })
            
            # ÏµúÍ≥† Ï†êÏàòÎ•º Í∞ÄÏßÑ Ï†ê ÏÑ†ÌÉù
            if ensemble_candidates:
                best_candidate = max(ensemble_candidates, key=lambda x: x['score'])
                s3_ensemble_point = best_candidate['point']
            else:
                # Stage2 Í≤∞Í≥ºÍ∞Ä ÏóÜÏúºÎ©¥ Stage1 Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©
                if s1_original_point:
                    s3_ensemble_point = s1_original_point
                else:
                    s3_ensemble_point = [0, 0]

            s3_end = time.time()
            s3_time = s3_end - s3_start
            
            # ÎîîÎ≤ÑÍ∑∏ Ï†ïÎ≥¥ Ï∂úÎ†• (ÏÉÅÏúÑ 3Í∞úÎßå) - run_gui_actorÏôÄ ÎèôÏùºÌïú ÌòïÌÉú
            if ensemble_candidates:
                print(f"üéØ Stage3 Ensemble Candidates (Top 3):")
                for i, candidate in enumerate(sorted(ensemble_candidates, key=lambda x: x['score'], reverse=True)[:3]):
                    print(f"  Rank {i+1}: S2_rank={candidate['s2_rank']}, S1={candidate['s1_score']:.3f}, S2={candidate['s2_score']:.3f}, Ensemble={candidate['score']:.3f}")
            else:
                print(f"üéØ Stage3: No ensemble candidates, using Stage1 result")
            
            # ÏãúÍ∞ÅÌôîÎ•º ÏúÑÌï¥ ÌõÑÎ≥¥Îì§ Ï†ÄÏû•
            s3_ensemble_candidates = ensemble_candidates
            
            # ÏïôÏÉÅÎ∏î Í≤∞Í≥ºÎ°ú ÏÑ±Í≥µ Ïó¨Î∂Ä ÌôïÏù∏
            stage3_success = point_in_bbox(s3_ensemble_point, original_bbox)
            
            s3_hit = "‚úÖ" if stage3_success else "‚ùå"
            if stage3_success:
                stage3_success_count += 1

            #! ==================================================================
            #! [Common Processing]
            #! ==================================================================
            
            # Í≥µÌÜµ ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            s1_time_sum += s1_time
            s2_time_sum += s2_time
            s3_time_sum += s3_time
            s1_tflops_sum += s1_tflops
            s2_tflops_sum += s2_tflops
                
            # ÏÑ±Îä• Î°úÍπÖ
            total_time = s1_time + s2_time
            if TFOPS_PROFILING:
                total_tflops_this = s1_tflops + s2_tflops  # Stage3Îäî FLOPs Ï†úÏô∏

            #! ==================================================================
            #! [Visualization - After Time Measurement]
            #! ==================================================================
            
            # ÏãúÍ∞ÅÌôîÏö© ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï (stage3 Í≤∞Í≥ºÏóê Îî∞Îùº)
            if VISUALIZE and (not VIS_ONLY_WRONG or not stage3_success):
                inst_dir_name = re.sub(r'\W+', '_', instruction).strip('_')
                result_folder = "correct" if stage3_success else "incorrect"
                inst_dir = os.path.join(save_dir, f"{task}_visualize_{result_folder}", f"{num_action}_{inst_dir_name}")

                # Stage1 ÏãúÍ∞ÅÌôî
                visualize_stage1_attention_crops(
                    s1_pred=s1_pred,
                    resized_image=resized_image, 
                    crop_list=s1_crop_list,
                    original_image=original_image,
                    save_dir=inst_dir,
                    instruction=instruction,
                    gt_bbox=original_bbox,
                    s1_predicted_point=s1_original_point
                )
                
                # Stage2 Multi-Image ÏãúÍ∞ÅÌôî
                if s2_pred and s1_crop_list:  # Stage2 Í≤∞Í≥ºÍ∞Ä ÏûàÏùÑ ÎïåÎßå ÏãúÍ∞ÅÌôî
                    visualize_stage2_multi_attention(
                        s2_pred=s2_pred,
                        crop_list=s1_crop_list,
                        original_image=original_image,
                        save_dir=inst_dir,
                        instruction=instruction,
                        predicted_point=s2_corrected_point
                    )
                
                # Stage3 ÏïôÏÉÅÎ∏î ÏãúÍ∞ÅÌôî
                visualize_stage3_point_ensemble(
                    s3_ensemble_candidates=s3_ensemble_candidates if 's3_ensemble_candidates' in locals() else [],
                    original_image=original_image,
                    crop_list=s1_crop_list,
                    original_bbox=original_bbox,
                    s3_ensemble_point=s3_ensemble_point,
                    s2_corrected_point=s2_corrected_point,
                    s1_original_point=s1_original_point,
                    stage1_ratio=STAGE1_ENSEMBLE_RATIO,
                    stage2_ratio=STAGE2_ENSEMBLE_RATIO,
                    save_dir=inst_dir,
                    vis_only_wrong=VIS_ONLY_WRONG,
                    stage3_success=stage3_success
                )


            num_attention_crops = len(s1_crop_list)
            print(f"‚úÇÔ∏è  Attention Crops : {num_attention_crops}")
            print(f"üïñ Times - S1: {s1_time:.2f}s | S2: {s2_time:.2f}s | Total: {total_time:.2f}s")
            if TFOPS_PROFILING:
                print(f"üî• FLOPs - S1: {s1_tflops:.2f} | S2: {s2_tflops:.2f} | Total: {total_tflops_this:.2f} TFLOPs")
            print(f"{'‚úÖ Success' if stage3_success else '‚ùåüéØ Fail'}")

            #! ==================================================================
            #! [End]
            #! ==================================================================

            if MEMORY_EVAL:
                time.sleep(0.1)
                stop_flag = True
                monitor_thread.join()

                # ÌîºÌÅ¨ Î©îÎ™®Î¶¨ Í≥ÑÏÇ∞ (GB Îã®ÏúÑ, ÏÜåÏàòÏ†ê 3ÏûêÎ¶¨)
                peak_memory = max(mem_log) if mem_log else 0.0
                peak_memory_gb = round(peak_memory, 3)

                if MEMORY_VIS:
                    import matplotlib.pyplot as plt
                    plt.figure(figsize=(10, 4))
                    plt.plot(time_log, mem_log)
                    plt.xlabel("Time (s)")
                    plt.ylabel("GPU Memory Allocated (GB)")
                    plt.title("GPU Memory Usage Over Time")
                    plt.grid(True)
                    plt.savefig(f"{memory_dir}/{num_action}_{filename}")
                    plt.close()  # Î©îÎ™®Î¶¨ ÎàÑÏàò Î∞©ÏßÄÎ•º ÏúÑÌï¥ close Ï∂îÍ∞Ä

            if MEMORY_EVAL:
                peak_memory_sum += peak_memory_gb

            # data_sourceÎ≥Ñ ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            if data_source not in data_source_stats:
                data_source_stats[data_source] = {
                    'num_action': 0,
                    's1_time_sum': 0.0,
                    's2_time_sum': 0.0,
                    's3_time_sum': 0.0,
                    's1_tflops_sum': 0.0,
                    's2_tflops_sum': 0.0,
                    'total_tflops': 0.0,
                    'stage1_success_count': 0,
                    'crop_success_count': 0,
                    'stage2_success_count': 0,
                    'stage3_success_count': 0,
                    'peak_memory_sum': 0.0
                }
            
            stats = data_source_stats[data_source]
            stats['num_action'] += 1
            stats['s1_time_sum'] += s1_time
            stats['s2_time_sum'] += s2_time
            stats['s3_time_sum'] += s3_time
            if TFOPS_PROFILING:
                stats['s1_tflops_sum'] += s1_tflops
                stats['s2_tflops_sum'] += s2_tflops
                stats['total_tflops'] += total_tflops_this
            if MEMORY_EVAL:
                stats['peak_memory_sum'] += peak_memory_gb
            if s1_success:
                stats['stage1_success_count'] += 1
            if crop_success:
                stats['crop_success_count'] += 1
            if stage2_success:
                stats['stage2_success_count'] += 1
            if stage3_success:
                stats['stage3_success_count'] += 1

            up2now_s1_score = stage1_success_count / num_action * 100
            up2now_crop_score = crop_success_count / num_action * 100
            up2now_s2_score = stage2_success_count / num_action * 100
            up2now_s3_ensemble_score = stage3_success_count / num_action * 100
            # print(f"Up2Now Crop Accuracy: {up2now_crop_score:.2f}%")
            print(f"Up2Now Stage1 Accuracy: {up2now_s1_score:.2f}%")
            print(f"Up2Now Stage2 Accuracy: {up2now_s2_score:.2f}%")
            print(f"Up2Now Stage3 Ensemble Accuracy: {up2now_s3_ensemble_score:.2f}%")

            # Iter log - Í∞úÏÑ†Îêú Î°úÍπÖ
            append_iter_log(
                idx=j+1,
                orig_w=original_image.size[0],
                orig_h=original_image.size[1],
                resize_ratio=s1_pred['resize_ratio'],
                num_crop=num_attention_crops,
                crop_hit=crop_hit,
                s1_time=f"{s1_time:.3f}",
                s1_tflops=f"{s1_tflops:.2f}",
                s1_hit=s1_hit,
                s2_time=f"{s2_time:.3f}",
                s2_tflops=f"{s2_tflops:.2f}",
                s2_hit=s2_hit,
                s3_time=f"{s3_time:.3f}",
                s3_hit=s3_hit,
                total_time=f"{total_time:.3f}",
                total_tflops=f"{total_tflops_this:.2f}",
                peak_memory_gb=f"{peak_memory_gb:.3f}" if MEMORY_EVAL else "N/A",
                crop_acc_uptonow=f"{up2now_crop_score:.2f}",
                s1_acc_uptonow=f"{up2now_s1_score:.2f}",
                s2_acc_uptonow=f"{up2now_s2_score:.2f}",
                s3_acc_uptonow=f"{up2now_s3_ensemble_score:.2f}",
                filename=filename_wo_ext,
                instruction=instruction[:50] + "..." if len(instruction) > 50 else instruction
            )

            # JSON Í∏∞Î°ù - ÌïµÏã¨ Ï†ïÎ≥¥Îßå
            item_res = {
                'filename': filename,
                'orig_w': original_image.size[0],
                'orig_h': original_image.size[1],
                'instruction': instruction,
                'gt_bbox': original_bbox,
                'data_source': data_source,
                'num_crop': num_attention_crops,
                'crop_success': crop_success,
                'stage1_success': s1_success,
                'stage2_success': stage2_success,
                'stage3_success': stage3_success,
                's1_hit': s1_hit,
                'crop_hit': crop_hit,
                's2_hit': s2_hit,
                's3_hit': s3_hit,
                's3_ensemble_point': s3_ensemble_point,
                's1_original_point': s1_original_point,
                's2_original_point': s2_corrected_point,
                's1_time': s1_time,
                's2_time': s2_time,
                's3_time': s3_time,
                'total_time': total_time,
                's1_tflops': s1_tflops,
                's2_tflops': s2_tflops,
                'total_tflops': s1_tflops+s2_tflops,
                'peak_memory_gb': peak_memory_gb if MEMORY_EVAL else None,
                'ensemble_config': {
                    'attention_ratio': STAGE1_ENSEMBLE_RATIO,
                    'crop_ratio': STAGE2_ENSEMBLE_RATIO
                }
            }
            task_res.append(item_res)

        #! ==================================================
        # Í≤∞Í≥º Json Ï†ïÎ¶¨
        os.makedirs(os.path.join(save_dir, "json"), exist_ok=True)
        with open(os.path.join(save_dir, "json", dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        # ÏµúÏ¢Ö ÏÑ±Îä• Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        metrics = {
            "task": task,
            "total_samples": num_action,
            "crop_accuracy": crop_success_count / num_action * 100,
            "stage1_accuracy": stage1_success_count / num_action * 100,
            "stage2_accuracy": stage2_success_count / num_action * 100,
            "stage3_accuracy": stage3_success_count / num_action * 100,
            "avg_times": {
                "stage1": s1_time_sum / num_action,
                "stage2": s2_time_sum / num_action,
                "stage3": s3_time_sum / num_action,
                "total": (s1_time_sum + s2_time_sum + s3_time_sum) / num_action
            },
            "avg_flops_tflops": {
                "stage1": s1_tflops_sum / num_action,
                "stage2": s2_tflops_sum / num_action,
                "total": (s1_tflops_sum + s2_tflops_sum) / num_action
            },
            "avg_peak_memory_gb": round(peak_memory_sum / num_action, 3) if MEMORY_EVAL else None,
            "hyperparameters": {
                "select_threshold": SELECT_THRESHOLD,
                "attn_impl": ATTN_IMPL,
                "STAGE1_ensemble_ratio": STAGE1_ENSEMBLE_RATIO,
                "STAGE2_ensemble_ratio": STAGE2_ENSEMBLE_RATIO
            }
        }

        with open(os.path.join(save_dir, f"results_{task}.json"), "w") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=4)

        # data_sourceÎ≥Ñ Î©îÌä∏Î¶≠ Ï†ÄÏû•
        data_source_metrics = {}
        for ds, stats in data_source_stats.items():
            if stats['num_action'] > 0:
                data_source_metrics[ds] = {
                    "task": task,
                    "data_source": ds,
                    "total_samples": stats['num_action'],
                    "crop_accuracy": stats['crop_success_count'] / stats['num_action'] * 100,
                    "stage1_accuracy": stats['stage1_success_count'] / stats['num_action'] * 100,
                    "stage2_accuracy": stats['stage2_success_count'] / stats['num_action'] * 100,
                    "stage3_accuracy": stats['stage3_success_count'] / stats['num_action'] * 100,
                    "avg_times": {
                        "stage1": stats['s1_time_sum'] / stats['num_action'],
                        "stage2": stats['s2_time_sum'] / stats['num_action'],
                        "stage3": stats['s3_time_sum'] / stats['num_action'],
                        "total": (stats['s1_time_sum'] + stats['s2_time_sum'] + stats['s3_time_sum']) / stats['num_action']
                    },
                    "avg_flops_tflops": {
                        "stage1": stats['s1_tflops_sum'] / stats['num_action'],
                        "stage2": stats['s2_tflops_sum'] / stats['num_action'],
                        "total": (stats['s1_tflops_sum'] + stats['s2_tflops_sum']) / stats['num_action']
                    },
                    "avg_peak_memory_gb": round(stats['peak_memory_sum'] / stats['num_action'], 3) if MEMORY_EVAL else None,
                    "hyperparameters": {
                        "select_threshold": SELECT_THRESHOLD,
                        "attn_impl": ATTN_IMPL,
                        "STAGE1_ensemble_ratio": STAGE1_ENSEMBLE_RATIO,
                        "STAGE2_ensemble_ratio": STAGE2_ENSEMBLE_RATIO
                    }
                }
        
        with open(os.path.join(save_dir, f"source_results_{task}.json"), "w") as dsf:
            json.dump(data_source_metrics, dsf, ensure_ascii=False, indent=4)

        # Ï†ÑÏ≤¥ Í≤∞Í≥ºÎ•º CSV ÌååÏùºÏóê Ìïú Ï§Ñ Ï∂îÍ∞Ä
        results_csv_path = "../_results"
        os.makedirs(results_csv_path, exist_ok=True)
        csv_file_path = os.path.join(results_csv_path, f"results_{task}.csv")
        
        # CSV Ìó§Îçî Ï†ïÏùò
        csv_headers = [
            "method",
            "min_resize", "max_resize", "select_threshold", "stage1_ensemble_ratio", "crop_width", "crop_height",
            "total_samples", "crop_accuracy", "stage1_accuracy", "stage2_accuracy", "stage3_accuracy",
            "avg_stage1_time", "avg_stage2_time", "avg_stage3_time", "avg_total_time",
            "avg_stage1_tflops", "avg_stage2_tflops", "avg_total_tflops", "avg_peak_memory_gb",
            "timestamp"
        ]
        
        # CSV Îç∞Ïù¥ÌÑ∞ Ìñâ ÏÉùÏÑ±
        import datetime
        csv_row = [
            method,
            MIN_RESIZE, MAX_RESIZE, SELECT_THRESHOLD, STAGE1_ENSEMBLE_RATIO, CROP_WIDTH, CROP_HEIGHT,
            num_action, 
            round(metrics['crop_accuracy'], 2),
            round(metrics['stage1_accuracy'], 2),
            round(metrics['stage2_accuracy'], 2), 
            round(metrics['stage3_accuracy'], 2),
            round(metrics['avg_times']['stage1'], 4),
            round(metrics['avg_times']['stage2'], 4),
            round(metrics['avg_times']['stage3'], 4),
            round(metrics['avg_times']['total'], 4),
            round(metrics['avg_flops_tflops']['stage1'], 2),
            round(metrics['avg_flops_tflops']['stage2'], 2),
            round(metrics['avg_flops_tflops']['total'], 2),
            metrics['avg_peak_memory_gb'] if metrics['avg_peak_memory_gb'] else 0.0,
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        ]
        
        # CSV ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ Ìó§ÎçîÏôÄ Ìï®Íªò ÏÉùÏÑ±, ÏûàÏúºÎ©¥ Îç∞Ïù¥ÌÑ∞ ÌñâÎßå Ï∂îÍ∞Ä
        import csv
        file_exists = os.path.exists(csv_file_path)
        
        with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # ÌååÏùºÏù¥ ÏóÜÍ±∞ÎÇò ÎπÑÏñ¥ÏûàÏúºÎ©¥ Ìó§Îçî Ï∂îÍ∞Ä
            if not file_exists or os.path.getsize(csv_file_path) == 0:
                writer.writerow(csv_headers)
            
            # Îç∞Ïù¥ÌÑ∞ Ìñâ Ï∂îÍ∞Ä
            writer.writerow(csv_row)
        
        print(f"üìù Results saved to CSV: {csv_file_path}")

        # ÏµúÏ¢Ö Í≤∞Í≥º Ï∂úÎ†•
        print("=" * 60)
        print(f"üìä Final Results for {task}:")
        print(f"Total Samples: {num_action}")
        print(f"Crop Accuracy: {metrics['crop_accuracy']:.2f}%")
        print(f"Stage1 Accuracy: {metrics['stage1_accuracy']:.2f}%")
        print(f"Stage2 Accuracy: {metrics['stage2_accuracy']:.2f}%")
        print(f"Stage3 Ensemble Accuracy: {metrics['stage3_accuracy']:.2f}%")
        print(f"Avg Times: S1 {metrics['avg_times']['stage1']:.3f}s | S2 {metrics['avg_times']['stage2']:.3f}s | S3 {metrics['avg_times']['stage3']:.3f}s | Total {metrics['avg_times']['total']:.3f}s")
        print(f"Avg FLOPs: S1 {metrics['avg_flops_tflops']['stage1']:.2f} | S2 {metrics['avg_flops_tflops']['stage2']:.2f} | Total {metrics['avg_flops_tflops']['total']:.2f} TFLOPs")
        if MEMORY_EVAL and metrics['avg_peak_memory_gb'] is not None:
            print(f"Avg Peak Memory: {metrics['avg_peak_memory_gb']:.3f} GB")
        print(f"Ensemble Config: Attention {STAGE1_ENSEMBLE_RATIO:.1f}, Crop {STAGE2_ENSEMBLE_RATIO:.1f}")
        
        print("=" * 60)