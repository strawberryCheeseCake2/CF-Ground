'''
Final version
'''

# _query ì´ê±¸ë¡œ ëª¨ë¸ ì¶”ë¡ í•˜ë©´ ë˜ëŠ”ê±°

import os
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('gpu', type=int, default=0, help='GPU number')
parser.add_argument('--r', type=float, default=0.50, help='Stage 1 Resize ratio')
parser.add_argument('--th', type=float, default=0.11, help='Stage 1 Crop threshold')
parser.add_argument('--p', type=int, default=20, help='Stage 1 Crop Padding')
parser.add_argument('--v', action='store_true', help='Whether to save visualization images')
args = parser.parse_args()


# dataset = load_dataset("demisama/UGround-Offline-Evaluation") ì´ê±¸ë¡œ ë¸”ë½ ë¶ˆëŸ¬ì˜¤ê¸°



os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)

#! Hyperparameter =====================================================================================

ATTN_IMPL = "eager"                      # attention implement "eager" "sdpa" "flash" "efficient"

# Image Resize Ratios
RESIZE_RATIO = args.r

# Connected Region Based Cropping
REGION_THRESHOLD = args.th              # ì—°ê²°ëœ ì˜ì—­ ê²€ì¶œì„ ìœ„í•œ ì„ê³„ê°’ (0~1)  # TODO: 0.1 ~ 0.5 ì¤‘ ìµœì  ì°¾ê¸°
MIN_PATCHES = 1                         # ìµœì†Œ íŒ¨ì¹˜ ìˆ˜ (ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œê±°)
BBOX_PADDING = args.p                   # bbox ìƒí•˜ì¢Œìš°ë¡œ í™•ì¥í•  í”½ì…€  # TODO: 0 ~ 50 ì¤‘ ìµœì  ì°¾ê¸°

# Ensemble Hyperparameters

# ìµœëŒ€ PIXELS ì œí•œ
MAX_PIXELS = 3211264  # Processë‹¨ì—ì„œ ì ìš©

# csvì— ê¸°ë¡í•  method ì´ë¦„
method = "mind2web_qwen2vl_vanilla_0924"

memo = f"0923_gpt4o"
#! Argument ==========================================================================================

SEED = 0

# Dataset & Model
MLLM_PATH = "microsoft/GUI-Actor-2B-Qwen2-VL"
SCREENSPOT_IMGS = "/data/hj/UGround/offline_evaluation/Multimodal-Mind2Web/data/sample/cross_"       # input image ê²½ë¡œ
SCREENSPOT_JSON = "../data"                          # input image jsoníŒŒì¼ ê²½ë¡œ
# TASKS = ["domain", "website", "task"]
TASKS = ["domain"]
SAMPLE_RANGE = slice(None)

# Visualize & Logging
VISUALIZE = args.v if args.v else True  # ì‹œê°í™” ì—¬ë¶€
VIS_ONLY_WRONG = False                                # Trueë©´ í‹€ë¦° ê²ƒë§Œ ì‹œê°í™”, Falseë©´ ëª¨ë“  ê²ƒ ì‹œê°í™”
TFOPS_PROFILING = True
MEMORY_VIS = False

# Save Path
SAVE_DIR = f"../attn_output/" + method + "/" + memo

#! ==================================================================================================

# Standard Library
import os
import sys
import time
import re
import json
import logging
logging.disable(logging.CRITICAL)  # ëª¨ë“  ë¡œê¹… í˜¸ì¶œ ë¬´ë ¥í™”
from typing import Dict, List
from collections import deque

# Third-Party Libraries
import numpy as np
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoTokenizer, set_seed

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from util.iter_logger import init_iter_logger, append_iter_log  # log csv ê¸°ë¡ íŒŒì¼
# from gui_actor.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer
from gui_actor.modeling import Qwen2VLForConditionalGenerationWithPointer
from gui_actor.inference import inference, ForceFollowTokensLogitsProcessor
from gui_actor.constants import DEFAULT_POINTER_PAD_TOKEN, DEFAULT_POINTER_END_TOKEN
if TFOPS_PROFILING:
    from deepspeed.profiling.flops_profiler import FlopsProfiler

#! ==============================================================================================


# 

DIR_MAX_LEN = 100

def truncate_to_bytes(filename: str, max_bytes: int) -> str:
    """
    ë¬¸ìì—´ì„ ì§€ì •ëœ ë°”ì´íŠ¸ ê¸¸ì´ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¦…ë‹ˆë‹¤.
    ë©€í‹°ë°”ì´íŠ¸ ë¬¸ìê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # 1. ë¬¸ìì—´ì„ UTF-8 ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©
    encoded_bytes = filename.encode('utf-8')

    # 2. ë°”ì´íŠ¸ ê¸¸ì´ê°€ ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ìë¥´ê¸°
    if len(encoded_bytes) <= max_bytes:
        return filename

    truncated_bytes = encoded_bytes[:max_bytes]

    # 3. ë””ì½”ë”© ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë§ˆì§€ë§‰ ë°”ì´íŠ¸ë¥¼ ì œê±°í•˜ë©° ì‹œë„
    while True:
        try:
            # ë””ì½”ë”© ì„±ê³µ ì‹œ, í•´ë‹¹ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ê³  ì¢…ë£Œ
            return truncated_bytes.decode('utf-8')
        except UnicodeDecodeError:
            # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ (ë¬¸ì ì¤‘ê°„ì— ì˜ë¦° ê²½ìš°), ë§ˆì§€ë§‰ ë°”ì´íŠ¸ë¥¼ í•˜ë‚˜ ì œê±°
            truncated_bytes = truncated_bytes[:-1]

# 

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        return super(NpEncoder, self).default(obj)

def warm_up_model(model, tokenizer, processor):
    print("ğŸ‹ï¸â€â™‚ï¸ Warming up the model...")
    dummy_instruction = "This is a dummy instruction for warm-up."
    dummy_image = Image.new("RGB", (1000, 1000), color=(255, 255, 255))  # 1000x1000 í°ìƒ‰ ì´ë¯¸ì§€
    dummy_msgs = create_conversation_stage1(image=dummy_image, instruction=dummy_instruction)
    
    # ì˜ˆì—´ìš© inference ì‹¤í–‰
    for _ in range(3):  # 3ë²ˆ ë°˜ë³µ
        with torch.no_grad():
            _ = inference(dummy_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    print("ğŸ‹ï¸â€â™‚ï¸ Warm-up complete!")

def create_conversation_stage1(image, instruction):
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        # ê¸°ì¡´ content
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": instruction,
                }
            ],
        },
    ]
    return conversation



def point_in_bbox(point, bbox):
    """ì ì´ bbox ì•ˆì— ìˆëŠ”ì§€ í™•ì¸"""
    if point is None or bbox is None:
        return False
    return bbox[0] <= point[0] <= bbox[2] and bbox[1] <= point[1] <= bbox[3]

#! ================================================================================================

if __name__ == '__main__':
    
    set_seed(SEED)

    # Model Import (NVIDIA CUDA)
    model = Qwen2VLForConditionalGenerationWithPointer.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
        device_map="balanced",
        # max_memory=max_memory, 
        low_cpu_mem_usage=True
    )
    # Qwen2VL
    # grounding_system_message = "You are a GUI agent. You are given a task and a screenshot of the screen. You need to perform a series of pyautogui actions to complete the task."
    # Qwen2.5VL
    grounding_system_message = "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, your task is to locate the screen element that corresponds to the instruction. You should output a PyAutoGUI action that performs a click on the correct position. To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. For example, you can output: pyautogui.click(<your_special_token_here>)."
    # Model Import (Mac)
    # model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
    #     MLLM_PATH, torch_dtype="auto", attn_implementation=ATTN_IMPL,
    #     device_map="mps", # Mac
    #     # max_memory=max_memory, 
    #     low_cpu_mem_usage=False
    # )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH, max_pixels=MAX_PIXELS)
    logits_processor_pointer = ForceFollowTokensLogitsProcessor(
        token_a_id=tokenizer.encode(DEFAULT_POINTER_PAD_TOKEN)[0],
        forced_sequence=[
            tokenizer.encode(DEFAULT_POINTER_END_TOKEN)[0]
        ]
    )
    prof = FlopsProfiler(model)

    warm_up_model(model, tokenizer, processor)

    if TFOPS_PROFILING:
        prof.start_profile()

    # save_dir í´ë”ëª…ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„± (save_dir -> save_dir_1 -> save_dir_2)
    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    # ì „ì²´ task í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
    total_samples = 0
    total_success = 0
    total_s1_time = 0.0
    total_s1_tflops = 0.0

    # CSV í—¤ë” ì •ì˜ (ëª¨ë“  taskì—ì„œ ê³µí†µ ì‚¬ìš©)
    csv_headers = [
        "method",
        "resize_ratio", "region_threshold", "bbox_padding",
        "total_samples", "crop_accuracy", "stage1_accuracy", "stage2_accuracy", "stage3_accuracy",
        "avg_stage1_time", "avg_stage2_time", "avg_stage3_time", "avg_total_time",
        "avg_stage1_tflops", "avg_stage2_tflops", "avg_total_tflops",
        "timestamp"
    ]

# import json

# data = []
# # 'cross_domain_plan.jsonl' íŒŒì¼ì„ 'r'(ì½ê¸°) ëª¨ë“œë¡œ ì—½ë‹ˆë‹¤.
# # utf-8 ì¸ì½”ë”©ì„ ëª…ì‹œí•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# with open('cross_domain_plan.jsonl', 'r', encoding='utf-8') as f:
#     for line in f:
#         # ê° ì¤„(line)ì„ JSON ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ data ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
#         data.append(json.loads(line))

# # ì²˜ìŒ 5ê°œì˜ ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ì—¬ í™•ì¸í•©ë‹ˆë‹¤.
# for item in data[:5]:
#     print(item)

# # ì „ì²´ ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
# print(f"ì´ {len(data)}ê°œì˜ ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


    # Data loading setup
    data_folder = '/data/hj/UGround/offline_evaluation/Multimodal-Mind2Web/data/gpt-4o_results/'
    # data_folder = '/data/hj/UGround/offline_evaluation/Multimodal-Mind2Web/data/gpt-4-turbo_results/'
    # Process
    for task in TASKS:
        # ê° taskë³„ë¡œ ë³„ë„ì˜ ë¡œê·¸ íŒŒì¼ ìƒì„±
        init_iter_logger(  
            save_dir=save_dir,
            csv_name=f"iter_log_{task}.csv",
            md_name=f"iter_log_{task}.md",
            headers=[  # ìˆœì„œ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°
                "idx", "orig_w", "orig_h", "resize_ratio",
                "num_crop", "crop_hit",
                "s1_time", "s1_tflops", "s1_hit", 
                "s2_time", "s2_tflops", "s2_hit", 
                "s3_time", "s3_hit",
                "total_time", "total_tflops",
                "crop_acc_uptonow", "s1_acc_uptonow", "s2_acc_uptonow", "s3_acc_uptonow",
                "filename", "instruction"
            ],
            write_md=False, use_fsync=True, use_lock=True
        )
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"



        #data ë¶ˆëŸ¬ì˜¤ê¸°
        data = []
        target = data_folder + 'cross_' + task + '_query.jsonl'
        with open(target, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line))

        screenspot_data = data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        # í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        task_res = []
        num_action = 0
        time_sum = 0.0
        tflops_sum = 0.0
        success_count = 0
        
        # data_sourceë³„ í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        data_source_stats = {}

        if MEMORY_VIS:
            memory_dir = os.path.join(save_dir, "gpu_usage", task)
            os.makedirs(memory_dir, exist_ok=True)

        for j, item in tqdm(enumerate(screenspot_data)):

            s1_tflops = s2_tflops = 0.0
            num_action += 1

            print("\n\n----------------------\n")
            
            # íŒŒì¼ ë° ë°ì´í„° ë¡œë“œ
            filename = item["image"]
            filename_wo_ext, ext = os.path.splitext(filename)

            blockpath='/data/hj/UGround/offline_evaluation/Multimodal-Mind2Web/data/blocks_dir/'

            img_path = os.path.join(blockpath, filename)

            ###HERE!!
            if not os.path.exists(img_path):
                print("ì—†ëŠ”ë°ìš”", item)
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["description"]

            pos_candidates_list = []
            raw_bbox = item['bbox']
            ground_truth_bboxes = [ [x[0], x[1], x[0]+x[2], x[1]+x[3]] for x in raw_bbox]
            
            orig_w, orig_h = original_image.size

            # data_source ì •ë³´ ì¶”ì¶œ (ì—†ìœ¼ë©´ "unknown"ìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •)
            data_source = item.get("data_source", "unknown")

            if not ground_truth_bboxes:
                ground_truth_bboxes.append([0,0,0,0])  # ì„ì‹œë¡œ ë§‰ì•„ë‘ .
                print("No GT bbox!!", j)
            if len(ground_truth_bboxes) > 1:
                print("Multiple GT bbox!!", j)

            if TFOPS_PROFILING:
                prof.reset_profile()

            s1_start = time.time()
            
            original_bbox = ground_truth_bboxes[0] # ì„ì‹œë¡œ ë§‰ì•„ë‘ .

            conversation = [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": grounding_system_message,
                        }
                    ]
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            # "image": example["image"], # PIL.Image.Image or str to path
                            "image": original_image, # PIL.Image.Image or str to path
                            # "image_url": "https://xxxxx.png" or "https://xxxxx.jpg" or "file://xxxxx.png" or "data:image/png;base64,xxxxxxxx", will be split by "base64,"
                        },
                        {
                            "type": "text",
                            "text": item["description"]
                        },
                    ],
                },
            ]
            
            pred = inference(conversation, model, tokenizer, processor, logits_processor=logits_processor_pointer)

            s1_end = time.time()
            s1_time = s1_end - s1_start

            if TFOPS_PROFILING:
                s1_tflops = prof.get_total_flops() / 1e12

            # Stage1 Grounding ì„±ê³µ ì—¬ë¶€ í™•ì¸ (ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼)
            success = False
            original_point = None
            if pred and "topk_points" in pred and pred["topk_points"]:
                predicted_point = pred["topk_points"][0]  # ì •ê·œí™”ëœ ì¢Œí‘œ (0~1)
                # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                original_point = [
                    predicted_point[0] * original_image.size[0],
                    predicted_point[1] * original_image.size[1]
                ]
                success = any(point_in_bbox(original_point, bbox) for bbox in ground_truth_bboxes)
            
            
            s1_hit = "âœ…" if success else "âŒ"
            if success:
                success_count += 1

            #! ==================================================================
            #! [Visualization - After Time Measurement]
            #! ==================================================================
            

            #! ==================================================================
            #! [Common Processing]
            #! ==================================================================
            
            # ê³µí†µ í†µê³„ ì—…ë°ì´íŠ¸
            total_time = s1_time
            time_sum += s1_time
            tflops_sum += s1_tflops

            print(f"ğŸ•– Times - S1: {s1_time:.2f}s")
            if TFOPS_PROFILING:
                print(f"ğŸ”¥ FLOPs - S1: {s1_tflops:.2f} TFLOPs")
            print(f"{'âœ… Success' if success else 'âŒğŸ¯ Fail'}")

            #! ==================================================================
            #! [Statistics & Logging]
            #! ==================================================================

            # data_sourceë³„ í†µê³„ ì—…ë°ì´íŠ¸
            if data_source not in data_source_stats:
                data_source_stats[data_source] = {
                    'num_action': 0,
                    'time_sum': 0.0,
                    'tflops_sum': 0.0,
                    'success_count': 0
                }
            
            stats = data_source_stats[data_source]
            stats['num_action'] += 1
            stats['time_sum'] += s1_time
            if TFOPS_PROFILING:
                stats['tflops_sum'] += s1_tflops
            if success:
                stats['success_count'] += 1

            up2now_s1_score = stats['success_count'] / stats['num_action'] * 100
            # print(f"Up2Now Crop Accuracy: {up2now_crop_score:.2f}%")
            print(f"Up2Now Accuracy: {up2now_s1_score:.2f}%")

            # Iter log - ê°œì„ ëœ ë¡œê¹…
            append_iter_log(
                idx=j+1,
                orig_w=original_image.size[0],
                orig_h=original_image.size[1],
                s1_time=f"{s1_time:.3f}",
                s1_tflops=f"{s1_tflops:.2f}",
                s1_hit=s1_hit,
                s1_acc_uptonow=f"{up2now_s1_score:.2f}",
                filename=filename_wo_ext,
                instruction=instruction[:50] + "..." if len(instruction) > 50 else instruction
            )

            # JSON ê¸°ë¡ - í•µì‹¬ ì •ë³´ë§Œ
            item_res = {
                'filename': filename,
                'orig_w': original_image.size[0],
                'orig_h': original_image.size[1],
                'instruction': instruction,
                'gt_bbox': original_bbox,
                'data_source': data_source,
                'stage1_success': success,
                's1_hit': s1_hit,
                'original_point': original_point,
                's1_time': s1_time,
                'total_time': total_time,
                'total_tflops': s1_tflops
            }
            task_res.append(item_res)

        #! ==================================================
        # ê²°ê³¼ Json ì •ë¦¬
        os.makedirs(os.path.join(save_dir, "json"), exist_ok=True)
        with open(os.path.join(save_dir, "json", dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        # ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            "task": task,
            "total_samples": num_action,
            "stage1_accuracy": success_count / num_action * 100,
            "avg_times": {
                "stage1": time_sum / num_action,
            },
            "avg_flops_tflops": {
                "stage1": tflops_sum / num_action,
            },
            "hyperparameters": {
                "region_threshold": REGION_THRESHOLD,
                "bbox_padding": BBOX_PADDING,
                "min_patches": MIN_PATCHES,
                "attn_impl": ATTN_IMPL
            }
        }

        with open(os.path.join(save_dir, f"results_{task}.json"), "w") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=4)

        # ì „ì²´ ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— í•œ ì¤„ ì¶”ê°€
        results_csv_path = "../_results"
        os.makedirs(results_csv_path, exist_ok=True)
        csv_file_path = os.path.join(results_csv_path, f"results_{task}.csv")
        
        # CSV ë°ì´í„° í–‰ ìƒì„±
        import datetime
        csv_row = [
            method,
            RESIZE_RATIO, REGION_THRESHOLD, BBOX_PADDING,
            num_action, 
            round(metrics['stage1_accuracy'], 2),
            round(metrics['avg_times']['stage1'], 4),
            round(metrics['avg_flops_tflops']['stage1'], 2),
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        ]
        
        # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±, ìˆìœ¼ë©´ ë°ì´í„° í–‰ë§Œ ì¶”ê°€
        import csv
        file_exists = os.path.exists(csv_file_path)
        
        with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ í—¤ë” ì¶”ê°€
            if not file_exists or os.path.getsize(csv_file_path) == 0:
                writer.writerow(csv_headers)
            
            # ë°ì´í„° í–‰ ì¶”ê°€
            writer.writerow(csv_row)
        
        print(f"ğŸ“ Results saved to CSV: {csv_file_path}")

        # ì „ì²´ task í†µê³„ì— ëˆ„ì 
        total_samples += num_action
        total_success += success_count
        total_s1_time += time_sum
        total_s1_tflops += tflops_sum

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("=" * 60)
        print(f"ğŸ“Š Final Results for {task}:")
        print(f"Total Samples: {num_action}")
        print(f"Stage1 Accuracy: {metrics['stage1_accuracy']:.2f}%")
        print(f"Avg Times: {metrics['avg_times']['stage1']:.3f}s")
        print(f"Avg FLOPs: {metrics['avg_flops_tflops']['stage1']:.2f} TFLOPs")
        
        print("=" * 60)

    print("\nğŸ“Š All Task Done!")

    # ì „ì²´ ê²°ê³¼ ê³„ì‚° ë° ì €ì¥
    total_success_rate = total_success / total_samples
    
    # ì „ì²´ í‰ê·  ì‹œê°„
    avg_s1_time = total_s1_time / total_samples
    
    # ì „ì²´ í‰ê·  TFLOPS
    avg_s1_tflops = total_s1_tflops / total_samples
    
    print(f"Total Sample num: {total_samples}")
    print(f"Total Stage1 Success Rate: {total_success_rate:.4f}")
    print(f"Total avg Stage1 time: {avg_s1_time:.4f}s")
    print(f"Total avg Stage1 TFLOPS: {avg_s1_tflops:.4f}")
    
    # ì „ì²´ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    cumulative_csv_path = os.path.join("../_results", "results_all.csv")
    
    # ì „ì²´ ê²°ê³¼ CSV í–‰ ìƒì„±
    cumulative_csv_row = [
        method,
        RESIZE_RATIO, REGION_THRESHOLD, BBOX_PADDING,
        total_samples,
        round(total_success_rate * 100, 2),
        round(avg_s1_time, 4),
        round(avg_s1_tflops, 2),
        datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    ]
    
    # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±, ìˆìœ¼ë©´ ë°ì´í„° í–‰ë§Œ ì¶”ê°€
    file_exists = os.path.exists(cumulative_csv_path)
    
    with open(cumulative_csv_path, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ í—¤ë” ì¶”ê°€
        if not file_exists or os.path.getsize(cumulative_csv_path) == 0:
            writer.writerow(csv_headers)
        
        # ì „ì²´ ê²°ê³¼ í–‰ ì¶”ê°€
        writer.writerow(cumulative_csv_row)

    print(f"ğŸ“ Total Results : {cumulative_csv_path}")