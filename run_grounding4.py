# Standard Library
import json
import os
import re
import shutil
import sys
import time
from copy import deepcopy
from pathlib import Path
from typing import Dict, List

# Third-Party Libraries
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.patheffects as pe
from matplotlib.lines import Line2D
import numpy as np
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoTokenizer, set_seed

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from qwen_vl_utils import process_vision_info
from crop5 import run_segmentation_recursive  #! ì–´ë–¤ crop íŒŒì¼ ì‚¬ìš©?

#! Argument =======================

SEED = 0

# Enviroment
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= "3"  # ëª‡ë²ˆ GPU ì‚¬ìš©í• ì§€ ("0,1", "2" ë“±)
max_memory = {
    0: "75GiB",
    # 1: "75GiB",
    # 2: "75GiB",
    "cpu": "120GiB",  # ë‚¨ëŠ” ê±´ CPU ì˜¤í”„ë¡œë”©xs
}

# Dataset & Model
MLLM_PATH = "zonghanHZH/ZonUI-3B"
SCREENSPOT_IMGS = "./data/screenspotv2_imgs"  # input image ê²½ë¡œ
SCREENSPOT_TEST = "./data"  # jsoníŒŒì¼ ê²½ë¡œ
SAVE_DIR = "./attn_output/" + "0821_all"  #! ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ë°©ë²•ì„ ë°”ê¾¼ë‹¤ë©´ ë°”ê¿”ì„œ ê¸°ë¡í•˜ê¸°)
SAMPLING = False  # data ì„ì„ì§€
TASKS = ["mobile"]
SAMPLE_RANGE = slice(3,4)  #! ìƒ˜í”Œ ë²”ìœ„ ì§€ì • (3ë²ˆ ìƒ˜í”Œì´ë©´ 3,4 / 5~9ë²ˆ ìƒ˜í”Œì´ë©´ 5,10 / ì „ì²´ ì‚¬ìš©ì´ë©´ None)

#! Hyperparameter =================

# Model Architecture
LAYER_NUM_LIST = [3, 7, 11, 15, 31]
LAYER_NUM = 31

# Stage 1: Segmentation & Selection
VAR_THRESH = 120  # Segmentation variance threshold
TOP_Q = 0.5  # Top quantile for crop selection

# Stage 2: Attention Refinement  
AGG_START = 20  # Starting layer for attention aggregation

# Image Resize Ratios
S1_RESIZE_RATIO = 0.30  # Stage 1 crop resize ratio
S2_RESIZE_RATIO = 0.50  # Stage 2 crop resize ratio  
THUMBNAIL_RESIZE_RATIO = 0.05  # Thumbnail resize ratio

TOPK_SINKS  = 20             # ê³µí†µ sink ì¢Œí‘œë¡œ ì¡ì„ ê°œìˆ˜
PER_MAP_TOPN_FRAC = 0.05     # ê° ë§µì—ì„œ ìƒìœ„ ëª‡ %ë¥¼ "ìƒìœ„ê°’"ìœ¼ë¡œ ê°„ì£¼í• ì§€ (ë¹ˆë„ìˆ˜ ì§‘ê³„ìš©)
RENORMALIZE = False           # 0ìœ¼ë¡œ ë§Œë“  í›„ vision span ë‚´ì—ì„œ ì¬ì •ê·œí™”í• ì§€ ì—¬ë¶€

#! ================================

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NpEncoder, self).default(obj)

def find_vision_spans(input_ids, vs_id, ve_id):
    ids = input_ids.tolist()
    spans = []
    i = 0
    while True:
        try:
            s = ids.index(vs_id, i) + 1
            e = ids.index(ve_id, s)
            spans.append((s, e))
            i = e + 1
        except ValueError:
            break
    return spans

def calc_acc(score_list):
    return sum(score_list) / len(score_list) if len(score_list) != 0 else 0

def process_inputs(msgs, crop_list):
    text = processor.apply_chat_template(
      msgs, tokenize=False, add_generation_prompt=True
    )

    image_inputs, video_inputs = process_vision_info(msgs)

    inputs = processor(
        text=text,
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )

    inputs = inputs.to(model.device)


    # # 3) grid í¬ê¸° ë½‘ì•„ë‘ê¸°
    img_proc_out = processor.image_processor(images=image_inputs)
    grid = img_proc_out["image_grid_thw"]
    # # (batch, num_imgs, 3) í˜¹ì€ (num_imgs, 3) í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ ë½‘ê¸°
    if grid.ndim == 3:
        grid = grid[0]   # (num_imgs, 3)

    for i, (t,h,w) in enumerate(grid):
        crop_list[i]['patch_shape'] = (t, h//2, w//2)
        

    return inputs, crop_list

def create_msgs(crop_list, question):
    # msgì— ì¶”ê°€
    msgs = [{
        "role": "user",
        "content": [],
    }]

    for crop in crop_list:
        img = crop["resized_img"]
        msgs[0]["content"].append({"type": "image", "image": img})
    msgs[0]["content"].append({"type": "text", "text": question})
    return msgs

def create_stage_crop_list(crop_list: List, resize_dict: Dict, use_thumbnail: bool = True):
    stage_crop_list = []

    for crop in crop_list:
        crop_level = crop.get("level")
        if not use_thumbnail and crop_level == 0: continue

        # í˜„ì¬ cropì˜ levelì— ë§ëŠ” resize ë¹„ìœ¨ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        ratio = resize_dict.get(crop_level)

        # ë§Œì•½ levelì´ resize_dictì— ì—†ë‹¤ë©´(e.g., None, 2), cropì„ ëˆ„ë½ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # ëŒ€ì‹ , level 1ì˜ ë¹„ìœ¨ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if ratio is None:
            ratio = resize_dict.get(1) # level 1 ë¹„ìœ¨ë¡œ ëŒ€ì²´
            if ratio is None: # level 1 ë¹„ìœ¨ì¡°ì°¨ ì—†ë‹¤ë©´ ê²½ê³  í›„ ìŠ¤í‚µ
                print(f"Warning: Skipping crop ID {crop.get('id')} due to missing resize ratio for level {crop_level} and no fallback.")
                continue

        new_crop = deepcopy(crop)
        crop_img = deepcopy(crop["img"])

        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
        crop_width, crop_height = crop_img.size
        crop_img = crop_img.resize((int(crop_width * ratio), int(crop_height * ratio)))

        new_crop["resized_img"] = crop_img

        stage_crop_list.append(new_crop)

    return stage_crop_list

def run_single_forward(inputs, crop_list: List):
    # Add vision span to crop list from inputs
    vision_start_id = processor.tokenizer.convert_tokens_to_ids("<|vision_start|>")
    vision_end_id = processor.tokenizer.convert_tokens_to_ids("<|vision_end|>")

    spans = find_vision_spans(inputs["input_ids"][0], vision_start_id, vision_end_id)

    # cropì´ë‘ span ê°™ì´ ê´€ë¦¬
    for i, span in enumerate(spans):
      crop_list[i]["token_span"] = span

    # inference
    with torch.no_grad():
      output = model(**inputs, output_attentions=True)

    return output, crop_list

def attn2df(attn_output, crop_list: List):
    _data = []

    # cropë§ˆë‹¤ ë°›ì€ ì–´í…ì…˜ ë½‘ì•„ë‚´ê¸°
    for i, crop in enumerate(crop_list):
        (st, end) = crop["token_span"] # cropì˜ í† í° ì‹œì‘, ë index ë½‘ê¸°

        layer_avgs = [] # e.g. [0.0(ë ˆì´ì–´ 0ì—ì„œ ì´ cropì´ ë°›ì€ ì–´í…ì…˜), 0.1 (ë ˆì´ì–´ 1ì—ì„œ cropì´ ë°›ì€ ì–´í…ì…˜), 0.2, ...]
        for li in range(LAYER_NUM):
            # ë ˆì´ì–´ë§ˆë‹¤ í•´ë‹¹ cropì´ ë°›ì€ ì–´í…ì…˜ ë²¡í„° ì¶”ì¶œ
            att_vector = (
                attn_output.attentions[li][0, :, -1, st:end] # starting answer token(í…ìŠ¤íŠ¸ í† í° ë§ˆì§€ë§‰)
                .mean(dim=0)
                .to(torch.float32)
                .cpu()
                .numpy()
            )
            # ì–´í…ì…˜ ì‹±í¬ë¥¼ ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚°
            mean_att_without_sink = att_vector.mean()  # calculate_mean_without_sinks í•¨ìˆ˜ê°€ ì—†ì–´ì„œ ì¼ë‹¨ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
            layer_avgs.append(mean_att_without_sink)

        _data.append(layer_avgs)

    df = pd.DataFrame(
            _data,
            columns=[f"layer{layer_idx+1}" for layer_idx in range(LAYER_NUM_LIST[-1])],
            index=[crop["id"] for crop in crop_list]
            )

    return df

def get_top_q_crop_ids(top_q, attn_df):
    df = deepcopy(attn_df)
     # LAYER_NUMê¹Œì§€ì˜ ì–´í…ì…˜ í•© êµ¬í•˜ê¸°
    cols_to_sum = [f"layer{i}" for i in range(20, LAYER_NUM + 1)]  #! 
    df[f"sum_until_{LAYER_NUM}"] = df[cols_to_sum].sum(axis=1)

    # 1.1. get top-q crop index
    # quantile ì„ê³„ì¹˜ë¡œ ì„ íƒ
    keep_frac = top_q  # ì˜ˆ: 0.2 â†’ ìƒìœ„ 20% ìœ ì§€
    thr = df[f"sum_until_{LAYER_NUM}"].quantile(1 - keep_frac)

    top_q_crop_ids = df.index[df[f"sum_until_{LAYER_NUM}"] >= thr].tolist()

    # # guardrail: ìµœì†Œ/ìµœëŒ€ ê°œìˆ˜ ë³´ì •(ì„ íƒ)
    if len(top_q_crop_ids) < 1:
        raise Exception("no crop selected")
    return top_q_crop_ids

def check_gt_in_top_q_crops(top_q_bboxes: List, gt_bbox: List):
    gt_center_x = (gt_bbox[0] + gt_bbox[2]) / 2.0
    gt_center_y = (gt_bbox[1] + gt_bbox[3]) / 2.0
    gt_center_point = (gt_center_x, gt_center_y)
    gt_point_in_top_q_crops = any(point_in_bbox(gt_center_point, bbox) for bbox in top_q_bboxes)
    return gt_point_in_top_q_crops

def run_selection_pass(msgs, crop_list, top_q, drop_indices: List, attn_vis_dir: str):
    """
    [Stage 1ìš©] Attention ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•˜ê³ , threshold(top_q)ë¥¼ ë„˜ëŠ” ìƒìœ„ cropë“¤ì„ ì„ íƒ(selection)í•©ë‹ˆë‹¤.
    """
    s1_inputs, crop_list = process_inputs(msgs=msgs, crop_list=crop_list)
    attn_output, crop_list = run_single_forward(inputs=s1_inputs, crop_list=crop_list)
    df = attn2df(attn_output=attn_output, crop_list=crop_list)

    # Stage 1 ì§„ë‹¨ìš© ì‹œê°í™” (ê°œë³„ cropì˜ low-res attention)
    # TODO: ì¶”ë¡ ë§Œ í• ë•ŒëŠ” ë¹¼ê¸°
    visualize_attn_map(attn_output=attn_output, msgs=msgs, crop_list=crop_list, attn_vis_dir=attn_vis_dir)

    raw_attn_res = df.to_dict(orient="index")

    for i in drop_indices:
        df = df.drop(index=i, errors='ignore')

    top_q_crop_ids = get_top_q_crop_ids(top_q=top_q, attn_df=df)
    # print(f"instruction: {instruction}, selected crops: {top_q_crop_ids}")

    top_q_bboxes = []
    for crop in crop_list:
        if crop.get("id") not in drop_indices and crop.get("id") in top_q_crop_ids:
            top_q_bboxes.append(crop.get("bbox"))

    return top_q_crop_ids, top_q_bboxes, raw_attn_res, crop_list

def run_refinement_pass(crop_list: List, question: str, original_image: Image, save_dir: str, gt_bbox: List):
    """
    [Stage 2ìš©] ì„ íƒëœ crop ë¦¬ìŠ¤íŠ¸ ì „ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… Attention Mapì„ ìƒì„±í•˜ê³ ,
    Grounding Accuracyë¥¼ ê³„ì‚°í•˜ì—¬ ì„±ê³µ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("Running second forward pass for refinement...")
    os.makedirs(save_dir, exist_ok=True)

    # 1. Stage 2ì— ë§ëŠ” í¬ê¸°ë¡œ ì´ë¯¸ì§€ë“¤ì„ ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  ëª¨ë¸ ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    # for crop in crop_list:
    #     print(f"id: {crop['id']}, level{crop['level']}")

    s2_resized_crop_list = create_stage_crop_list(
        crop_list=crop_list,
        resize_dict={0: THUMBNAIL_RESIZE_RATIO, 1: S2_RESIZE_RATIO},
        use_thumbnail=True
    )

    os.makedirs(f"{save_dir}/s2_photo", exist_ok=True)
    for _crop in s2_resized_crop_list:
        _crop['resized_img'].save(f"{save_dir}/s2_photo/crop{_crop['id']}.png")


    msgs_s2 = create_msgs(crop_list=s2_resized_crop_list, question=question)

    # 2. ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ attention ê²°ê³¼ë§Œ ì–»ìŠµë‹ˆë‹¤.
    s2_inputs, s2_crop_list_w_shape = process_inputs(msgs=msgs_s2, crop_list=s2_resized_crop_list)
    s2_attn_output, s2_final_crop_list = run_single_forward(
        inputs=s2_inputs, 
        crop_list=s2_crop_list_w_shape)

    # 3. ëª¨ë“  cropì˜ attentionì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë§µì„ ìƒì„±í•˜ê³  Grounding ì„±ê³µ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    print("Generating final aggregated attention map and checking grounding accuracy...")
    
    # ì €ì¥ íŒŒì¼ëª…ì„ "result.png"ë¡œ ë³€ê²½
    s2_individual_maps_dir = os.path.join(save_dir, "individual_maps_refined")

    # (0) ì‚¬ìš©í•  query text token ì •í•˜ê¸°
    input_ids_1d = s2_inputs["input_ids"][0]
    last_vision_end = get_last_vision_end_index(input_ids_1d, processor)
    seq_len = input_ids_1d.shape[0]
    if last_vision_end < 0 or last_vision_end >= seq_len - 1:
        query_indices = [seq_len - 1]
    else:
        query_indices = list(range(last_vision_end + 1, seq_len))

    exclude = set()

    # (a) ë§ˆì§€ë§‰ í…ìŠ¤íŠ¸ í† í°(ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ë§ˆì§€ë§‰ í† í°) ì œì™¸
    if seq_len > 0:
        exclude.add(seq_len - 1)

    # (b) ë§ˆì§€ë§‰ <|im_start|> ì œì™¸
    im_start_id = processor.tokenizer.convert_tokens_to_ids("<|im_start|>")
    im_pos = (input_ids_1d == im_start_id).nonzero(as_tuple=False).flatten().tolist()
    if len(im_pos) > 0:
        exclude.add(int(im_pos[-1]))

    # (c) '?' í† í° ì œì™¸ (í† í° ë¬¸ìì—´ ì •ê·œí™” í›„ ë¹„êµ)
    cand_ids = input_ids_1d[query_indices].tolist()
    cand_tokens = processor.tokenizer.convert_ids_to_tokens(cand_ids)

    def is_qmark_token(tok: str) -> bool:
        # GPT2ë¥˜ 'Ä ', sentencepieceë¥˜ 'â–' ì œê±° + trim
        t = tok.replace("Ä ", "").replace("â–", "").strip()
        return t in {"?", "ï¼Ÿ"}

    for idx, tok in zip(query_indices, cand_tokens):
        if is_qmark_token(tok):
            exclude.add(int(idx))

    # ìµœì¢… í•„í„°ë§
    query_indices = [i for i in query_indices if i not in exclude]

    # (1) ëª¨ë“  ë ˆì´ì–´Ã—ëª¨ë“  í…ìŠ¤íŠ¸ ì§ˆì˜ í† í°ì— ëŒ€í•œ ë§µ ìˆ˜ì§‘ & ë¹ˆë„ ëˆ„ì 
    s2_final_crop_list = collect_maps_and_sink_counts(
        s2_attn_output, 
        s2_inputs, 
        s2_final_crop_list,
        query_indices=query_indices,
        # grounding_query=exclude,
        layer_range=range(AGG_START, LAYER_NUM), 
        processor=processor, 
        per_map_topn_frac=PER_MAP_TOPN_FRAC,
    )

    # (2) ì´ë¯¸ì§€ë³„ ê³µí†µ sink ì¢Œí‘œ Top-K ì¶”ì¶œ
    for i, crop in enumerate(s2_final_crop_list):
        t, h2, w2 = crop['patch_shape']
        sink_freqs = crop['sink_freqs']
        # sink_freqs = per_img_sink_freqs[img_idx]

        sinks = pick_common_sinks(sink_freqs, h2, w2, k=TOPK_SINKS)
        crop['sink_coords'] = sinks
        # sink_coords_per_img.append(sinks)

    # (3) ë§ˆìŠ¤í‚¹ í›„ ë§µ ì¬ê³„ì‚°
    s2_final_crop_list = zero_out_sinks_and_remap(
        model_output=s2_attn_output,
        crop_list=s2_final_crop_list,
        layer_range=range(AGG_START, LAYER_NUM),
        query_indices=exclude,
        renormalize=RENORMALIZE
    )

    for i, crop in enumerate(s2_final_crop_list):
    # ì›ë³¸ í‰ê· ë§µ
        # if len(per_img_maps[img_idx]) > 0:
        if len(crop["att_maps"]) > 0:
            att_avg_orig = np.mean(crop["att_maps"], axis=0)
            crop["att_avg_orig"] = att_avg_orig
        else:
            # t, h2, w2 = final_shapes[img_idx]
            t, h2, w2 = crop["patch_shape"]
            att_avg_orig = np.zeros((h2, w2), dtype=np.float32)
            crop["att_avg_orig"] = att_avg_orig

        # ë§ˆìŠ¤í‚¹ í‰ê· ë§µ
        # if len(per_img_maps_masked[img_idx]) > 0:
        if len(crop["masked_att_maps"]) > 0:
            att_avg_masked = np.mean(crop["masked_att_maps"], axis=0)
            crop["att_avg_masked"] = att_avg_masked
        else:
            t, h2, w2 = crop["patch_shape"]
            att_avg_masked = np.zeros((h2, w2), dtype=np.float32)
            crop["att_avg_masked"] = att_avg_masked

    is_success = visualize_aggregated_attention(
        # attn_output=s2_attn_output, 
        crop_list=s2_final_crop_list,
        original_image=original_image,
        inst_dir=inst_dir,
        gt_bbox=gt_bbox,
        individual_maps_dir=s2_individual_maps_dir
    )
    return is_success

def visualize_crop(save_dir, gt_bbox, top_q_bboxes, instruction, filename, click_point=None):
    # Visualize ground truth and selected crop on the image
    result_img = Image.open(img_path)

    draw = ImageDraw.Draw(result_img)
    # Draw ground truth bbox in green
    draw.rectangle(gt_bbox, outline="green", width=2)

    font = None
    try:
        font = ImageFont.truetype("DejaVuSans-Bold.ttf", 20)
    except IOError:
        font = ImageFont.load_default()

    for bbox in top_q_bboxes:
        draw.rectangle(bbox, outline="red", width=2)
        text_to_draw = f"{instruction}"
        crop_left, crop_top, crop_right, crop_bottom = bbox
        inst_position = (crop_left, crop_top)
        draw.text(inst_position, text_to_draw, fill="red", font=font)


    # Draw click point as an orange circle
    if click_point is not None:
        click_x, click_y = click_point[0], click_point[1]
        radius = 5
        draw.ellipse((click_x - radius, click_y - radius, click_x + radius, click_y + radius), outline="orange", width=3)

    # Ensure the save directory exists
    os.makedirs(save_dir, exist_ok=True)
    # Save the result image
    result_path = os.path.join(save_dir, filename)
    # print(f"Top Q box is saved at : {result_path}")
    result_img.save(result_path)

def visualize_attn_map(attn_output, msgs, crop_list, attn_vis_dir):
    image_inputs, _ = process_vision_info(msgs)

    # grid í¬ê¸° ë½‘ì•„ë‘ê¸°
    img_proc_out = processor.image_processor(images=image_inputs)
    grid = img_proc_out["image_grid_thw"]
    # (batch, num_imgs, 3) í˜¹ì€ (num_imgs, 3) í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ ë½‘ê¸°
    if grid.ndim == 3:
        grid = grid[0]   # (num_imgs, 3)

    # ìµœì¢… token-map ì°¨ì›: t Ã— (h//2) Ã— (w//2)
    final_shapes = [
        (t, h//2, w//2)
        for t, h, w in grid
    ]

    num_imgs = len(crop_list)
    fig, axes = plt.subplots(1, num_imgs, figsize=(5*num_imgs, 5))

    for i, crop in enumerate(crop_list):
        (st, end) = crop["token_span"] # cropì˜ í† í° ì‹œì‘, ë index ë½‘ê¸°
        t, h2, w2 = final_shapes[i]
        att_maps = []

        # for li in range(L):
        for li in range(20, 32):
            att = (
                attn_output.attentions[li]         # (batch, heads, seq_q, seq_k)
                [0, :, -1, st:end]              # batch=0, ë§ˆì§€ë§‰ query í† í°, vision span
                .mean(dim=0)                 # head í‰ê· 
                .to(torch.float32)           # bfloat16 â†’ float32
                .cpu()
                .numpy()
            )
            att_map = att.reshape(t, h2, w2).mean(axis=0)  # ì‹œê°„ì¶• í‰ê· 
            att_maps.append(att_map)

        att_avg = np.mean(att_maps, axis=0)  # 32ê°œ ë ˆì´ì–´ í‰ê· 

        ax = axes[i] if num_imgs > 1 else axes
        im = ax.imshow(att_avg, cmap="viridis", interpolation="nearest")
        ax.set_title(f"crop{crop['id']}")
        ax.axis("off")

    plt.tight_layout()

    out_dir = Path(attn_vis_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    _save_path = os.path.join(out_dir, "attn_map.png")

    fig.savefig(_save_path, dpi=300, bbox_inches="tight", facecolor="white")
    # print(f"attn_map saved at: {_save_path}")

def get_last_vision_end_index(input_ids_1d, processor):
    """ì‹œí€€ìŠ¤ì—ì„œ ë§ˆì§€ë§‰ <|vision_end|> í† í° ì¸ë±ìŠ¤ ë°˜í™˜ (ì—†ìœ¼ë©´ -1)."""
    vision_end_id = processor.tokenizer.convert_tokens_to_ids("<|vision_end|>")
    ve_positions = (input_ids_1d == vision_end_id).nonzero(as_tuple=False).flatten()
    return int(ve_positions[-1].item()) if len(ve_positions) > 0 else -1

def top_indices(flat_values, topn):
    """1D ë°°ì—´ì—ì„œ ìƒìœ„ topn ì¸ë±ìŠ¤(ì •ë ¬ëœ ë‚´ë¦¼ì°¨ìˆœ) ë°˜í™˜."""
    topn = int(min(topn, flat_values.size))
    if topn <= 0:
        return np.array([], dtype=np.int64)
    # argpartitionìœ¼ë¡œ ë¹ ë¥´ê²Œ ìƒìœ„ ì§‘í•© ë½‘ê³ , ê°’ ê¸°ì¤€ ì •ë ¬
    part = np.argpartition(flat_values, -topn)[-topn:]
    return part[np.argsort(flat_values[part])[::-1]]

def pick_common_sinks(counts, h2, w2, k=10):
    """
    ë¹ˆë„ ëˆ„ì (counts: shape=(h2*w2,))ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë†’ì€ ì¢Œí‘œ ìƒìœ„ kê°œ ì„ íƒ.
    ë°˜í™˜: [(r,c), ...], ê¸¸ì´<=k (countsê°€ ì ìœ¼ë©´ ë” ì ì„ ìˆ˜ ìˆìŒ)
    """
    hw = h2 * w2 # ì´ë¯¸ì§€ íŒ¨ì¹˜ ê°œìˆ˜
    k = min(k, hw)
    if k <= 0:
        return []
    top_flat = top_indices(counts.astype(np.float32), k)
    coords = [(int(idx // w2), int(idx % w2)) for idx in top_flat]
    return coords

def zero_sink_in_att_vec(att_vec, t, h2, w2, sink_coords, renormalize=False, eps=1e-12):
    """
    vision span ë‚´ ì–´í…ì…˜ ë²¡í„°(att_vec: shape=(t*h2*w2,))ì—ì„œ
    sink ì¢Œí‘œë“¤ì˜ ëª¨ë“  ì‹œê°„ ìœ„ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •. (ì„ íƒì  ì¬ì •ê·œí™”)
    """
    vec = att_vec.copy()
    hw = h2 * w2 # ì´ë¯¸ì§€ íŒ¨ì¹˜ ê°œìˆ˜
    base_idxs = [row * w2 + col for (row, col) in sink_coords]
    if len(base_idxs) == 0:
        return vec
    # ì‹œê°„ ì¶•ìœ¼ë¡œ stride=hw
    for base in base_idxs:
        vec[base: hw * t: hw] = 0.0
    if renormalize:
        s = vec.sum()
        if s > eps:
            vec = vec / s
    return vec

def zero_out_sinks_and_remap(
        model_output, 
        # spans, 
        # final_shapes, 
        # sink_coords_per_img,
        crop_list, 
        layer_range, 
        query_indices, 
        renormalize=False
    ):
    """
    ì •í•´ì§„ sink ì¢Œí‘œë“¤ì„ 0ìœ¼ë¡œ ë§Œë“  ë’¤, ë‹¤ì‹œ (h2,w2) ë§µë“¤ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜.
    Returns:
        per_img_maps_masked: ì´ë¯¸ì§€ë³„ë¡œ (ë§ˆìŠ¤í‚¹ í›„) (h2,w2) ë§µ ë¦¬ìŠ¤íŠ¸
    """
    # per_img_maps_masked = []
    # for img_idx, (st, end) in enumerate(spans):
    for i, crop in enumerate(crop_list):
        # t, h2, w2 = final_shapes[img_idx]
        t, h2, w2 = crop['patch_shape']
        # sink_coords = sink_coords_per_img[img_idx]
        sink_coords = crop['sink_coords']
        crop_span = crop['token_span']
        st, end = crop_span

        img_maps = []

        for li in layer_range:
            if li < 0 or li >= len(model_output.attentions):
                continue
            layer_att = model_output.attentions[li]  # (B,H,Q,K)
            for q_idx in query_indices:
                att_vec = (
                    layer_att[0, :, q_idx, st:end]
                    .mean(dim=0)
                    .to(torch.float32)
                    .detach()
                    .cpu()
                    .numpy()
                )  # (t*h2*w2,)
                att_vec_masked = zero_sink_in_att_vec(att_vec, t, h2, w2, sink_coords, renormalize=renormalize)
                att_map_masked = att_vec_masked.reshape(t, h2, w2).mean(axis=0)
                img_maps.append(att_map_masked)
        crop['masked_att_maps'] = img_maps
        # per_img_maps_masked.append(img_maps)
    return crop_list

def collect_maps_and_sink_counts(
        model_output,
        inputs, 
        # spans, 
        crop_list,
        # final_shapes, 
        layer_range, 
        # vision_end_id,
        query_indices,
        # grounding_query,
        processor, 
        per_map_topn_frac=0.01,
    ):
    """
    ëª¨ë“  ì´ë¯¸ì§€ span, ëª¨ë“  ë ˆì´ì–´, ëª¨ë“  (ë§ˆì§€ë§‰ VE ì´í›„) í…ìŠ¤íŠ¸ í† í° ì§ˆì˜ì— ëŒ€í•´
    (h2, w2) ì–´í…ì…˜ ë§µì„ ìˆ˜ì§‘í•˜ê³ , ê° ë§µì—ì„œ ìƒìœ„ê°’ ë¹ˆë„ìˆ˜ë¥¼ ëˆ„ì (counts)í•©ë‹ˆë‹¤.
    
    Returns:
        per_img_maps:  ë¦¬ìŠ¤íŠ¸ ê¸¸ì´=num_imgs, ê° ì›ì†ŒëŠ” (ë§µ ë¦¬ìŠ¤íŠ¸). ê° ë§µ shape=(h2, w2)
        per_img_counts: íŒ¨ì¹˜ë§ˆë‹¤ ì–´í…ì…˜ ìŠ¤ì½”ì–´ top nì— ë“  ë¹ˆë„ìˆ˜; ë¦¬ìŠ¤íŠ¸ ê¸¸ì´=num_imgs, ê° ì›ì†ŒëŠ” counts ë°°ì—´ shape=(h2*w2,) 
    """
    input_ids_1d = inputs["input_ids"][0]
    seq_len = input_ids_1d.shape[0]
    vision_end_id = processor.tokenizer.convert_tokens_to_ids("<|vision_end|>")
    last_vision_end = get_last_vision_end_index(input_ids_1d, processor)

    # ë§ˆì§€ë§‰ <|vision_end|> ì´í›„ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í† í° ì¸ë±ìŠ¤ (ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš©)
    # if last_vision_end < 0 or last_vision_end >= seq_len - 1:
    #     query_indices = [seq_len - 1]
    # else:
    #     # ë„ˆë¬´ ê¸¸ë©´ ë©”ëª¨ë¦¬ í­ë°œ ë°©ì§€: í•„ìš”í•˜ë©´ ìŠ¬ë¼ì´ìŠ¤/ìƒ˜í”Œë§ ê°€ëŠ¥
    #     query_indices = list(range(last_vision_end + 1, seq_len))

    per_img_maps   = []  # ê° ì´ë¯¸ì§€ë³„ë¡œ [(h2,w2)ë§µ, ...]
    per_img_sink_freqs = []  # ê° ì´ë¯¸ì§€ë³„ë¡œ (h2*w2,) ì¹´ìš´íŠ¸

    for i, crop in enumerate(crop_list):
        crop_span = crop['token_span']
        st, end = crop_span
        # cropì—ì„œ shapeë„ ë½‘ê¸°
        crop_patch_shape = crop['patch_shape']

        t, h2, w2 = crop_patch_shape
        hw = h2 * w2
        span_len = (end - st)
        assert span_len == t * hw, f"Span/token shape mismatch: span={span_len}, t*h*w={t*hw}"

        all_maps = []
        sink_freqs = np.zeros(hw, dtype=np.int64) # ì´ë¯¸ì§€ ë‹¹ ì´ë¯¸ì§€ íŒ¨ì¹˜ ê°œìˆ˜

        # per-map ìƒìœ„ ì§‘ê³„ ê¸°ì¤€ ê°œìˆ˜ (ìµœì†Œ 5ê°œëŠ” ì§‘ê³„)
        per_map_topn = max(5, int(per_map_topn_frac * hw))

        for li in layer_range:
            # ì•ˆì „ì¥ì¹˜: ë ˆì´ì–´ ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
            if li < 0 or li >= len(model_output.attentions):
                continue
            layer_att = model_output.attentions[li]  # (Batch, Head, Query, Key)

            for q_idx in query_indices:
            # for q_idx in grounding_query:
                # H í‰ê· , batch=0, ì§ˆì˜ q_idx, keyëŠ” vision span s:e
                att_vec = (
                    layer_att[0, :, q_idx, st:end]
                    .mean(dim=0)
                    .to(torch.float32)
                    .detach()
                    .cpu()
                    .numpy()
                )  
                
                # shape: (t*h*w,)
                # ì‹œê°„ í‰ê· í•˜ì—¬ (h2,w2) ë§µ
                att_map = att_vec.reshape(t, h2, w2).mean(axis=0)  # (h2,w2)
                all_maps.append(att_map)

                # ìƒìœ„ê°’ ë¹ˆë„ ëˆ„ì 
                flat = att_map.reshape(-1)
                idxs = top_indices(flat, per_map_topn)
                sink_freqs[idxs] += 1

        crop['att_maps'] = all_maps
        crop['sink_freqs'] = sink_freqs
        # per_img_maps.append(all_maps)
        # per_img_sink_freqs.append(sink_freqs)
    # return per_img_maps, per_img_sink_freqs
    return crop_list

def upsample_att_map(att_map_low_res: np.ndarray, size):
    """
    Pillowë¥¼ ì´ìš©í•œ bilinear ì—…ìƒ˜í”Œ (size=(H, W))
    ì…ë ¥/ì¶œë ¥ ëª¨ë‘ float32 ìœ ì§€
    """
    h, w = size
    # ì•ˆì „ì¥ì¹˜: ìŒìˆ˜/NaN ì œê±°
    m = np.nan_to_num(att_map_low_res.astype(np.float32), nan=0.0, neginf=0.0, posinf=0.0)
    if m.size == 0:
        return np.zeros((h, w), dtype=np.float32)
    # ê°’ ë²”ìœ„ë¥¼ ì¼ë‹¨ 0 ì´ìƒìœ¼ë¡œ í´ë¨í”„
    m[m < 0] = 0.0
    im = Image.fromarray(m)
    im = im.resize((w, h), resample=Image.BILINEAR)
    out = np.array(im).astype(np.float32)
    # scaleì— ë”°ë¼ ê°’ì´ ì•½ê°„ ë³€í•  ìˆ˜ ìˆì–´ 0 ì´ìƒìœ¼ë¡œ ì¬í´ë¨í”„
    out[out < 0] = 0.0
    return out

def point_in_bbox(point, bbox):
    """
    point: (x, y)
    bbox: (left, top, right, bottom)
    ê²½ê³„ í¬í•¨
    """
    x, y = point
    l, t, r, b = bbox
    return (l <= x <= r) and (t <= y <= b)

def boxfilter_sum(arr: np.ndarray, r: int):
    """
    ëë¶€ë¶„ ë³´ì • ì—†ìŒ(neighbor-sum).
    (2r+1)x(2r+1) ì°½ê³¼ ì‹¤ì œ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì˜ 'í•©'ë§Œ ê³„ì‚°.
    í‰ê·  ì•„ë‹˜. ê°€ì¥ìë¦¬ëŠ” ì°½ì´ ëœ ê²¹ì¹˜ë¯€ë¡œ ì†í•´ë³´ê²Œ ë¨.
    """
    if r <= 0:
        return arr.astype(np.float32, copy=True)

    a = arr.astype(np.float32, copy=False)
    H, W = a.shape

    # ì ë¶„ì˜ìƒ: ìƒë‹¨/ì¢Œì¸¡ 0 íŒ¨ë”©ì„ í•œ ì¹¸ ì¶”ê°€í•´ì„œ ë²¡í„°í™” ê³„ì‚° ìš©ì´í•˜ê²Œ êµ¬ì„±
    ii = np.pad(a, ((1, 0), (1, 0)), mode='constant').cumsum(axis=0).cumsum(axis=1)

    ys = np.arange(H)[:, None]   # Hx1
    xs = np.arange(W)[None, :]   # 1xW

    y0 = np.clip(ys - r, 0, H)
    y1 = np.clip(ys + r + 1, 0, H)
    x0 = np.clip(xs - r, 0, W)
    x1 = np.clip(xs + r + 1, 0, W)

    # ì ë¶„ì˜ìƒ ì¸ë±ìŠ¤ëŠ” +1 íŒ¨ë”© ê³ ë ¤í•´ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
    S = ii[y1, x1] - ii[y0, x1] - ii[y1, x0] + ii[y0, x0]
    return S

#! Stage 2 ë§ì¶”ê¸° ë©”ì¸ í•¨ìˆ˜ (neighsum ë°©ì‹)
def visualize_aggregated_attention(
        crop_list,
        original_image, inst_dir, gt_bbox, individual_maps_dir=None,

        neigh_radius = 20,         #! ì´ì›ƒí•©(box filter) ë°˜ê²½ r â†’ (2r+1)^2 ì°½

        topk_points = 5,           # ìƒìœ„ ì  ê°œìˆ˜ (ë³´ì—¬ì£¼ê¸°ìš©)
        min_dist_pix = 200,        # ìƒìœ„ ì  ì‚¬ì´ ìµœì†Œ ê°„ê²© (í”½ì…€)

        use_levels=(0, 1, 2),     # í•©ì¹  crop level
        star_marker_size= 8,      # ë³„ í¬ê¸° (1ë“±)
        dot_marker_size = 5,      # ì  í¬ê¸° (2~5ë“±)
        text_fontsize= 7          # ì ìˆ˜ í…ìŠ¤íŠ¸ í°íŠ¸ í¬ê¸°
    ):
    """
    ì´ì›ƒí•© ê¸°ë°˜ ìµœëŒ€ì  íƒìƒ‰:
    - í•©ì„± ë§µ ì •ê·œí™” í›„ boxfilter_sum(neigh_radius)ë¡œ ì´ì›ƒí•© ê³„ì‚°
    - greedy ë¹„ìµœëŒ€ ì–µì œ(NMS)ë¡œ ìƒìœ„ topk_points ì¢Œí‘œ ì„ íƒ(ê°„ê²© min_dist_pix)
    - ì‹œê°í™”:
        â€¢ s2_result_only: ì›ë³¸+í•©ì„±ë§µ+GT ë°•ìŠ¤ë§Œ
        â€¢ s2_result_star: top-1ì€ ë³„(*), top-k ëª¨ë‘ëŠ” ì´ì›ƒí•© ì ìˆ˜ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
    - ì„±ê³µ íŒì •ì€ top-1 ì ì´ GT ë°•ìŠ¤ ì•ˆì´ë©´ True
    """

    os.makedirs(os.path.dirname(inst_dir + "/stage2"), exist_ok=True)
    if individual_maps_dir:
        os.makedirs(individual_maps_dir, exist_ok=True)

    # ìº”ë²„ìŠ¤ ë° í•©ì„± ë§µ ì¤€ë¹„
    W, H = original_image.size
    aggregated_attention_map = np.zeros((H, W), dtype=np.float32)

    # ê° cropì˜ ë§µì„ ì›ë³¸ ì¢Œí‘œê³„ë¡œ ì—…ìƒ˜í”Œí•˜ì—¬ í•©ì„±
    for crop in crop_list:
        if 'bbox' not in crop or 'att_avg_masked' not in crop:
            continue
        if ('level' in crop) and (crop['level'] not in use_levels):
            continue

        left, top, right, bottom = map(int, crop['bbox'])
        cw = max(0, right - left)
        ch = max(0, bottom - top)
        if cw == 0 or ch == 0:
            continue

        att_low = crop['att_avg_masked']
        att_up = upsample_att_map(att_low, size=(ch, cw))  # íŒŒì¼ ìƒë‹¨ ì •ì˜ ê°€ì •

        # ê°œë³„ ë§µ ì €ì¥(ì˜µì…˜)
        if individual_maps_dir:
            indiv = np.zeros((H, W), dtype=np.float32)
            indiv[top:bottom, left:right] = att_up
            plt.figure(figsize=(10, 10 * H / W))
            plt.imshow(original_image, extent=(0, W, H, 0))
            plt.imshow(indiv, cmap='viridis', alpha=0.6, extent=(0, W, H, 0))
            plt.axis('off')
            ttl = f"Crop ID: {crop.get('id','?')}, Level: {crop.get('level','?')}"
            plt.title(ttl)
            path = os.path.join(individual_maps_dir, f"individual_attn_crop_{crop.get('id','unk')}.png")
            plt.savefig(path, dpi=150, bbox_inches='tight', pad_inches=0)
            plt.close()

        aggregated_attention_map[top:bottom, left:right] += att_up

    # ì´ì›ƒí•© ê¸°ë°˜ ìƒìœ„ ì  ì„ ì •
    top_points = []
    scores = []  # boxfilter_sumìœ¼ë¡œ ì–»ì€ ì´ì›ƒí•© ê°’

    if aggregated_attention_map.max() > 0:
        normalized = aggregated_attention_map / (aggregated_attention_map.max() + 1e-8)
        smoothed = boxfilter_sum(normalized, neigh_radius)  # íŒŒì¼ ìƒë‹¨ ì •ì˜ ê°€ì •

        # greedy NMSë¡œ ìƒìœ„ Kê°œ ì  ì„ íƒ
        sm = smoothed.copy()
        Hh, Ww = sm.shape

        for _ in range(int(topk_points)):
            idx = int(np.argmax(sm))
            vy, vx = divmod(idx, Ww)
            best_val = sm[vy, vx]
            if not np.isfinite(best_val) or best_val <= 0:
                break
            # ì  ê¸°ë¡
            top_points.append((int(vx), int(vy)))
            scores.append(float(best_val))
            # ì •ì‚¬ê°í˜• ì–µì œ
            y1 = max(0, vy - min_dist_pix); y2 = min(Hh - 1, vy + min_dist_pix)
            x1 = max(0, vx - min_dist_pix); x2 = min(Ww - 1, vx + min_dist_pix)
            sm[y1:y2+1, x1:x2+1] = -np.inf

    # ì„±ê³µ íŒì •: top-1 ê¸°ì¤€
    is_grounding_success = False
    if len(top_points) > 0:
        cx, cy = top_points[0]
        gl, gt, gr, gb = gt_bbox
        is_grounding_success = (gl <= cx <= gr) and (gt <= cy <= gb)
        print(f"top-1: {(cx, cy)} , GT={gt_bbox}, neigh_sum={scores[0]:.6f}")
    else:
        print("Aggregated attention map empty ë˜ëŠ” peak ì—†ìŒ")

    # ì‹œê°í™”: ê³µí†µ ë°”íƒ•
    fig, ax = plt.subplots(figsize=(10, 10 * H / W))
    ax.imshow(original_image, extent=(0, W, H, 0))
    ax.imshow(aggregated_attention_map, cmap='viridis', alpha=0.6, extent=(0, W, H, 0))

    # ê·¸ëƒ¥ Attention ìƒíƒœë§Œ ì €ì¥ -> ê°€ë¦¬ëŠ”ê±° ì—†ì´ ë³´ì´ë„ë¡.
    plt.savefig(inst_dir + "/s2_result_only.png", dpi=300, bbox_inches="tight", pad_inches=0)

    # GT ë°•ìŠ¤(ì´ˆë¡)
    gl, gt, gr, gb = gt_bbox
    gt_rect = patches.Rectangle((gl, gt), gr - gl, gb - gt, linewidth=3, edgecolor='lime', facecolor='none')
    ax.add_patch(gt_rect)

    # ë²”ë¡€
    green_patch = patches.Patch(color='lime', label='Ground Truth BBox')
    star_legend = Line2D([0], [0], marker='*', color='w', label='NeighSum Top-1', 
                         markerfacecolor='yellow', markeredgecolor='black', markersize=star_marker_size)
    ax.legend([green_patch, star_legend], ['Ground Truth BBox', 'NeighSum Top-1'], loc='best')

    ax.axis('off')
    ax.set_title("Attention (aggregated) + NeighSum Peaks")
    plt.tight_layout()

    # ì‹œê°í™”: top-1 ë³„í‘œ, top-2~5 ê²€ì • ì , top-k í…ìŠ¤íŠ¸ ë¼ë²¨
    if len(top_points) > 0:
        # top-1 ë³„í‘œ
        ax.plot(top_points[0][0], top_points[0][1], 'y*',
                markersize=star_marker_size, markeredgecolor='black')

        # top-2~5 ê²€ì • ì 
        for i in range(1, min(len(top_points), topk_points)):
            px, py = top_points[i]
            ax.plot(px, py, 'o', 
                    markersize=dot_marker_size, markerfacecolor='black', markeredgecolor='white', markeredgewidth=0.9)

        # top-k í…ìŠ¤íŠ¸(ëª¨ë‘ í‘œê¸°: ì ìˆ˜ë§Œ)
        for (px, py), sc in zip(top_points, scores):
            label = f"{sc:.3f}"
            ax.text(px + 10, py - 10, label,
                    fontsize=text_fontsize, color='white', ha='left', va='top',
                    path_effects=[pe.withStroke(linewidth=2, foreground='black')])


    plt.savefig(inst_dir + "/s2_result_star.png", dpi=300, bbox_inches="tight", pad_inches=0)
    plt.close(fig)

    return bool(is_grounding_success)



#! ================================================================================================

if __name__ == '__main__':

    set_seed(SEED)


    # save_dir í´ë”ëª…ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„± (save_dir -> save_dir_1 -> save_dir_2)
    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    # Model Import
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation="eager",
        device_map="balanced", max_memory=max_memory, low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH)
    model.eval()

    # Question
    # question_template="""Where should you tap to {task_prompt}?"""
    question_template="""
You are an assistant trained to navigate the android phone. Given a
task instruction, a screen observation, guess where should you tap.
# Intruction
{task_prompt}"""

    # Process
    for task in TASKS:
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"
        screenspot_data = json.load(open(os.path.join(SCREENSPOT_TEST, dataset), 'r'))
        screenspot_data = screenspot_data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        task_res = list()
        num_action = 0
        corr_action = 0
        res_board_dict = dict()

        for stage_num in range(1, 2+1):
          if stage_num == 1:
            res_board_dict[stage_num] = {
              "is_coarse": True, "gt_score_list": [], "top_q_crop_ids": [],
              "is_gt_in_top_q": False, "raw_attn_dict": None,
            }
          elif stage_num == 2:
            res_board_dict[stage_num] = {
              "is_coarse": True, "gt_score_list": [], "sub_res": [],
            }

        num_wrong_format = 0
        num_segmentation_failed = 0

        for j, item in tqdm(enumerate(screenspot_data)):
            item_res = dict()
            num_action += 1
            filename = item["img_filename"]
            filename_wo_ext, ext = os.path.splitext(filename)
            img_path = os.path.join(SCREENSPOT_IMGS, filename)
            if not os.path.exists(img_path):
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["instruction"]
            original_bbox = item["bbox"]
            original_bbox = [original_bbox[0], original_bbox[1], original_bbox[0] + original_bbox[2], original_bbox[1] + original_bbox[3]]
            question = question_template.format(task_prompt=instruction)

            inst_dir_name = re.sub(r'\W+', '_', instruction).strip('_')

            #* ì´ ë°˜ë³µë¬¸ ë‚´ì—ì„œëŠ” ë³´ê¸° ì‰½ê²Œ inst dir, s1_dir, s2_dirë§Œ ì‚¬ìš©
            inst_dir = os.path.join(save_dir, "seg", filename_wo_ext, inst_dir_name)
            s1_dir = os.path.join(inst_dir, "stage1")
            s2_dir = os.path.join(inst_dir, "stage2")
            os.makedirs(s1_dir, exist_ok=True)
            os.makedirs(s2_dir, exist_ok=True)

            # ==================================================================
            # Stage 1 | Segmentation
            print("\n[Stage 1] Starting Segmentation...")
            stage1_start = time.time()
            crop_list = run_segmentation_recursive(
                image_path=img_path, max_depth=1, window_size=120,
                output_json_path=f"{s1_dir}/output.json",
                output_image_path=s1_dir,
                start_id=0,
                var_thresh=VAR_THRESH
            )

            if crop_list is None:
                print(f"Segmentation failed for {img_path}. Skipping this image.")
                num_segmentation_failed += 1
                continue

            only_lv_1 = [crop for crop in crop_list if crop.get("level") == 1]
            if len(only_lv_1) == 0:
                print(f"No level 1 crops found for {img_path}. Skipping this image.")
                num_segmentation_failed += 1
                continue

            stage_crop_list = create_stage_crop_list(
                crop_list=crop_list,
                resize_dict={0: THUMBNAIL_RESIZE_RATIO, 1: S1_RESIZE_RATIO},
                use_thumbnail=True
            )
            stage1_end = time.time()

            # ==================================================================
            # Stage 1-2 | Find Top Q
            stage1_2_start = time.time()
            msgs = create_msgs(crop_list=stage_crop_list, question=question)

            # Top Q ê³ ë¥´ê¸°
            s1_top_q_crop_ids, s1_top_q_bboxes, s1_raw_res_dict, s1_crop_list_out = run_selection_pass(
                msgs=msgs, crop_list=stage_crop_list, top_q=TOP_Q, drop_indices=[0], attn_vis_dir=s1_dir
            )
            stage1_2_end = time.time()

            # Stage 1 Total Time
            total_time = stage1_2_end - stage1_start
            print(f"Stage 1 Time: {total_time:.2f} sec / Segmentation: {stage1_end - stage1_start:.2f} sec / Find Top Q: {stage1_2_end - stage1_2_start:.2f} sec")

            print(f"Selected Top Q Crops from Stage 1: {s1_top_q_crop_ids}")

            # ==================================================================

            #  Stage 1 | ëª¨ë“  Crop Visualize
            all_crops_bboxes = [crop["bbox"] for crop in stage_crop_list]
            visualize_crop(save_dir=inst_dir, gt_bbox=original_bbox, top_q_bboxes=all_crops_bboxes,
                            instruction=instruction, filename="s1_all_crop.png", click_point=None)
            
            # Stage 1 | top Q Visualize + GTê°€ ì•ˆì— ë“¤ì–´ê°€ëŠ”ì§€ ì²´í¬
            visualize_crop(save_dir=inst_dir, gt_bbox=original_bbox, top_q_bboxes=s1_top_q_bboxes,
                            instruction=instruction, filename="s1_top_q.png", click_point=None)
            s1_is_gt_in_top_q = check_gt_in_top_q_crops(top_q_bboxes=s1_top_q_bboxes, gt_bbox=original_bbox)

            res_board_dict[1]["gt_score_list"].append(1 if s1_is_gt_in_top_q else 0)
            if s1_is_gt_in_top_q:
                print("â˜‘ï¸ Top Q contain Ground Truth")
            else:
                print("ğŸ™…ğŸ» Top Q NOT contain Ground Truth")

            res_board_dict[1]["top_q_crop_ids"] = s1_top_q_crop_ids
            res_board_dict[1]["raw_attn_dict"] = s1_raw_res_dict
            res_board_dict[1]["is_gt_in_top_q"] = bool(s1_is_gt_in_top_q)

            for c in s1_crop_list_out:
                if "img" in c: del c["img"]
                if "resized_img" in c: del c["resized_img"]
                if "token_span" in c: del c["token_span"]
            res_board_dict[1]["crop_list"] = s1_crop_list_out


            #! [Stage 2] Attention Refinement Pass -------------------------------------
            print("\n[Stage 2] Starting Attention Refinement Pass...")

            original_crop_map = {c['id']: c for c in crop_list}
            s2_input_crop_ids = set()
            if 0 in original_crop_map:
                s2_input_crop_ids.add(0)
            for crop_id in s1_top_q_crop_ids:
                s2_input_crop_ids.add(crop_id)
            s2_input_crops = [original_crop_map[cid] for cid in s2_input_crop_ids if cid in original_crop_map]

            # run_refinement_pass í˜¸ì¶œ ì‹œ gt_bboxë¥¼ ë„˜ê²¨ì£¼ê³ , ì„±ê³µ ì—¬ë¶€ë¥¼ final_successì— ì €ì¥
            final_success = run_refinement_pass(
                crop_list=s2_input_crops,
                question=question,
                original_image=original_image,
                save_dir=s2_dir,
                gt_bbox=original_bbox # gt_bbox ì „ë‹¬
            )
            if final_success:
                print("âœ… Grounding Success")
            else:
                print("âŒ Grounding Fail")
            
            # -------------------------------------------------------------------------
            # ê° inst_dir_nameë§ˆë‹¤ ì„±ê³µ ì—¬ë¶€ì— ë”°ë¼ ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½
            new_inst_dir_name = f"{inst_dir_name} {'âœ…' if final_success else 'âŒ'}"
            new_dir = os.path.join(save_dir, "seg", filename_wo_ext, new_inst_dir_name)
            if os.path.exists(new_dir):  # ê¸°ì¡´ì— ë™ì¼í•œ ì´ë¦„ì˜ í´ë”ê°€ ìˆìœ¼ë©´ ì‚­ì œ
                try:
                    shutil.rmtree(new_dir)
                    print(f"Existing directory '{new_dir}' removed.")
                except Exception as e:
                    print(f"Failed to remove existing directory '{new_dir}': {e}")
            if os.path.exists(inst_dir):
                try:
                    os.rename(inst_dir, new_dir)
                except Exception as e:
                    print(f"Failed to rename directory: {e}")
            # -------------------------------------------------------------------------

            res_board_dict[2]["is_gt_in_top_q"] = bool(final_success)
            res_board_dict[2]["gt_score_list"].append(1 if final_success else 0)

            _gt_score_list = res_board_dict[2]["gt_score_list"]
            up2now_gt_score = calc_acc(_gt_score_list)
            print(f"Up2Now S2 Grounding Accuracy:{up2now_gt_score}") # ì¶œë ¥ ë©”ì‹œì§€ ìˆ˜ì •
            print("\n----------------------\n")

            item_res['filename'] = filename
            item_res['data_type'] = item["data_type"]
            item_res['data_source'] = item["data_source"]
            item_res['instruction'] = instruction
            item_res['stage1_res'] = res_board_dict[1]
            item_res['stage2_res'] = res_board_dict[2]
            item_res['gt_bbox'] = original_bbox
            item_res['num_crop'] = len(stage_crop_list)
            task_res.append(item_res)

        #! ==================================================
        # ë§ˆì§€ë§‰ ê²°ê³¼ ëª¨ìŒ ì •ë¦¬
        with open(os.path.join(save_dir, dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        print("==================================================")
        print(task + ": Total num: " + str(num_action))
        print(task + ": Wrong format num: " + str(num_wrong_format))
        print(task + f": Ground truth included Acc: " + str(calc_acc(_gt_score_list)))

        metrics = {
            "task": task,
            "total_num": num_action,
            "segmentation_failed": num_segmentation_failed,
            "num_segmentation_failed": num_segmentation_failed,
            "acc": calc_acc(_gt_score_list)
        }

        with open(os.path.join(save_dir, f"{task}_metrics.json"), "w") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=4)
