# run_gui_actor.py

#! Argument =======================
SEED = 0

# Enviroment
import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= "1"  # ëª‡ë²ˆ GPU ì‚¬ìš©í• ì§€ ("0,1", "2" ë“±)
max_memory = {
    0: "75GiB",
    # 1: "75GiB",
    # 2: "75GiB",
    "cpu": "120GiB",  # ë‚¨ëŠ” ê±´ CPU ì˜¤í”„ë¡œë”©xs
}

# Dataset & Model
MLLM_PATH = "microsoft/GUI-Actor-3B-Qwen2.5-VL"
SCREENSPOT_IMGS = "./data/screenspotv2_imgs"  # input image ê²½ë¡œ
SCREENSPOT_JSON = "./data"  # jsoníŒŒì¼ ê²½ë¡œ
TASKS = ["mobile"]
SAMPLE_RANGE = slice(None)  #! ìƒ˜í”Œ ë²”ìœ„ ì§€ì • (3ë²ˆ ìƒ˜í”Œì´ë©´ 3,4 / 5~9ë²ˆ ìƒ˜í”Œì´ë©´ 5,10 / ì „ì²´ ì‚¬ìš©ì´ë©´ None)
SAVE_DIR = "./attn_output/" + "0824_hoon"  #! ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ë°©ë²•ì„ ë°”ê¾¼ë‹¤ë©´ ë°”ê¿”ì„œ ê¸°ë¡í•˜ê¸°)

# Visualize
STAGE0_VIS = False
STAGE1_VIS = False
STAGE2_VIS = False
ITER_LOG = True  # csv, md

#! Hyperparameter =================

# Model Architecture
LAYER_NUM = 31

# Stage 1: Segmentation & Selection
SELECT_THRE = 0.70  # score >= tau * max_score ì¸ ëª¨ë“  crop select
EARLY_EXIT = True

# Stage 2: Attention Refinement  
AGG_START = 20  # Starting layer for attention aggregation

# Image Resize Ratios
S1_RESIZE_RATIO = 0.25  # Stage 1 crop resize ratio
S2_RESIZE_RATIO = 0.50  # Stage 2 crop resize ratio  
THUMBNAIL_RESIZE_RATIO = 0.10  # Thumbnail resize ratio


# Question
# QUESTION_TEMPLATE="""Where should you tap to {task_prompt}?"""
QUESTION_TEMPLATE="""
You are an assistant trained to navigate the android phone. Given a
task instruction, a screen observation, guess where should you tap.
# Intruction
{task_prompt}"""

#! ==============================================

# Standard Library
import json
import re
import sys
import time
from copy import deepcopy
from pathlib import Path
from typing import List
from math import sqrt

# Third-Party Libraries
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.patheffects as pe
from matplotlib.lines import Line2D
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
import torch
# from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoTokenizer, set_seed
from transformers import AutoProcessor, AutoTokenizer, set_seed

# Project-Local Modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from qwen_vl_utils import process_vision_info
from iter_logger import init_iter_logger, append_iter_log  # log csv ê¸°ë¡ íŒŒì¼
from gui_actor.modeling_qwen25vl import Qwen2_5_VLForConditionalGenerationWithPointer
from gui_actor.multi_image_inference import inference
from visualize_util import visualize_results
from crop2_2 import crop  #! ì–´ë–¤ crop íŒŒì¼ ì‚¬ìš©?
from thop import profile #! flops

#! ==============================================

class ModelKwargsWrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
    def forward(self, inputs_dict):
        return self.model(**inputs_dict)

def get_model_inputs(msgs, tokenizer, processor, device): # profiler í•œí…Œ ëª¨ë¸ ì…ë ¥ ì£¼ê¸° ìœ„í•´ì„œ
    """Prepares model inputs from a conversation dictionary for FLOPs calculation."""
    user_content = next((item['content'] for item in msgs if item['role'] == 'user'), [])
    images = [content['image'] for content in user_content if content['type'] == 'image']
    text = tokenizer.apply_chat_template(
        msgs,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = processor(text=[text], images=images, return_tensors='pt')
    # Move all tensor inputs to the correct device
    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        return super(NpEncoder, self).default(obj)

def create_guiactor_msgs(crop_list, instruction):
    user_content = []
    for crop in crop_list:
        img = crop["resized_img"]
        user_content.append({"type": "image", "image": img})

    user_content.append({
        "type": "text",
        "text": instruction,
    })
    conversation = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": (
                        "You are a GUI agent. Given a screenshot of the current GUI and a human instruction, "
                        "your task is to locate the screen element that corresponds to the instruction. "
                        "You should output a PyAutoGUI action that performs a click on the correct position. "
                        "To indicate the click location, we will use some special tokens, which is used to refer to a visual patch later. "
                        "For example, you can output: pyautogui.click(<your_special_token_here>)."
                    ),
                }
            ]
        },
        {
            "role": "user",
            "content": user_content,
        },
    ]
    return conversation

def resize_crop_list(crop_list: List, ratio: float):
    stage_crop_list = []

    for crop in crop_list:
        crop_id = crop.get("id")

        # ì¸ë„¤ì¼(id=0)ì€ thumbnail ë¹„ìœ¨, ë‚˜ë¨¸ì§€ëŠ” crop ë¹„ìœ¨ ì‚¬ìš©
        if crop_id == 0:
            crop_ratio = THUMBNAIL_RESIZE_RATIO
        else:
            crop_ratio = ratio

        new_crop = deepcopy(crop)
        crop_img = deepcopy(crop["img"])

        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
        crop_width, crop_height = crop_img.size
        crop_img = crop_img.resize((int(crop_width * crop_ratio), int(crop_height * crop_ratio)))
        new_crop["resized_img"] = crop_img
        stage_crop_list.append(new_crop)

    return stage_crop_list

def select_crop(crop_list, tau):
    """
    score >= tau * max_score ì¸ ëª¨ë“  cropì˜ idë§Œ ë°˜í™˜ (id==0ì€ ë¬´ì‹œ)
    """
    filtered = [crop for crop in crop_list if crop.get("id") != 0]
    scores = [float(c["s1_att_sum"]) for c in filtered]
    max_score = max(scores)
    threshold = max_score * float(tau)
    out_ids = []
    for crop in filtered:
        cid = crop.get("id")
        if float(crop["s1_att_sum"]) >= threshold:
            out_ids.append(cid)
    return out_ids

def check_gt_in_selected_crops(top_q_bboxes: List, gt_bbox: List):
    def rect_intersects(a, b):
        # a, b: [left, top, right, bottom]
        al, at, ar, ab = a
        bl, bt, br, bb = b
        # êµì§‘í•© ì˜ì—­ ê³„ì‚°
        inter_left = max(al, bl)
        inter_top = max(at, bt)
        inter_right = min(ar, br)
        inter_bottom = min(ab, bb)
        return (inter_right > inter_left) and (inter_bottom > inter_top)
    return any(rect_intersects(gt_bbox, bbox) for bbox in top_q_bboxes)

def compute_attention_scores(crop_list, per_image_outputs):
    """ê° cropì˜ attention score ê³„ì‚°"""
    for i, crop in enumerate(crop_list):
        crop_att_scores_np = np.array(per_image_outputs[i]['attn_scores'][0])
        total_att_score = np.sum(crop_att_scores_np)
        
        # ë©´ì ì˜ ì œê³±ê·¼ìœ¼ë¡œ normalize
        bbox = crop.get('bbox')
        if bbox is not None:
            left, top, right, bottom = bbox
            area = max(1, (right - left) * (bottom - top))
        else:
            area = 1
        crop['s1_att_sum'] = total_att_score / sqrt(area)

def find_top_crop_for_early_exit(crop_list, per_image_outputs):
    """Early Exitìš© ìµœê³  ì ìˆ˜ cropê³¼ point ì°¾ê¸°"""
    top_score = -1
    top_point = None
    top_crop_id = -1
    
    for i, crop in enumerate(crop_list):
        if crop.get("id") == 0:  # ì¸ë„¤ì¼ì€ ìŠ¤í‚µ
            continue
            
        crop_att_scores_np = np.array(per_image_outputs[i]['attn_scores'][0])
        crop_top_point = per_image_outputs[i]['topk_points'][0]
        crop_top_score = np.max(crop_att_scores_np)
        
        crop['top_point'] = {'point': crop_top_point, 'score': crop_top_score}
        
        if top_score < crop_top_score:
            top_score = crop_top_score
            top_point = crop_top_point
            top_crop_id = crop['id']
    
    return top_point, top_crop_id

def get_highest_attention_patch_bbox(image_result: dict) -> list:
    """
    per_image ê²°ê³¼ì—ì„œ ì–´í…ì…˜ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ íŒ¨ì¹˜ë¥¼ ì°¾ì•„ 
    í•´ë‹¹ íŒ¨ì¹˜ì˜ ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ ë°˜í™˜
    """
    # 1. ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
    attn_scores = np.array(image_result['attn_scores'][0])
    n_width = image_result['n_width']
    n_height = image_result['n_height']

    # 2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ íŒ¨ì¹˜ì˜ 1ì°¨ì› ì¸ë±ìŠ¤ ì°¾ê¸°
    highest_score_idx = np.argmax(attn_scores)

    # 3. 1ì°¨ì› ì¸ë±ìŠ¤ë¥¼ 2ì°¨ì› íŒ¨ì¹˜ ê·¸ë¦¬ë“œ ì¢Œí‘œ (patch_x, patch_y)ë¡œ ë³€í™˜
    # (patch_xëŠ” ê°€ë¡œ ì¸ë±ìŠ¤, patch_yëŠ” ì„¸ë¡œ ì¸ë±ìŠ¤)
    patch_y = highest_score_idx // n_width
    patch_x = highest_score_idx % n_width

    # 4. íŒ¨ì¹˜ ê·¸ë¦¬ë“œ ì¢Œí‘œë¥¼ ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¡œ ê³„ì‚°
    # ê° íŒ¨ì¹˜ì˜ ì •ê·œí™”ëœ ë„ˆë¹„ì™€ ë†’ì´
    patch_norm_width = 1.0 / n_width
    patch_norm_height = 1.0 / n_height
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
    left = patch_x * patch_norm_width
    top = patch_y * patch_norm_height
    right = (patch_x + 1) * patch_norm_width
    bottom = (patch_y + 1) * patch_norm_height
    
    return [left, top, right, bottom]

def check_early_exit_condition(top_point, top_crop_id, crop_list, per_image_outputs, gt_bbox):
    """Early Exit ì¡°ê±´ í™•ì¸"""
    if top_point is None or top_crop_id == -1:
        return False, False
    
    ori_w, ori_h = original_image.size
    
    # ì¸ë„¤ì¼ì˜ ìµœê³  attention patch ì°¾ê¸°
    thumb_res = next((res for res in per_image_outputs if res['index'] == 0), None)
    thumb_top_patch_bbox = get_highest_attention_patch_bbox(thumb_res)
    l, t, r, b = thumb_top_patch_bbox
    denorm_thumb_top_patch_bbox = [l*ori_w, t*ori_h, r*ori_w, b*ori_h]
    
    # ì¢Œí‘œ ë³´ì •
    top_crop = next((c for c in crop_list if c['id'] == top_crop_id), None)
    top_crop_bbox = top_crop["bbox"]
    corrected_point = denormalize_crop_point(
        point_in_crop=top_point, 
        crop_size=top_crop['img'].size,
        crop_bbox=top_crop_bbox
    )
    
    should_exit_early = point_in_bbox(corrected_point, denorm_thumb_top_patch_bbox)

    # Early Exit ë§ì•˜ëŠ”ê°€?
    early_exit_success = point_in_bbox(corrected_point, gt_bbox)
    
    return should_exit_early, early_exit_success

def run_selection_pass_with_guiactor(msgs, crop_list, gt_bbox: List, attn_vis_dir: str):
    """Stage 1 inference ë° Early Exit íŒë‹¨"""
    
    # Inference ìˆ˜í–‰
    pred = inference(msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    per_image_outputs = pred["per_image"]
    
    # Attention scores ê³„ì‚°
    compute_attention_scores(crop_list, per_image_outputs)
    
    # Early Exit ì²´í¬
    should_exit_early, early_exit_success = False, False
    
    if EARLY_EXIT:
        top_point, top_crop_id = find_top_crop_for_early_exit(crop_list, per_image_outputs)
        should_exit_early, early_exit_success = check_early_exit_condition(
            top_point, top_crop_id, crop_list, per_image_outputs, gt_bbox
        )    # Early Exití•˜ë©´ select_crop ìŠ¤í‚µ
    if should_exit_early:
        top_q_crop_ids = []
        top_q_bboxes = []
    else:
        # Select crop: score >= tau * max_scoreì¸ crops ì„ íƒ
        top_q_crop_ids = select_crop(crop_list, tau=SELECT_THRE)
        top_q_bboxes = [crop["bbox"] for crop in crop_list if crop.get("id") in top_q_crop_ids]
    
    # ì‹œê°í™” (í•„ìš”ì‹œ)
    if STAGE1_VIS and EARLY_EXIT and should_exit_early:
        _visualize_early_exit_results(crop_list, pred, gt_bbox, attn_vis_dir)
    elif STAGE1_VIS and not should_exit_early:
        _visualize_stage1_results(crop_list, pred, attn_vis_dir)
    
    return top_q_crop_ids, top_q_bboxes, crop_list, should_exit_early, early_exit_success

def _visualize_early_exit_results(crop_list, pred, gt_bbox, attn_vis_dir):
    """Early Exit ì‹œê°í™”"""
    s1_att_vis_path = attn_vis_dir + "/output.png"
    visualize_results(crop_list, pred, instruction=instruction, save_path=s1_att_vis_path)
    
    # ì„ì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬ (Early Exitì´ë¯€ë¡œ crop selection ì—†ìŒ)
    visualize_crop(save_dir=attn_vis_dir, gt_bbox=gt_bbox, 
                   top_q_bboxes=[], instruction=instruction, filename="ee_gt_vis.png")

def _visualize_stage1_results(crop_list, pred, attn_vis_dir):
    """ì¼ë°˜ Stage1 ì‹œê°í™”"""
    s1_att_vis_path = attn_vis_dir + "/output.png"
    visualize_results(crop_list, pred, instruction=instruction, save_path=s1_att_vis_path)

def denormalize_crop_point(point_in_crop, crop_size, crop_bbox):
    crop_w, crop_h = crop_size

    scaled_point = [point_in_crop[0] * crop_w, point_in_crop[1] * crop_h]
    corrected_point = [scaled_point[0] + crop_bbox[0], scaled_point[1] + crop_bbox[1]] 

    return corrected_point

def find_best_crop_point(crop_list, per_image_outputs):
    """ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ cropê³¼ point ì°¾ê¸°"""
    top_score = -1
    top_point = None
    top_crop_id = -1

    for i, crop in enumerate(crop_list):
        if crop.get("id") == 0:  # ì¸ë„¤ì¼ ìŠ¤í‚µ
            continue
            
        crop_att_scores_np = np.array(per_image_outputs[i]['attn_scores'][0])
        crop_top_point = per_image_outputs[i]['topk_points'][0]
        crop_top_score = np.max(crop_att_scores_np)

        crop['top_point'] = {'point': crop_top_point, 'score': crop_top_score}

        if top_score < crop_top_score:
            top_score = crop_top_score
            top_point = crop_top_point
            top_crop_id = crop['id']
    
    return top_point, top_crop_id

def run_refinement_pass_with_guiactor(crop_list: List, instruction: str, original_image: Image, save_dir: str, gt_bbox: List):
    """Stage 2: ì„ íƒëœ cropë“¤ë¡œ ìµœì¢… grounding ìˆ˜í–‰"""
    
    # Stage 2 ìš© ë¦¬ì‚¬ì´ì¦ˆ
    s2_resized_crop_list = resize_crop_list(crop_list=crop_list, ratio=S2_RESIZE_RATIO)
    s2_msgs = create_guiactor_msgs(crop_list=s2_resized_crop_list, instruction=instruction)

    # Inference
    pred = inference(s2_msgs, model, tokenizer, processor, use_placeholder=True, topk=3)
    per_image_outputs = pred["per_image"]
    
    # ìµœê³  ì ìˆ˜ crop ì°¾ê¸°
    top_point, top_crop_id = find_best_crop_point(s2_resized_crop_list, per_image_outputs)
    
    if top_point is None:
        return False
    
    # ì›ë³¸ cropì—ì„œ bbox ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    top_crop = next((c for c in crop_list if c['id'] == top_crop_id), None)
    if top_crop is None:
        return False
        
    top_crop_bbox = top_crop["bbox"]
    
    # ì¢Œí‘œ ë³´ì • ë° ì„±ê³µ ì—¬ë¶€ íŒë‹¨
    corrected_point = denormalize_crop_point(
        point_in_crop=top_point, 
        crop_size=top_crop['img'].size, 
        crop_bbox=top_crop_bbox
    )
    is_success = point_in_bbox(corrected_point, gt_bbox)

    # ì‹œê°í™” (í•„ìš”ì‹œ)
    if STAGE2_VIS:
        _visualize_stage2_results(save_dir, s2_resized_crop_list, pred, gt_bbox, corrected_point, instruction)
        
    return is_success

def _visualize_stage2_results(save_dir, crop_list, pred, gt_bbox, click_point, instruction):
    """Stage 2 ê²°ê³¼ ì‹œê°í™”"""
    s2_att_vis_path = save_dir + "/output.png"
    visualize_results(crop_list, pred, instruction=instruction, save_path=s2_att_vis_path)
    visualize_crop(save_dir=save_dir, gt_bbox=gt_bbox, top_q_bboxes=[], 
                   instruction=instruction, filename="gt_vis.png", click_point=click_point)

def visualize_crop(save_dir, gt_bbox, top_q_bboxes, instruction, filename, click_point=None):
    # Visualize ground truth and selected crop on the image
    result_img = Image.open(img_path)

    draw = ImageDraw.Draw(result_img)
    # Draw ground truth bbox in green
    draw.rectangle(gt_bbox, outline="green", width=2)

    font = None
    try:
        font = ImageFont.truetype("DejaVuSans-Bold.ttf", 20)
    except IOError:
        font = ImageFont.load_default()

    for bbox in top_q_bboxes:
        draw.rectangle(bbox, outline="red", width=2)
        text_to_draw = f"{instruction}"
        crop_left, crop_top, crop_right, crop_bottom = bbox
        inst_position = (crop_left, crop_top)
        draw.text(inst_position, text_to_draw, fill="red", font=font)

    # Draw click point as an orange circle
    if click_point is not None:
        click_x, click_y = click_point[0], click_point[1]
        radius = 13
        draw.ellipse((click_x - radius, click_y - radius, click_x + radius, click_y + radius), outline="purple", width=3)

    # Ensure the save directory exists
    os.makedirs(save_dir, exist_ok=True)
    # Save the result image
    result_path = os.path.join(save_dir, filename)
    # print(f"Top Q box is saved at : {result_path}")
    result_img.save(result_path)

def visualize_attn_map(attn_output, msgs, crop_list, attn_vis_dir):
    image_inputs, _ = process_vision_info(msgs)

    # grid í¬ê¸° ë½‘ì•„ë‘ê¸°
    img_proc_out = processor.image_processor(images=image_inputs)
    grid = img_proc_out["image_grid_thw"]
    # (batch, num_imgs, 3) í˜¹ì€ (num_imgs, 3) í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ ë½‘ê¸°
    if grid.ndim == 3:
        grid = grid[0]   # (num_imgs, 3)

    # ìµœì¢… token-map ì°¨ì›: t Ã— (h//2) Ã— (w//2)
    final_shapes = [
        (t, h//2, w//2)
        for t, h, w in grid
    ]

    num_imgs = len(crop_list)
    fig, axes = plt.subplots(1, num_imgs, figsize=(5*num_imgs, 5))

    for i, crop in enumerate(crop_list):
        (st, end) = crop["token_span"] # cropì˜ í† í° ì‹œì‘, ë index ë½‘ê¸°
        t, h2, w2 = final_shapes[i]
        att_maps = []

        # for li in range(L):
        for li in range(AGG_START, LAYER_NUM):
            att = (
                attn_output.attentions[li]         # (batch, heads, seq_q, seq_k)
                [0, :, -1, st:end]              # batch=0, ë§ˆì§€ë§‰ query í† í°, vision span
                .mean(dim=0)                 # head í‰ê· 
                .to(torch.float32)           # bfloat16 â†’ float32
                .cpu()
                .numpy()
            )
            att_map = att.reshape(t, h2, w2).mean(axis=0)  # ì‹œê°„ì¶• í‰ê· 
            att_maps.append(att_map)

        att_avg = np.mean(att_maps, axis=0)  # 32ê°œ ë ˆì´ì–´ í‰ê· 

        ax = axes[i] if num_imgs > 1 else axes
        im = ax.imshow(att_avg, cmap="viridis", interpolation="nearest")
        ax.set_title(f"crop{crop['id']}")
        ax.axis("off")

    plt.tight_layout()

    out_dir = Path(attn_vis_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    _save_path = os.path.join(out_dir, "attn_map.png")

    fig.savefig(_save_path, dpi=300, bbox_inches="tight", facecolor="white")
    # print(f"attn_map saved at: {_save_path}")

def upsample_att_map(att_map_low_res: np.ndarray, size):
    """
    Pillowë¥¼ ì´ìš©í•œ bilinear ì—…ìƒ˜í”Œ (size=(H, W))
    ì…ë ¥/ì¶œë ¥ ëª¨ë‘ float32 ìœ ì§€
    """
    h, w = size
    # ì•ˆì „ì¥ì¹˜: ìŒìˆ˜/NaN ì œê±°
    m = np.nan_to_num(att_map_low_res.astype(np.float32), nan=0.0, neginf=0.0, posinf=0.0)
    if m.size == 0:
        return np.zeros((h, w), dtype=np.float32)
    # ê°’ ë²”ìœ„ë¥¼ ì¼ë‹¨ 0 ì´ìƒìœ¼ë¡œ í´ë¨í”„
    m[m < 0] = 0.0
    im = Image.fromarray(m)
    im = im.resize((w, h), resample=Image.BILINEAR)
    out = np.array(im).astype(np.float32)
    # scaleì— ë”°ë¼ ê°’ì´ ì•½ê°„ ë³€í•  ìˆ˜ ìˆì–´ 0 ì´ìƒìœ¼ë¡œ ì¬í´ë¨í”„
    out[out < 0] = 0.0
    return out

def point_in_bbox(point, bbox):
    """
    point: (x, y)
    bbox: (left, top, right, bottom)
    ê²½ê³„ í¬í•¨
    """
    x, y = point
    l, t, r, b = bbox
    return (l <= x <= r) and (t <= y <= b)

def boxfilter_sum(arr: np.ndarray, r: int):
    """
    ëë¶€ë¶„ ë³´ì • ì—†ìŒ(neighbor-sum).
    (2r+1)x(2r+1) ì°½ê³¼ ì‹¤ì œ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì˜ 'í•©'ë§Œ ê³„ì‚°.
    í‰ê·  ì•„ë‹˜. ê°€ì¥ìë¦¬ëŠ” ì°½ì´ ëœ ê²¹ì¹˜ë¯€ë¡œ ì†í•´ë³´ê²Œ ë¨.
    """
    if r <= 0:
        return arr.astype(np.float32, copy=True)

    a = arr.astype(np.float32, copy=False)
    H, W = a.shape

    # ì ë¶„ì˜ìƒ: ìƒë‹¨/ì¢Œì¸¡ 0 íŒ¨ë”©ì„ í•œ ì¹¸ ì¶”ê°€í•´ì„œ ë²¡í„°í™” ê³„ì‚° ìš©ì´í•˜ê²Œ êµ¬ì„±
    ii = np.pad(a, ((1, 0), (1, 0)), mode='constant').cumsum(axis=0).cumsum(axis=1)

    ys = np.arange(H)[:, None]   # Hx1
    xs = np.arange(W)[None, :]   # 1xW

    y0 = np.clip(ys - r, 0, H)
    y1 = np.clip(ys + r + 1, 0, H)
    x0 = np.clip(xs - r, 0, W)
    x1 = np.clip(xs + r + 1, 0, W)

    # ì ë¶„ì˜ìƒ ì¸ë±ìŠ¤ëŠ” +1 íŒ¨ë”© ê³ ë ¤í•´ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
    S = ii[y1, x1] - ii[y0, x1] - ii[y1, x0] + ii[y0, x0]
    return S

def visualize_aggregated_attention(
        crop_list,
        original_image, inst_dir, gt_bbox, individual_maps_dir=None,
        neigh_radius = 20,         #! ì´ì›ƒí•©(box filter) ë°˜ê²½ r â†’ (2r+1)^2 ì°½
        topk_points = 5,           # ìƒìœ„ ì  ê°œìˆ˜ (ë³´ì—¬ì£¼ê¸°ìš©)
        min_dist_pix = 200,        # ìƒìœ„ ì  ì‚¬ì´ ìµœì†Œ ê°„ê²© (í”½ì…€)
        star_marker_size= 8,      # ë³„ í¬ê¸° (1ë“±)
        dot_marker_size = 5,      # ì  í¬ê¸° (2~5ë“±)
        text_fontsize= 7          # ì ìˆ˜ í…ìŠ¤íŠ¸ í°íŠ¸ í¬ê¸°
    ):
    """
    ì´ì›ƒí•© ê¸°ë°˜ ìµœëŒ€ì  íƒìƒ‰:
    - í•©ì„± ë§µ ì •ê·œí™” í›„ boxfilter_sum(neigh_radius)ë¡œ ì´ì›ƒí•© ê³„ì‚°
    - greedy ë¹„ìµœëŒ€ ì–µì œ(NMS)ë¡œ ìƒìœ„ topk_points ì¢Œí‘œ ì„ íƒ(ê°„ê²© min_dist_pix)
    - ì‹œê°í™”:
        â€¢ s2_result_only: ì›ë³¸+í•©ì„±ë§µ+GT ë°•ìŠ¤ë§Œ
        â€¢ s2_result_star: top-1ì€ ë³„(*), top-k ëª¨ë‘ëŠ” ì´ì›ƒí•© ì ìˆ˜ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
    - ì„±ê³µ íŒì •ì€ top-1 ì ì´ GT ë°•ìŠ¤ ì•ˆì´ë©´ True
    """

    os.makedirs(os.path.dirname(inst_dir + "/stage2"), exist_ok=True)
    if individual_maps_dir:
        os.makedirs(individual_maps_dir, exist_ok=True)

    # ìº”ë²„ìŠ¤ ë° í•©ì„± ë§µ ì¤€ë¹„
    W, H = original_image.size
    aggregated_attention_map = np.zeros((H, W), dtype=np.float32)

    # ê° cropì˜ ë§µì„ ì›ë³¸ ì¢Œí‘œê³„ë¡œ ì—…ìƒ˜í”Œí•˜ì—¬ í•©ì„±
    for crop in crop_list:
        if 'bbox' not in crop or 'att_avg_masked' not in crop:
            continue

        left, top, right, bottom = map(int, crop['bbox'])
        cw = max(0, right - left)
        ch = max(0, bottom - top)
        if cw == 0 or ch == 0:
            continue

        att_low = crop['att_avg_masked']
        att_up = upsample_att_map(att_low, size=(ch, cw))  # íŒŒì¼ ìƒë‹¨ ì •ì˜ ê°€ì •

        # ê°œë³„ ë§µ ì €ì¥(ì˜µì…˜)
        if individual_maps_dir:
            indiv = np.zeros((H, W), dtype=np.float32)
            indiv[top:bottom, left:right] = att_up
            plt.figure(figsize=(10, 10 * H / W))
            plt.imshow(original_image, extent=(0, W, H, 0))
            plt.imshow(indiv, cmap='viridis', alpha=0.6, extent=(0, W, H, 0))
            plt.axis('off')
            ttl = f"Crop ID: {crop.get('id','?')}"
            plt.title(ttl)
            path = os.path.join(individual_maps_dir, f"individual_attn_crop_{crop.get('id','unk')}.png")
            plt.savefig(path, dpi=150, bbox_inches='tight', pad_inches=0)
            plt.close()

        aggregated_attention_map[top:bottom, left:right] += att_up

    # ì´ì›ƒí•© ê¸°ë°˜ ìƒìœ„ ì  ì„ ì •
    top_points = []
    scores = []  # boxfilter_sumìœ¼ë¡œ ì–»ì€ ì´ì›ƒí•© ê°’

    if aggregated_attention_map.max() > 0:
        normalized = aggregated_attention_map / (aggregated_attention_map.max() + 1e-8)
        smoothed = boxfilter_sum(normalized, neigh_radius)  # íŒŒì¼ ìƒë‹¨ ì •ì˜ ê°€ì •

        # greedy NMSë¡œ ìƒìœ„ Kê°œ ì  ì„ íƒ
        sm = smoothed.copy()
        Hh, Ww = sm.shape

        for _ in range(int(topk_points)):
            idx = int(np.argmax(sm))
            vy, vx = divmod(idx, Ww)
            best_val = sm[vy, vx]
            if not np.isfinite(best_val) or best_val <= 0:
                break
            # ì  ê¸°ë¡
            top_points.append((int(vx), int(vy)))
            scores.append(float(best_val))
            # ì •ì‚¬ê°í˜• ì–µì œ
            y1 = max(0, vy - min_dist_pix); y2 = min(Hh - 1, vy + min_dist_pix)
            x1 = max(0, vx - min_dist_pix); x2 = min(Ww - 1, vx + min_dist_pix)
            sm[y1:y2+1, x1:x2+1] = -np.inf

    # ì„±ê³µ íŒì •: top-1 ê¸°ì¤€
    is_grounding_success = False
    if len(top_points) > 0:
        cx, cy = top_points[0]
        gl, gt, gr, gb = gt_bbox
        is_grounding_success = (gl <= cx <= gr) and (gt <= cy <= gb)
        print(f"ğŸ¯ Our Grounding: {(cx, cy)} , GT: {gt_bbox}, Neigh_sum: {scores[0]:.2f}")
    else:
        print("Aggregated attention map empty ë˜ëŠ” peak ì—†ìŒ")

    # ì‹œê°í™”: ê³µí†µ ë°”íƒ•
    fig, ax = plt.subplots(figsize=(10, 10 * H / W))
    ax.imshow(original_image, extent=(0, W, H, 0))
    ax.imshow(aggregated_attention_map, cmap='viridis', alpha=0.6, extent=(0, W, H, 0))

    # ê·¸ëƒ¥ Attention ìƒíƒœë§Œ ì €ì¥ -> ê°€ë¦¬ëŠ”ê±° ì—†ì´ ë³´ì´ë„ë¡.
    plt.savefig(inst_dir + "/s2_result_only.png", dpi=300, bbox_inches="tight", pad_inches=0)

    # GT ë°•ìŠ¤(ì´ˆë¡)
    gl, gt, gr, gb = gt_bbox
    gt_rect = patches.Rectangle((gl, gt), gr - gl, gb - gt, linewidth=3, edgecolor='lime', facecolor='none')
    ax.add_patch(gt_rect)

    # ë²”ë¡€
    green_patch = patches.Patch(color='lime', label='Ground Truth BBox')
    star_legend = Line2D([0], [0], marker='*', color='w', label='NeighSum Top-1', 
                         markerfacecolor='yellow', markeredgecolor='black', markersize=star_marker_size)
    ax.legend([green_patch, star_legend], ['Ground Truth BBox', 'NeighSum Top-1'], loc='best')

    ax.axis('off')
    ax.set_title("Attention (aggregated) + NeighSum Peaks")
    plt.tight_layout()

    # ì‹œê°í™”: top-1 ë³„í‘œ, top-2~5 ê²€ì • ì , top-k í…ìŠ¤íŠ¸ ë¼ë²¨
    if len(top_points) > 0:
        # top-1 ë³„í‘œ
        ax.plot(top_points[0][0], top_points[0][1], 'y*',
                markersize=star_marker_size, markeredgecolor='black')

        # top-2~5 ê²€ì • ì 
        for i in range(1, min(len(top_points), topk_points)):
            px, py = top_points[i]
            ax.plot(px, py, 'o', 
                    markersize=dot_marker_size, markerfacecolor='black', markeredgecolor='white', markeredgewidth=0.9)

        # top-k í…ìŠ¤íŠ¸(ëª¨ë‘ í‘œê¸°: ì ìˆ˜ë§Œ)
        for (px, py), sc in zip(top_points, scores):
            label = f"{sc:.3f}"
            ax.text(px + 10, py - 10, label,
                    fontsize=text_fontsize, color='white', ha='left', va='top',
                    path_effects=[pe.withStroke(linewidth=2, foreground='black')])


    plt.savefig(inst_dir + "/s2_result_star.png", dpi=300, bbox_inches="tight", pad_inches=0)
    plt.close(fig)

    return bool(is_grounding_success)

#! ================================================================================================

if __name__ == '__main__':

    set_seed(SEED)

    # Model Import
    model = Qwen2_5_VLForConditionalGenerationWithPointer.from_pretrained(
        MLLM_PATH, torch_dtype="auto", attn_implementation="sdpa",
        device_map="balanced", max_memory=max_memory, low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(MLLM_PATH)
    processor = AutoProcessor.from_pretrained(MLLM_PATH)
    model.eval()

    # save_dir í´ë”ëª…ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê³ ìœ í•œ ì´ë¦„ ìƒì„± (save_dir -> save_dir_1 -> save_dir_2)
    save_dir = SAVE_DIR
    suffix = 0
    while os.path.exists(save_dir):
        suffix += 1
        save_dir = f"{SAVE_DIR}_{suffix}"
    os.makedirs(save_dir)

    # ë°”ë¡œë°”ë¡œ log csv, md ì €ì¥ ì–´ë–»ê²Œ í• ì§€
    init_iter_logger(  
        save_dir=save_dir,
        headers=[  # ìˆœì„œ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°
            "idx", "crop_time", "num_crop", "early_exit", "num_selected_crop", 
            "s1_time", "s1_flops_gflops", "s1_hit", "s2_time", "s2_flops_gflops", "s2_hit", 
            "total_time", "total_flops_gflops", "acc_uptonow", "filename", "instruction"
        ],
        write_md=True,
        use_fsync=True,
        use_lock=False
    )

    # Process
    for task in TASKS:
        task_res = dict()
        dataset = "screenspot_" + task + "_v2.json"
        screenspot_data = json.load(open(os.path.join(SCREENSPOT_JSON, dataset), 'r'))
        screenspot_data = screenspot_data[SAMPLE_RANGE]

        print("Num of sample: " + str(len(screenspot_data)), flush=True)

        # í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        task_res = []
        num_action = 0
        seg_time_sum = s1_time_sum = s2_time_sum = total_flops = 0.0
        early_exit_count = early_exit_success_count = final_success_count = 0

        for j, item in tqdm(enumerate(screenspot_data)):

            print("\n\n----------------------\n")

            num_action += 1
            
            # íŒŒì¼ ë° ë°ì´í„° ë¡œë“œ
            filename = item["img_filename"]
            filename_wo_ext, ext = os.path.splitext(filename)
            img_path = os.path.join(SCREENSPOT_IMGS, filename)
            if not os.path.exists(img_path):
                continue

            original_image = Image.open(img_path).convert("RGB")
            instruction = item["instruction"]
            original_bbox = item["bbox"]
            original_bbox = [original_bbox[0], original_bbox[1], 
                           original_bbox[0] + original_bbox[2], original_bbox[1] + original_bbox[3]]
            
            # ë””ë ‰í† ë¦¬ ì„¤ì • (ì‹œê°í™”ìš© - í•„ìš”ì‹œì—ë§Œ)
            if any([STAGE0_VIS, STAGE1_VIS, STAGE2_VIS]):
                inst_dir_name = re.sub(r'\W+', '_', instruction).strip('_')
                inst_dir = os.path.join(save_dir, "seg", filename_wo_ext, inst_dir_name)
                s1_dir = os.path.join(inst_dir, "stage1") 
                s2_dir = os.path.join(inst_dir, "stage2")
                os.makedirs(s1_dir, exist_ok=True)
                os.makedirs(s2_dir, exist_ok=True)
            else:
                inst_dir = s1_dir = s2_dir = None

            #! ==================================================================
            #! Stage 0 | Segmentation
            #! ==================================================================

            seg_start = time.time()
            crop_list = crop(
                image_path=img_path,
                save_visualization=False
            )
            s0_crop_list = resize_crop_list(crop_list=crop_list, ratio=S1_RESIZE_RATIO)
            seg_end = time.time()
            seg_time = seg_end - seg_start

            if STAGE0_VIS and inst_dir:
                all_crops_bboxes = [crop["bbox"] for crop in s0_crop_list]
                visualize_crop(save_dir=inst_dir, gt_bbox=original_bbox, top_q_bboxes=all_crops_bboxes,
                                instruction=instruction, filename="s1_all_crop.png", click_point=None)

            #! ==================================================================
            #! Stage 1 | Find Top Q + Inference
            #! ==================================================================

            # Calculate Stage 1 FLOPs
            s1_msgs = create_guiactor_msgs(crop_list=s0_crop_list, instruction=instruction)
            with torch.no_grad():
                s1_inputs = get_model_inputs(s1_msgs, tokenizer, processor, model.device)
                wrapped_model = ModelKwargsWrapper(model)
                s1_flops, _ = profile(wrapped_model, inputs=(s1_inputs,), verbose=False)

            s1_start = time.time()

            s1_top_q_crop_ids, s1_top_q_bboxes, s0_crop_list_out, should_exit_early, early_exit_success = run_selection_pass_with_guiactor(
                msgs=s1_msgs,
                crop_list=s0_crop_list,
                gt_bbox=original_bbox,
                attn_vis_dir=s1_dir or ""
            )
            s1_infence_end = time.time()

            if should_exit_early:
               early_exit_count +=1
               if early_exit_success:
                  early_exit_success_count += 1

            s1_time = s1_infence_end - s1_start
            seg_time_sum += seg_time
            s1_time_sum += s1_time

            # GTê°€ ì•ˆì— ë“¤ì–´ê°€ëŠ”ì§€ ì²´í¬
            s1_hit = early_exit_success or (not should_exit_early and check_gt_in_selected_crops(s1_top_q_bboxes, original_bbox))

            # ë¶ˆí•„ìš”í•œ ë”•ì…”ë„ˆë¦¬ ì—°ì‚° ì œê±° - ê²°ê³¼ ì €ì¥ìš©ë„ë§Œ
            # res_board_dictëŠ” ì‚¬ì‹¤ìƒ ë¯¸ì‚¬ìš©
            
            #! ==================================================================
            #! [Stage 2] Attention Refinement Pass
            #! ==================================================================
            
            # Early Exit
            if should_exit_early:
                final_success = early_exit_success
                s2_time = 0.0
                s2_flops = 0.0
            else:
                original_crop_map = {c['id']: c for c in crop_list}
                s2_input_crop_ids = set()
                if 0 in original_crop_map:
                    s2_input_crop_ids.add(0)
                for crop_id in s1_top_q_crop_ids:
                    s2_input_crop_ids.add(crop_id)
                s2_input_crops = [original_crop_map[cid] for cid in s2_input_crop_ids if cid in original_crop_map]

                # Calculate Stage 2 FLOPs
                s2_resized_crops = resize_crop_list(crop_list=s2_input_crops, ratio=S2_RESIZE_RATIO)
                s2_msgs = create_guiactor_msgs(crop_list=s2_resized_crops, instruction=instruction)
                with torch.no_grad():
                    s2_inputs = get_model_inputs(s2_msgs, tokenizer, processor, model.device)
                    wrapped_model = ModelKwargsWrapper(model)
                    s2_flops, _ = profile(wrapped_model, inputs=(s2_inputs,), verbose=False)

                s2_inference_start = time.time()

                final_success = run_refinement_pass_with_guiactor(
                    crop_list=s2_input_crops,
                    instruction=instruction,
                    original_image=original_image,
                    save_dir=s2_dir or "",
                    gt_bbox=original_bbox
                )
                s2_inference_end = time.time()
                s2_time = s2_inference_end - s2_inference_start
                
            # ì„±ëŠ¥ ë¡œê¹…
            total_time = seg_time + s1_time + s2_time
            total_flops_this = s1_flops + (s2_flops if not should_exit_early else 0)
            total_flops += total_flops_this
            
            print(f"ğŸ•– Times - Seg: {seg_time:.2f}s | S1: {s1_time:.2f}s | S2: {s2_time:.2f}s | Total: {total_time:.2f}s")
            print(f"ğŸ”¥ FLOPs - S1: {s1_flops/1e9:.2f} | S2: {s2_flops/1e9:.2f} | Total: {total_flops_this/1e9:.2f} GFLOPs")
            print(f"{'âœ…ğŸš¨ Early Exit Success' if should_exit_early and early_exit_success else 'âŒğŸš¨ Early Exit Fail' if should_exit_early else 'âœ… Grounding Success' if final_success else 'âŒ Grounding Fail'}")

            #! ==================================================================
            #! [End]
            #! ==================================================================

            s2_time_sum += s2_time
            final_success_count += final_success

            up2now_gt_score = final_success_count / num_action * 100
            print(f"Up2Now Grounding Accuracy: {up2now_gt_score}%")

            # Iter log - ê°œì„ ëœ ë¡œê¹…
            append_iter_log(
                idx=j+1,
                filename=filename_wo_ext,
                instruction=instruction[:50] + "..." if len(instruction) > 50 else instruction,
                crop_time=f"{seg_time:.3f}",
                num_crop=len(s0_crop_list)-1,
                early_exit="â˜‘ï¸" if should_exit_early else "ğŸ«¥",
                num_selected_crop=len(s1_top_q_crop_ids) if not should_exit_early else 0,
                s1_time=f"{s1_time:.3f}",
                s1_flops_gflops=f"{s1_flops/1e9:.2f}",
                s1_hit="âœ…" if s1_hit else "âŒ",
                s2_time=f"{s2_time:.3f}",
                s2_flops_gflops=f"{s2_flops/1e9:.2f}" if not should_exit_early else "0.00",
                s2_hit="âœ…" if final_success else "âŒ",
                total_time=f"{total_time:.3f}",
                total_flops_gflops=f"{total_flops_this/1e9:.2f}",
                acc_uptonow=f"{up2now_gt_score:.2f}"
            )

            # JSON ê¸°ë¡ - í•µì‹¬ ì •ë³´ë§Œ
            item_res = {
                'filename': filename,
                'instruction': instruction,
                'gt_bbox': original_bbox,
                'num_crop': len(s0_crop_list) - 1,  # ì¸ë„¤ì¼ ì œì™¸
                'early_exit': should_exit_early,
                'early_exit_success': early_exit_success,
                's1_hit': s1_hit,
                's2_hit': final_success,
                'seg_time': seg_time,
                's1_time': s1_time,
                's2_time': s2_time,
                'total_time': total_time,
                's1_flops': s1_flops,
                's2_flops': s2_flops if not should_exit_early else 0,
                'total_flops': total_flops_this
            }
            task_res.append(item_res)

        #! ==================================================
        # Json ì •ë¦¬
        with open(os.path.join(save_dir, dataset), "w") as f:
            json.dump(task_res, f, indent=4, ensure_ascii=False, cls=NpEncoder)

        # ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            "task": task,
            "total_samples": num_action,
            "accuracy": final_success_count / num_action * 100,
            "early_exit_rate": early_exit_count / num_action * 100,
            "early_exit_success_rate": early_exit_success_count / early_exit_count * 100 if early_exit_count > 0 else 0,
            "avg_times": {
                "segmentation": seg_time_sum / num_action,
                "stage1": s1_time_sum / num_action,
                "stage2": s2_time_sum / num_action,
                "total": (seg_time_sum + s1_time_sum + s2_time_sum) / num_action
            },
            "avg_flops_gflops": total_flops / num_action / 1e9,
        }

        with open(os.path.join(save_dir, f"{task}_metrics.json"), "w") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=4)

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("=" * 60)
        print(f"ğŸ“Š Final Results for {task}:")
        print(f"Total Samples: {num_action}")
        print(f"Accuracy: {metrics['accuracy']:.2f}%")
        print(f"Early Exit Rate: {metrics['early_exit_rate']:.2f}%")
        print(f"Early Exit Success Rate: {metrics['early_exit_success_rate']:.2f}%") 
        print(f"Avg Times: Seg {metrics['avg_times']['segmentation']:.3f}s, S1 {metrics['avg_times']['stage1']:.3f}s, S2 {metrics['avg_times']['stage2']:.3f}s")
        print(f"Avg FLOPs: {metrics['avg_flops_gflops']:.2f} GFLOPs")
        print("=" * 60)